{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence Nanodegree\n",
    "\n",
    "## Convolutional Neural Networks\n",
    "\n",
    "## Project: Write an Algorithm for a Dog Identification App \n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, some template code has already been provided for you, and you will need to implement additional functionality to successfully complete this project. You will not need to modify the included code beyond what is requested. Sections that begin with **'(IMPLEMENTATION)'** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section, and the specifics of the implementation are marked in the code block with a 'TODO' statement. Please be sure to read the instructions carefully! \n",
    "\n",
    "> **Note**: Once you have completed all of the code implementations, you need to finalize your work by exporting the iPython Notebook as an HTML document. Before exporting the notebook to html, all of the code cells need to have been run so that reviewers can see the final implementation and output. You can then export the notebook by using the menu above and navigating to  \\n\",\n",
    "    \"**File -> Download as -> HTML (.html)**. Include the finished document along with this notebook as your submission.\n",
    "\n",
    "In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **'Question X'** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **'Answer:'**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.\n",
    "\n",
    ">**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut.  Markdown cells can be edited by double-clicking the cell to enter edit mode.\n",
    "\n",
    "The rubric contains _optional_ \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. If you decide to pursue the \"Stand Out Suggestions\", you should include the code in this IPython notebook.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "### Why We're Here \n",
    "\n",
    "In this notebook, you will make the first steps towards developing an algorithm that could be used as part of a mobile or web app.  At the end of this project, your code will accept any user-supplied image as input.  If a dog is detected in the image, it will provide an estimate of the dog's breed.  If a human is detected, it will provide an estimate of the dog breed that is most resembling.  The image below displays potential sample output of your finished project (... but we expect that each student's algorithm will behave differently!). \n",
    "\n",
    "![Sample Dog Output](images/sample_dog_output.png)\n",
    "\n",
    "In this real-world setting, you will need to piece together a series of models to perform different tasks; for instance, the algorithm that detects humans in an image will be different from the CNN that infers dog breed.  There are many points of possible failure, and no perfect algorithm exists.  Your imperfect solution will nonetheless create a fun user experience!\n",
    "\n",
    "### The Road Ahead\n",
    "\n",
    "We break the notebook into separate steps.  Feel free to use the links below to navigate the notebook.\n",
    "\n",
    "* [Step 0](#step0): Import Datasets\n",
    "* [Step 1](#step1): Detect Humans\n",
    "* [Step 2](#step2): Detect Dogs\n",
    "* [Step 3](#step3): Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "* [Step 4](#step4): Use a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 5](#step5): Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "* [Step 6](#step6): Write your Algorithm\n",
    "* [Step 7](#step7): Test Your Algorithm\n",
    "\n",
    "---\n",
    "<a id='step0'></a>\n",
    "## Step 0: Import Datasets\n",
    "\n",
    "### Import Dog Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of dog images.  We populate a few variables through the use of the `load_files` function from the scikit-learn library:\n",
    "- `train_files`, `valid_files`, `test_files` - numpy arrays containing file paths to images\n",
    "- `train_targets`, `valid_targets`, `test_targets` - numpy arrays containing onehot-encoded classification labels \n",
    "- `dog_names` - list of string-valued dog breed names for translating labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SMARASAN\\AppData\\Local\\Continuum\\anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 133 total dog categories.\n",
      "There are 8351 total dog images.\n",
      "\n",
      "There are 6680 training dog images.\n",
      "There are 835 validation dog images.\n",
      "There are 836 test dog images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    dog_files = np.array(data['filenames'])\n",
    "    dog_targets = np_utils.to_categorical(np.array(data['target']), 133)\n",
    "    return dog_files, dog_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset('dogImages/train')\n",
    "valid_files, valid_targets = load_dataset('dogImages/valid')\n",
    "test_files, test_targets = load_dataset('dogImages/test')\n",
    "\n",
    "# load list of dog names\n",
    "dog_names = [item[20:-1] for item in sorted(glob(\"dogImages/train/*/\"))]\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total dog categories.' % len(dog_names))\n",
    "print('There are %s total dog images.\\n' % len(np.hstack([train_files, valid_files, test_files])))\n",
    "print('There are %d training dog images.' % len(train_files))\n",
    "print('There are %d validation dog images.' % len(valid_files))\n",
    "print('There are %d test dog images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Human Dataset\n",
    "\n",
    "In the code cell below, we import a dataset of human images, where the file paths are stored in the numpy array `human_files`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13233 total human images.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(8675309)\n",
    "\n",
    "# load filenames in shuffled human dataset\n",
    "human_files = np.array(glob(\"lfw/*/*\"))\n",
    "random.shuffle(human_files)\n",
    "\n",
    "# print statistics about the dataset\n",
    "print('There are %d total human images.' % len(human_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step1'></a>\n",
    "## Step 1: Detect Humans\n",
    "\n",
    "We use OpenCV's implementation of [Haar feature-based cascade classifiers](http://docs.opencv.org/trunk/d7/d8b/tutorial_py_face_detection.html) to detect human faces in images.  OpenCV provides many pre-trained face detectors, stored as XML files on [github](https://github.com/opencv/opencv/tree/master/data/haarcascades).  We have downloaded one of these detectors and stored it in the `haarcascades` directory.\n",
    "\n",
    "In the next code cell, we demonstrate how to use this detector to find human faces in a sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of faces detected: 1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import cv2                \n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline                               \n",
    "\n",
    "# extract pre-trained face detector\n",
    "face_cascade = cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_alt.xml')\n",
    "\n",
    "# load color (BGR) image\n",
    "img = cv2.imread(human_files[3])\n",
    "# convert BGR image to grayscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# find faces in image\n",
    "faces = face_cascade.detectMultiScale(gray)\n",
    "\n",
    "# print number of faces detected in the image\n",
    "print('Number of faces detected:', len(faces))\n",
    "\n",
    "# get bounding box for each detected face\n",
    "for (x,y,w,h) in faces:\n",
    "    # add bounding box to color image\n",
    "    cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "    \n",
    "# convert BGR image to RGB for plotting\n",
    "cv_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# display the image, along with bounding box\n",
    "plt.imshow(cv_rgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before using any of the face detectors, it is standard procedure to convert the images to grayscale.  The `detectMultiScale` function executes the classifier stored in `face_cascade` and takes the grayscale image as a parameter.  \n",
    "\n",
    "In the above code, `faces` is a numpy array of detected faces, where each row corresponds to a detected face.  Each detected face is a 1D array with four entries that specifies the bounding box of the detected face.  The first two entries in the array (extracted in the above code as `x` and `y`) specify the horizontal and vertical positions of the top left corner of the bounding box.  The last two entries in the array (extracted here as `w` and `h`) specify the width and height of the box.\n",
    "\n",
    "### Write a Human Face Detector\n",
    "\n",
    "We can use this procedure to write a function that returns `True` if a human face is detected in an image and `False` otherwise.  This function, aptly named `face_detector`, takes a string-valued file path to an image as input and appears in the code block below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns \"True\" if face is detected in image stored at img_path\n",
    "def face_detector(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray)\n",
    "    return len(faces) > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Human Face Detector\n",
    "\n",
    "__Question 1:__ Use the code cell below to test the performance of the `face_detector` function.  \n",
    "- What percentage of the first 100 images in `human_files` have a detected human face?  \n",
    "- What percentage of the first 100 images in `dog_files` have a detected human face? \n",
    "\n",
    "Ideally, we would like 100% of human images with a detected face and 0% of dog images with a detected face.  You will see that our algorithm falls short of this goal, but still gives acceptable performance.  We extract the file paths for the first 100 images from each of the datasets and store them in the numpy arrays `human_files_short` and `dog_files_short`.\n",
    "\n",
    "__Answer:__ Percentage of the first 100 images in human_files with detected human face is 99.0 and \n",
    "Percentage of the first 100 images in dog_files with detected human face is 12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the first 100 images in human_files with detected human face is 99.0\n",
      "Percentage of the first 100 images in dog_files with detected human face is 12.0\n"
     ]
    }
   ],
   "source": [
    "human_files_short = human_files[:100]\n",
    "dog_files_short = train_files[:100]\n",
    "# Do NOT modify the code above this line.\n",
    "\n",
    "## TODO: Test the performance of the face_detector algorithm ## on the images in human_files_short and dog_files_short.\n",
    "human = 0\n",
    "dog = 0\n",
    "\n",
    "for each_image in human_files_short:\n",
    "    if face_detector(each_image):\n",
    "        human += 1\n",
    "        \n",
    "for each_image in dog_files_short:\n",
    "    if face_detector(each_image):\n",
    "        dog += 1\n",
    "\n",
    "        \n",
    "print (\"Percentage of the first 100 images in human_files with detected human face is {human_face}\"\n",
    "       .format(human_face=(human/len(human_files_short)) * 100))\n",
    "print (\"Percentage of the first 100 images in dog_files with detected human face is {dog_face}\"\n",
    "       .format(dog_face=(dog/len(dog_files_short)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Question 2:__ This algorithmic choice necessitates that we communicate to the user that we accept human images only when they provide a clear view of a face (otherwise, we risk having unneccessarily frustrated users!). In your opinion, is this a reasonable expectation to pose on the user? If not, can you think of a way to detect humans in images that does not necessitate an image with a clearly presented face?\n",
    "\n",
    "__Answer:__\n",
    "I dont think restricting a user to provide a clear view of the face will be a good approach, for an example consider the photos being posted in social networks most of them will not be with clear view of the face :). May be we can train our algorithm with wide variety of photos with different angles, backgrounds and lightening conditions then we may overcome the problem.\n",
    "We suggest the face detector from OpenCV as a potential way to detect human images in your algorithm, but you are free to explore other approaches, especially approaches that make use of deep learning :).  Please use the code cell below to design and test your own face detection algorithm.  If you decide to pursue this _optional_ task, report performance on each of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## (Optional) TODO: Report the performance of another  \n",
    "## face detection algorithm on the LFW dataset\n",
    "### Feel free to use as many code cells as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step2'></a>\n",
    "## Step 2: Detect Dogs\n",
    "\n",
    "In this section, we use a pre-trained [ResNet-50](http://ethereon.github.io/netscope/#/gist/db945b393d40bfa26006) model to detect dogs in images.  Our first line of code downloads the ResNet-50 model, along with weights that have been trained on [ImageNet](http://www.image-net.org/), a very large, very popular dataset used for image classification and other vision tasks.  ImageNet contains over 10 million URLs, each linking to an image containing an object from one of [1000 categories](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a).  Given an image, this pre-trained ResNet-50 model returns a prediction (derived from the available categories in ImageNet) for the object that is contained in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "\n",
    "# define ResNet50 model\n",
    "ResNet50_model = ResNet50(weights='imagenet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process the Data\n",
    "\n",
    "When using TensorFlow as backend, Keras CNNs require a 4D array (which we'll also refer to as a 4D tensor) as input, with shape\n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, \\text{rows}, \\text{columns}, \\text{channels}),\n",
    "$$\n",
    "\n",
    "where `nb_samples` corresponds to the total number of images (or samples), and `rows`, `columns`, and `channels` correspond to the number of rows, columns, and channels for each image, respectively.  \n",
    "\n",
    "The `path_to_tensor` function below takes a string-valued file path to a color image as input and returns a 4D tensor suitable for supplying to a Keras CNN.  The function first loads the image and resizes it to a square image that is $224 \\times 224$ pixels.  Next, the image is converted to an array, which is then resized to a 4D tensor.  In this case, since we are working with color images, each image has three channels.  Likewise, since we are processing a single image (or sample), the returned tensor will always have shape\n",
    "\n",
    "$$\n",
    "(1, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "The `paths_to_tensor` function takes a numpy array of string-valued image paths as input and returns a 4D tensor with shape \n",
    "\n",
    "$$\n",
    "(\\text{nb_samples}, 224, 224, 3).\n",
    "$$\n",
    "\n",
    "Here, `nb_samples` is the number of samples, or number of images, in the supplied array of image paths.  It is best to think of `nb_samples` as the number of 3D tensors (where each 3D tensor corresponds to a different image) in your dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with ResNet-50\n",
    "\n",
    "Getting the 4D tensor ready for ResNet-50, and for any other pre-trained model in Keras, requires some additional processing.  First, the RGB image is converted to BGR by reordering the channels.  All pre-trained models have the additional normalization step that the mean pixel (expressed in RGB as $[103.939, 116.779, 123.68]$ and calculated from all pixels in all images in ImageNet) must be subtracted from every pixel in each image.  This is implemented in the imported function `preprocess_input`.  If you're curious, you can check the code for `preprocess_input` [here](https://github.com/fchollet/keras/blob/master/keras/applications/imagenet_utils.py).\n",
    "\n",
    "Now that we have a way to format our image for supplying to ResNet-50, we are now ready to use the model to extract the predictions.  This is accomplished with the `predict` method, which returns an array whose $i$-th entry is the model's predicted probability that the image belongs to the $i$-th ImageNet category.  This is implemented in the `ResNet50_predict_labels` function below.\n",
    "\n",
    "By taking the argmax of the predicted probability vector, we obtain an integer corresponding to the model's predicted object class, which we can identify with an object category through the use of this [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def ResNet50_predict_labels(img_path):\n",
    "    # returns prediction vector for image located at img_path\n",
    "    img = preprocess_input(path_to_tensor(img_path))\n",
    "    return np.argmax(ResNet50_model.predict(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a Dog Detector\n",
    "\n",
    "While looking at the [dictionary](https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a), you will notice that the categories corresponding to dogs appear in an uninterrupted sequence and correspond to dictionary keys 151-268, inclusive, to include all categories from `'Chihuahua'` to `'Mexican hairless'`.  Thus, in order to check to see if an image is predicted to contain a dog by the pre-trained ResNet-50 model, we need only check if the `ResNet50_predict_labels` function above returns a value between 151 and 268 (inclusive).\n",
    "\n",
    "We use these ideas to complete the `dog_detector` function below, which returns `True` if a dog is detected in an image (and `False` if not)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### returns \"True\" if a dog is detected in the image stored at img_path\n",
    "def dog_detector(img_path):\n",
    "    prediction = ResNet50_predict_labels(img_path)\n",
    "    return ((prediction <= 268) & (prediction >= 151)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Assess the Dog Detector\n",
    "\n",
    "__Question 3:__ Use the code cell below to test the performance of your `dog_detector` function.  \n",
    "- What percentage of the images in `human_files_short` have a detected dog?  \n",
    "- What percentage of the images in `dog_files_short` have a detected dog?\n",
    "\n",
    "__Answer:__ Percentage of the first 100 images in human_files with detected dog face is 1.0 and\n",
    "            Percentage of the first 100 images in dog_files with detected dog face is 100.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of the first 100 images in human_files with detected dog face is 1.0\n",
      "Percentage of the first 100 images in dog_files with detected dog face is 100.0\n"
     ]
    }
   ],
   "source": [
    "### TODO: Test the performance of the dog_detector function\n",
    "### on the images in human_files_short and dog_files_short.\n",
    "\n",
    "human = 0\n",
    "dog = 0\n",
    "\n",
    "for each_image in human_files_short:\n",
    "    if dog_detector(each_image):\n",
    "        human += 1\n",
    "        \n",
    "for each_image in dog_files_short:\n",
    "    if dog_detector(each_image):\n",
    "        dog += 1\n",
    "        \n",
    "print (\"Percentage of the first 100 images in human_files with detected dog face is {human_face}\"\n",
    "       .format(human_face=(human/len(human_files_short)) * 100))\n",
    "print (\"Percentage of the first 100 images in dog_files with detected dog face is {dog_face}\"\n",
    "       .format(dog_face=(dog/len(dog_files_short)) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step3'></a>\n",
    "## Step 3: Create a CNN to Classify Dog Breeds (from Scratch)\n",
    "\n",
    "Now that we have functions for detecting humans and dogs in images, we need a way to predict breed from images.  In this step, you will create a CNN that classifies dog breeds.  You must create your CNN _from scratch_ (so, you can't use transfer learning _yet_!), and you must attain a test accuracy of at least 1%.  In Step 5 of this notebook, you will have the opportunity to use transfer learning to create a CNN that attains greatly improved accuracy.\n",
    "\n",
    "Be careful with adding too many trainable layers!  More parameters means longer training, which means you are more likely to need a GPU to accelerate the training process.  Thankfully, Keras provides a handy estimate of the time that each epoch is likely to take; you can extrapolate this estimate to figure out how long it will take for your algorithm to train. \n",
    "\n",
    "We mention that the task of assigning breed to dogs from images is considered exceptionally challenging.  To see why, consider that *even a human* would have great difficulty in distinguishing between a Brittany and a Welsh Springer Spaniel.  \n",
    "\n",
    "Brittany | Welsh Springer Spaniel\n",
    "- | - \n",
    "<img src=\"images/Brittany_02625.jpg\" width=\"100\"> | <img src=\"images/Welsh_springer_spaniel_08203.jpg\" width=\"200\">\n",
    "\n",
    "It is not difficult to find other dog breed pairs with minimal inter-class variation (for instance, Curly-Coated Retrievers and American Water Spaniels).  \n",
    "\n",
    "Curly-Coated Retriever | American Water Spaniel\n",
    "- | -\n",
    "<img src=\"images/Curly-coated_retriever_03896.jpg\" width=\"200\"> | <img src=\"images/American_water_spaniel_00648.jpg\" width=\"200\">\n",
    "\n",
    "\n",
    "Likewise, recall that labradors come in yellow, chocolate, and black.  Your vision-based algorithm will have to conquer this high intra-class variation to determine how to classify all of these different shades as the same breed.  \n",
    "\n",
    "Yellow Labrador | Chocolate Labrador | Black Labrador\n",
    "- | -\n",
    "<img src=\"images/Labrador_retriever_06457.jpg\" width=\"150\"> | <img src=\"images/Labrador_retriever_06455.jpg\" width=\"240\"> | <img src=\"images/Labrador_retriever_06449.jpg\" width=\"220\">\n",
    "\n",
    "We also mention that random chance presents an exceptionally low bar: setting aside the fact that the classes are slightly imabalanced, a random guess will provide a correct answer roughly 1 in 133 times, which corresponds to an accuracy of less than 1%.  \n",
    "\n",
    "Remember that the practice is far ahead of the theory in deep learning.  Experiment with many different architectures, and trust your intuition.  And, of course, have fun! \n",
    "\n",
    "### Pre-process the Data\n",
    "\n",
    "We rescale the images by dividing every pixel in every image by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 6680/6680 [00:56<00:00, 118.75it/s]\n",
      "100%|| 835/835 [00:06<00:00, 129.02it/s]\n",
      "100%|| 836/836 [00:06<00:00, 131.03it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        model.summary()\n",
    "\n",
    "We have imported some Python modules to get you started, but feel free to import as many modules as you need.  If you end up getting stuck, here's a hint that specifies a model that trains relatively fast on CPU and attains >1% test accuracy in 5 epochs:\n",
    "\n",
    "![Sample CNN](images/sample_cnn.png)\n",
    "           \n",
    "__Question 4:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  If you chose to use the hinted architecture above, describe why you think that CNN architecture should work well for the image classification task.\n",
    "\n",
    "__Answer:__  I have choosen to use the similar architecture as given above. In my architecture I have used 3 convlution layers which can detect patterns in the images to differentiate dog breeds and pooling layers to reduce dimensionality of an array. I have also used dropout layers to prevent overfitting and softmax to classify the output into 133 categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1226 (Conv2D)         (None, 224, 224, 16)      208       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_57 (MaxPooling (None, 112, 112, 16)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1227 (Conv2D)         (None, 112, 112, 32)      2080      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_58 (MaxPooling (None, 56, 56, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1228 (Conv2D)         (None, 56, 56, 64)        8256      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_59 (MaxPooling (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 50176)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 500)               25088500  \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 133)               66633     \n",
      "=================================================================\n",
      "Total params: 25,165,677\n",
      "Trainable params: 25,165,677\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(filters=16, kernel_size=2, padding='same', activation='relu', input_shape=(224, 224, 3)))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=2, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.\n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000/6680 [================>.............] - ETA: 1:52:47 - loss: 4.8894 - acc: 0.0000e+ - ETA: 59:12 - loss: 9.8570 - acc: 0.0000e+00 - ETA: 41:05 - loss: 11.7472 - acc: 0.0000e+0 - ETA: 32:20 - loss: 10.8651 - acc: 0.0000e+0 - ETA: 27:00 - loss: 9.7257 - acc: 0.0000e+0 - ETA: 23:19 - loss: 8.9019 - acc: 0.0000e+ - ETA: 20:42 - loss: 8.3467 - acc: 0.0000e+ - ETA: 18:43 - loss: 7.9239 - acc: 0.0000e+ - ETA: 17:12 - loss: 7.5818 - acc: 0.0000e+ - ETA: 16:03 - loss: 7.3177 - acc: 0.0000e+ - ETA: 15:01 - loss: 7.1047 - acc: 0.0000e+ - ETA: 14:16 - loss: 6.9225 - acc: 0.0000e+ - ETA: 13:37 - loss: 6.7697 - acc: 0.0000e+ - ETA: 13:02 - loss: 6.6380 - acc: 0.0000e+ - ETA: 12:33 - loss: 6.5183 - acc: 0.0000e+ - ETA: 12:05 - loss: 6.4159 - acc: 0.0000e+ - ETA: 11:40 - loss: 6.3223 - acc: 0.0000e+ - ETA: 11:20 - loss: 6.2386 - acc: 0.0000e+ - ETA: 10:58 - loss: 6.1678 - acc: 0.0000e+ - ETA: 10:39 - loss: 6.1076 - acc: 0.0000e+ - ETA: 10:24 - loss: 6.0491 - acc: 0.0024   - ETA: 10:08 - loss: 5.9947 - acc: 0.00 - ETA: 9:54 - loss: 5.9490 - acc: 0.0022 - ETA: 9:44 - loss: 5.9050 - acc: 0.002 - ETA: 9:32 - loss: 5.8662 - acc: 0.002 - ETA: 9:24 - loss: 5.8284 - acc: 0.001 - ETA: 9:12 - loss: 5.7930 - acc: 0.001 - ETA: 9:02 - loss: 5.7606 - acc: 0.001 - ETA: 8:54 - loss: 5.7308 - acc: 0.001 - ETA: 8:46 - loss: 5.7030 - acc: 0.001 - ETA: 8:38 - loss: 5.6795 - acc: 0.001 - ETA: 8:35 - loss: 5.6548 - acc: 0.004 - ETA: 8:26 - loss: 5.6310 - acc: 0.006 - ETA: 8:21 - loss: 5.6082 - acc: 0.005 - ETA: 8:15 - loss: 5.5885 - acc: 0.007 - ETA: 8:08 - loss: 5.5683 - acc: 0.009 - ETA: 8:01 - loss: 5.5520 - acc: 0.009 - ETA: 7:55 - loss: 5.5334 - acc: 0.009 - ETA: 7:49 - loss: 5.5188 - acc: 0.009 - ETA: 7:42 - loss: 5.5032 - acc: 0.008 - ETA: 7:37 - loss: 5.4881 - acc: 0.008 - ETA: 7:34 - loss: 5.4739 - acc: 0.008 - ETA: 7:29 - loss: 5.4596 - acc: 0.008 - ETA: 7:24 - loss: 5.4471 - acc: 0.008 - ETA: 7:23 - loss: 5.4356 - acc: 0.007 - ETA: 7:23 - loss: 5.4241 - acc: 0.007 - ETA: 7:20 - loss: 5.4130 - acc: 0.007 - ETA: 7:16 - loss: 5.4021 - acc: 0.007 - ETA: 7:12 - loss: 5.3917 - acc: 0.007 - ETA: 7:08 - loss: 5.3821 - acc: 0.007 - ETA: 7:05 - loss: 5.3723 - acc: 0.006 - ETA: 7:00 - loss: 5.3631 - acc: 0.006 - ETA: 6:56 - loss: 5.3543 - acc: 0.007 - ETA: 6:52 - loss: 5.3456 - acc: 0.007 - ETA: 6:49 - loss: 5.3375 - acc: 0.007 - ETA: 6:46 - loss: 5.3294 - acc: 0.007 - ETA: 6:42 - loss: 5.3220 - acc: 0.007 - ETA: 6:39 - loss: 5.3144 - acc: 0.006 - ETA: 6:36 - loss: 5.3073 - acc: 0.006 - ETA: 6:33 - loss: 5.3002 - acc: 0.006 - ETA: 6:29 - loss: 5.2932 - acc: 0.006 - ETA: 6:26 - loss: 5.2862 - acc: 0.006 - ETA: 6:23 - loss: 5.2810 - acc: 0.006 - ETA: 6:19 - loss: 5.2749 - acc: 0.007 - ETA: 6:16 - loss: 5.2686 - acc: 0.006 - ETA: 6:13 - loss: 5.2644 - acc: 0.006 - ETA: 6:10 - loss: 5.2587 - acc: 0.006 - ETA: 6:07 - loss: 5.2531 - acc: 0.006 - ETA: 6:04 - loss: 5.2479 - acc: 0.006 - ETA: 6:01 - loss: 5.2424 - acc: 0.006 - ETA: 5:58 - loss: 5.2371 - acc: 0.007 - ETA: 5:56 - loss: 5.2324 - acc: 0.007 - ETA: 5:53 - loss: 5.2278 - acc: 0.007 - ETA: 5:51 - loss: 5.2230 - acc: 0.007 - ETA: 5:48 - loss: 5.2188 - acc: 0.007 - ETA: 5:46 - loss: 5.2143 - acc: 0.007 - ETA: 5:43 - loss: 5.2099 - acc: 0.007 - ETA: 5:40 - loss: 5.2057 - acc: 0.007 - ETA: 5:38 - loss: 5.2016 - acc: 0.008 - ETA: 5:36 - loss: 5.1978 - acc: 0.008 - ETA: 5:34 - loss: 5.1939 - acc: 0.008 - ETA: 5:31 - loss: 5.1897 - acc: 0.008 - ETA: 5:29 - loss: 5.1860 - acc: 0.008 - ETA: 5:27 - loss: 5.1824 - acc: 0.008 - ETA: 5:24 - loss: 5.1790 - acc: 0.008 - ETA: 5:22 - loss: 5.1756 - acc: 0.008 - ETA: 5:20 - loss: 5.1724 - acc: 0.008 - ETA: 5:19 - loss: 5.1686 - acc: 0.008 - ETA: 5:17 - loss: 5.1667 - acc: 0.007 - ETA: 5:15 - loss: 5.1633 - acc: 0.007 - ETA: 5:13 - loss: 5.1609 - acc: 0.007 - ETA: 5:12 - loss: 5.1581 - acc: 0.007 - ETA: 5:09 - loss: 5.1554 - acc: 0.007 - ETA: 5:08 - loss: 5.1524 - acc: 0.008 - ETA: 5:06 - loss: 5.1495 - acc: 0.007 - ETA: 5:04 - loss: 5.1465 - acc: 0.008 - ETA: 5:02 - loss: 5.1443 - acc: 0.008 - ETA: 5:01 - loss: 5.1412 - acc: 0.008 - ETA: 4:59 - loss: 5.1397 - acc: 0.008 - ETA: 4:58 - loss: 5.1368 - acc: 0.008 - ETA: 4:56 - loss: 5.1347 - acc: 0.008 - ETA: 4:55 - loss: 5.1323 - acc: 0.008 - ETA: 4:53 - loss: 5.1299 - acc: 0.008 - ETA: 4:51 - loss: 5.1275 - acc: 0.008 - ETA: 4:50 - loss: 5.1252 - acc: 0.009 - ETA: 4:48 - loss: 5.1230 - acc: 0.009 - ETA: 4:46 - loss: 5.1210 - acc: 0.008 - ETA: 4:45 - loss: 5.1187 - acc: 0.008 - ETA: 4:43 - loss: 5.1163 - acc: 0.008 - ETA: 4:42 - loss: 5.1141 - acc: 0.008 - ETA: 4:41 - loss: 5.1115 - acc: 0.008 - ETA: 4:39 - loss: 5.1095 - acc: 0.008 - ETA: 4:38 - loss: 5.1072 - acc: 0.008 - ETA: 4:36 - loss: 5.1056 - acc: 0.008 - ETA: 4:34 - loss: 5.1037 - acc: 0.009 - ETA: 4:33 - loss: 5.1017 - acc: 0.009 - ETA: 4:31 - loss: 5.0997 - acc: 0.009 - ETA: 4:30 - loss: 5.0975 - acc: 0.008 - ETA: 4:29 - loss: 5.0954 - acc: 0.008 - ETA: 4:27 - loss: 5.0937 - acc: 0.008 - ETA: 4:25 - loss: 5.0919 - acc: 0.009 - ETA: 4:24 - loss: 5.0900 - acc: 0.009 - ETA: 4:22 - loss: 5.0884 - acc: 0.009 - ETA: 4:21 - loss: 5.0867 - acc: 0.009 - ETA: 4:19 - loss: 5.0849 - acc: 0.009 - ETA: 4:18 - loss: 5.0835 - acc: 0.009 - ETA: 4:16 - loss: 5.0817 - acc: 0.009 - ETA: 4:14 - loss: 5.0804 - acc: 0.009 - ETA: 4:14 - loss: 5.0789 - acc: 0.009 - ETA: 4:12 - loss: 5.0777 - acc: 0.009 - ETA: 4:11 - loss: 5.0764 - acc: 0.009 - ETA: 4:09 - loss: 5.0749 - acc: 0.009 - ETA: 4:08 - loss: 5.0733 - acc: 0.009 - ETA: 4:07 - loss: 5.0719 - acc: 0.009 - ETA: 4:05 - loss: 5.0707 - acc: 0.009 - ETA: 4:04 - loss: 5.0693 - acc: 0.009 - ETA: 4:02 - loss: 5.0678 - acc: 0.009 - ETA: 4:01 - loss: 5.0661 - acc: 0.009 - ETA: 3:59 - loss: 5.0654 - acc: 0.009 - ETA: 3:58 - loss: 5.0638 - acc: 0.009 - ETA: 3:56 - loss: 5.0628 - acc: 0.009 - ETA: 3:55 - loss: 5.0616 - acc: 0.009 - ETA: 3:53 - loss: 5.0603 - acc: 0.009 - ETA: 3:52 - loss: 5.0591 - acc: 0.009 - ETA: 3:50 - loss: 5.0576 - acc: 0.009 - ETA: 3:49 - loss: 5.0563 - acc: 0.009 - ETA: 3:48 - loss: 5.0543 - acc: 0.009 - ETA: 3:46 - loss: 5.0530 - acc: 0.009 - ETA: 3:44 - loss: 5.0515 - acc: 0.009 - ETA: 3:43 - loss: 5.0503 - acc: 0.009 - ETA: 3:42 - loss: 5.0496 - acc: 0.009 - ETA: 3:40 - loss: 5.0484 - acc: 0.009 - ETA: 3:39 - loss: 5.0479 - acc: 0.009 - ETA: 3:37 - loss: 5.0466 - acc: 0.009 - ETA: 3:36 - loss: 5.0455 - acc: 0.010 - ETA: 3:35 - loss: 5.0439 - acc: 0.009 - ETA: 3:34 - loss: 5.0425 - acc: 0.009 - ETA: 3:32 - loss: 5.0413 - acc: 0.009 - ETA: 3:31 - loss: 5.0406 - acc: 0.009 - ETA: 3:30 - loss: 5.0396 - acc: 0.009 - ETA: 3:29 - loss: 5.0384 - acc: 0.009 - ETA: 3:27 - loss: 5.0377 - acc: 0.009 - ETA: 3:26 - loss: 5.0366 - acc: 0.009 - ETA: 3:24 - loss: 5.0357 - acc: 0.009 - ETA: 3:23 - loss: 5.0346 - acc: 0.009 - ETA: 3:21 - loss: 5.0335 - acc: 0.009 - ETA: 3:20 - loss: 5.0325 - acc: 0.009 - ETA: 3:19 - loss: 5.0317 - acc: 0.009 - ETA: 3:17 - loss: 5.0316 - acc: 0.009 - ETA: 3:16 - loss: 5.0308 - acc: 0.009 - ETA: 3:14 - loss: 5.0296 - acc: 0.009 - ETA: 3:13 - loss: 5.0285 - acc: 0.009 - ETA: 3:12 - loss: 5.0275 - acc: 0.009 - ETA: 3:11 - loss: 5.0264 - acc: 0.009 - ETA: 3:10 - loss: 5.0258 - acc: 0.009 - ETA: 3:08 - loss: 5.0246 - acc: 0.009 - ETA: 3:07 - loss: 5.0237 - acc: 0.009 - ETA: 3:06 - loss: 5.0227 - acc: 0.009 - ETA: 3:04 - loss: 5.0218 - acc: 0.009 - ETA: 3:03 - loss: 5.0209 - acc: 0.009 - ETA: 3:02 - loss: 5.0195 - acc: 0.009 - ETA: 3:00 - loss: 5.0209 - acc: 0.009 - ETA: 2:59 - loss: 5.0200 - acc: 0.009 - ETA: 2:58 - loss: 5.0196 - acc: 0.009 - ETA: 2:56 - loss: 5.0187 - acc: 0.009 - ETA: 2:55 - loss: 5.0181 - acc: 0.009 - ETA: 2:54 - loss: 5.0172 - acc: 0.009 - ETA: 2:53 - loss: 5.0160 - acc: 0.009 - ETA: 2:52 - loss: 5.0151 - acc: 0.010 - ETA: 2:51 - loss: 5.0143 - acc: 0.010 - ETA: 2:49 - loss: 5.0137 - acc: 0.009 - ETA: 2:48 - loss: 5.0130 - acc: 0.010 - ETA: 2:46 - loss: 5.0122 - acc: 0.010 - ETA: 2:45 - loss: 5.0112 - acc: 0.010 - ETA: 2:44 - loss: 5.0103 - acc: 0.010 - ETA: 2:42 - loss: 5.0098 - acc: 0.010 - ETA: 2:41 - loss: 5.0091 - acc: 0.010 - ETA: 2:40 - loss: 5.0080 - acc: 0.010 - ETA: 2:39 - loss: 5.0073 - acc: 0.010 - ETA: 2:37 - loss: 5.0066 - acc: 0.0108\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 2:36 - loss: 5.0051 - acc: 0.010 - ETA: 2:35 - loss: 5.0048 - acc: 0.010 - ETA: 2:33 - loss: 5.0042 - acc: 0.011 - ETA: 2:32 - loss: 5.0032 - acc: 0.011 - ETA: 2:31 - loss: 5.0025 - acc: 0.011 - ETA: 2:29 - loss: 5.0021 - acc: 0.011 - ETA: 2:28 - loss: 5.0015 - acc: 0.011 - ETA: 2:27 - loss: 5.0005 - acc: 0.011 - ETA: 2:26 - loss: 5.0001 - acc: 0.011 - ETA: 2:24 - loss: 4.9991 - acc: 0.011 - ETA: 2:23 - loss: 4.9986 - acc: 0.011 - ETA: 2:22 - loss: 4.9975 - acc: 0.011 - ETA: 2:20 - loss: 4.9978 - acc: 0.011 - ETA: 2:19 - loss: 4.9968 - acc: 0.011 - ETA: 2:18 - loss: 4.9960 - acc: 0.011 - ETA: 2:17 - loss: 4.9950 - acc: 0.011 - ETA: 2:15 - loss: 4.9936 - acc: 0.011 - ETA: 2:14 - loss: 4.9927 - acc: 0.011 - ETA: 2:13 - loss: 4.9914 - acc: 0.012 - ETA: 2:12 - loss: 4.9916 - acc: 0.012 - ETA: 2:10 - loss: 4.9913 - acc: 0.012 - ETA: 2:09 - loss: 4.9907 - acc: 0.012 - ETA: 2:08 - loss: 4.9898 - acc: 0.012 - ETA: 2:07 - loss: 4.9889 - acc: 0.012 - ETA: 2:05 - loss: 4.9884 - acc: 0.012 - ETA: 2:04 - loss: 4.9873 - acc: 0.012 - ETA: 2:03 - loss: 4.9869 - acc: 0.012 - ETA: 2:02 - loss: 4.9863 - acc: 0.012 - ETA: 2:00 - loss: 4.9855 - acc: 0.012 - ETA: 1:59 - loss: 4.9837 - acc: 0.012 - ETA: 1:58 - loss: 4.9842 - acc: 0.012 - ETA: 1:57 - loss: 4.9833 - acc: 0.012 - ETA: 1:56 - loss: 4.9825 - acc: 0.012 - ETA: 1:54 - loss: 4.9823 - acc: 0.012 - ETA: 1:53 - loss: 4.9809 - acc: 0.012 - ETA: 1:52 - loss: 4.9803 - acc: 0.012 - ETA: 1:51 - loss: 4.9798 - acc: 0.012 - ETA: 1:49 - loss: 4.9790 - acc: 0.012 - ETA: 1:48 - loss: 4.9780 - acc: 0.012 - ETA: 1:47 - loss: 4.9768 - acc: 0.012 - ETA: 1:46 - loss: 4.9761 - acc: 0.012 - ETA: 1:45 - loss: 4.9749 - acc: 0.012 - ETA: 1:43 - loss: 4.9739 - acc: 0.013 - ETA: 1:42 - loss: 4.9729 - acc: 0.012 - ETA: 1:41 - loss: 4.9718 - acc: 0.012 - ETA: 1:40 - loss: 4.9706 - acc: 0.013 - ETA: 1:39 - loss: 4.9701 - acc: 0.013 - ETA: 1:37 - loss: 4.9685 - acc: 0.013 - ETA: 1:36 - loss: 4.9698 - acc: 0.013 - ETA: 1:35 - loss: 4.9689 - acc: 0.013 - ETA: 1:34 - loss: 4.9675 - acc: 0.013 - ETA: 1:33 - loss: 4.9659 - acc: 0.013 - ETA: 1:32 - loss: 4.9645 - acc: 0.013 - ETA: 1:30 - loss: 4.9631 - acc: 0.013 - ETA: 1:29 - loss: 4.9621 - acc: 0.013 - ETA: 1:28 - loss: 4.9607 - acc: 0.013 - ETA: 1:27 - loss: 4.9607 - acc: 0.013 - ETA: 1:26 - loss: 4.9599 - acc: 0.013 - ETA: 1:25 - loss: 4.9595 - acc: 0.013 - ETA: 1:23 - loss: 4.9585 - acc: 0.013 - ETA: 1:22 - loss: 4.9580 - acc: 0.013 - ETA: 1:21 - loss: 4.9568 - acc: 0.013 - ETA: 1:20 - loss: 4.9557 - acc: 0.014 - ETA: 1:19 - loss: 4.9547 - acc: 0.014 - ETA: 1:18 - loss: 4.9536 - acc: 0.014 - ETA: 1:16 - loss: 4.9531 - acc: 0.014 - ETA: 1:15 - loss: 4.9527 - acc: 0.014 - ETA: 1:14 - loss: 4.9513 - acc: 0.014 - ETA: 1:13 - loss: 4.9513 - acc: 0.014 - ETA: 1:12 - loss: 4.9501 - acc: 0.014 - ETA: 1:11 - loss: 4.9491 - acc: 0.014 - ETA: 1:09 - loss: 4.9487 - acc: 0.014 - ETA: 1:08 - loss: 4.9477 - acc: 0.014 - ETA: 1:07 - loss: 4.9465 - acc: 0.014 - ETA: 1:06 - loss: 4.9455 - acc: 0.014 - ETA: 1:05 - loss: 4.9454 - acc: 0.014 - ETA: 1:04 - loss: 4.9448 - acc: 0.014 - ETA: 1:03 - loss: 4.9436 - acc: 0.014 - ETA: 1:02 - loss: 4.9422 - acc: 0.014 - ETA: 1:00 - loss: 4.9408 - acc: 0.014 - ETA: 59s - loss: 4.9403 - acc: 0.014 - ETA: 58s - loss: 4.9394 - acc: 0.01 - ETA: 57s - loss: 4.9395 - acc: 0.01 - ETA: 56s - loss: 4.9392 - acc: 0.01 - ETA: 55s - loss: 4.9374 - acc: 0.01 - ETA: 53s - loss: 4.9363 - acc: 0.01 - ETA: 52s - loss: 4.9358 - acc: 0.01 - ETA: 51s - loss: 4.9348 - acc: 0.01 - ETA: 50s - loss: 4.9339 - acc: 0.01 - ETA: 49s - loss: 4.9333 - acc: 0.01 - ETA: 48s - loss: 4.9325 - acc: 0.01 - ETA: 47s - loss: 4.9312 - acc: 0.01 - ETA: 45s - loss: 4.9305 - acc: 0.01 - ETA: 44s - loss: 4.9299 - acc: 0.01 - ETA: 43s - loss: 4.9294 - acc: 0.01 - ETA: 42s - loss: 4.9284 - acc: 0.01 - ETA: 41s - loss: 4.9282 - acc: 0.01 - ETA: 40s - loss: 4.9278 - acc: 0.01 - ETA: 39s - loss: 4.9266 - acc: 0.01 - ETA: 37s - loss: 4.9263 - acc: 0.01 - ETA: 36s - loss: 4.9261 - acc: 0.01 - ETA: 35s - loss: 4.9251 - acc: 0.01 - ETA: 34s - loss: 4.9248 - acc: 0.01 - ETA: 33s - loss: 4.9240 - acc: 0.01 - ETA: 32s - loss: 4.9233 - acc: 0.01 - ETA: 31s - loss: 4.9228 - acc: 0.01 - ETA: 30s - loss: 4.9217 - acc: 0.01 - ETA: 29s - loss: 4.9218 - acc: 0.01 - ETA: 27s - loss: 4.9214 - acc: 0.01 - ETA: 26s - loss: 4.9203 - acc: 0.01 - ETA: 25s - loss: 4.9195 - acc: 0.01 - ETA: 24s - loss: 4.9189 - acc: 0.01 - ETA: 23s - loss: 4.9177 - acc: 0.01 - ETA: 22s - loss: 4.9170 - acc: 0.01 - ETA: 21s - loss: 4.9159 - acc: 0.01 - ETA: 20s - loss: 4.9149 - acc: 0.01 - ETA: 18s - loss: 4.9139 - acc: 0.01 - ETA: 17s - loss: 4.9134 - acc: 0.01 - ETA: 16s - loss: 4.9128 - acc: 0.01 - ETA: 15s - loss: 4.9121 - acc: 0.01 - ETA: 14s - loss: 4.9104 - acc: 0.01 - ETA: 13s - loss: 4.9096 - acc: 0.01 - ETA: 12s - loss: 4.9082 - acc: 0.01 - ETA: 11s - loss: 4.9074 - acc: 0.01 - ETA: 10s - loss: 4.9064 - acc: 0.01 - ETA: 8s - loss: 4.9065 - acc: 0.0175 - ETA: 7s - loss: 4.9060 - acc: 0.017 - ETA: 6s - loss: 4.9051 - acc: 0.017 - ETA: 5s - loss: 4.9047 - acc: 0.017 - ETA: 4s - loss: 4.9040 - acc: 0.017 - ETA: 3s - loss: 4.9028 - acc: 0.017 - ETA: 2s - loss: 4.9021 - acc: 0.017 - ETA: 1s - loss: 4.9019 - acc: 0.017 - 390s 58ms/step - loss: 4.9017 - acc: 0.0177 - val_loss: 4.5963 - val_acc: 0.0335\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 4.59629, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 5:58 - loss: 4.4130 - acc: 0.150 - ETA: 5:17 - loss: 4.4757 - acc: 0.100 - ETA: 5:03 - loss: 4.4685 - acc: 0.100 - ETA: 4:55 - loss: 4.4727 - acc: 0.100 - ETA: 4:51 - loss: 4.4290 - acc: 0.100 - ETA: 4:48 - loss: 4.4169 - acc: 0.091 - ETA: 4:45 - loss: 4.4206 - acc: 0.092 - ETA: 4:44 - loss: 4.4275 - acc: 0.093 - ETA: 4:42 - loss: 4.4497 - acc: 0.083 - ETA: 4:40 - loss: 4.4572 - acc: 0.075 - ETA: 4:38 - loss: 4.4802 - acc: 0.068 - ETA: 4:37 - loss: 4.4833 - acc: 0.070 - ETA: 4:36 - loss: 4.4903 - acc: 0.065 - ETA: 4:35 - loss: 4.4895 - acc: 0.067 - ETA: 4:34 - loss: 4.5143 - acc: 0.063 - ETA: 4:32 - loss: 4.5052 - acc: 0.062 - ETA: 4:31 - loss: 4.5060 - acc: 0.058 - ETA: 4:30 - loss: 4.5067 - acc: 0.061 - ETA: 4:29 - loss: 4.5007 - acc: 0.060 - ETA: 4:27 - loss: 4.4948 - acc: 0.060 - ETA: 4:26 - loss: 4.4702 - acc: 0.069 - ETA: 4:25 - loss: 4.4953 - acc: 0.065 - ETA: 4:24 - loss: 4.4892 - acc: 0.063 - ETA: 4:23 - loss: 4.4734 - acc: 0.068 - ETA: 4:22 - loss: 4.4682 - acc: 0.070 - ETA: 4:21 - loss: 4.4607 - acc: 0.069 - ETA: 4:20 - loss: 4.4658 - acc: 0.070 - ETA: 4:19 - loss: 4.4625 - acc: 0.067 - ETA: 4:18 - loss: 4.4707 - acc: 0.065 - ETA: 4:17 - loss: 4.4661 - acc: 0.066 - ETA: 4:16 - loss: 4.4655 - acc: 0.066 - ETA: 4:15 - loss: 4.4601 - acc: 0.065 - ETA: 4:15 - loss: 4.4625 - acc: 0.065 - ETA: 4:14 - loss: 4.4569 - acc: 0.067 - ETA: 4:13 - loss: 4.4476 - acc: 0.070 - ETA: 4:12 - loss: 4.4419 - acc: 0.069 - ETA: 4:12 - loss: 4.4311 - acc: 0.070 - ETA: 4:11 - loss: 4.4222 - acc: 0.072 - ETA: 4:10 - loss: 4.4331 - acc: 0.071 - ETA: 4:09 - loss: 4.4290 - acc: 0.073 - ETA: 4:08 - loss: 4.4325 - acc: 0.073 - ETA: 4:07 - loss: 4.4228 - acc: 0.076 - ETA: 4:06 - loss: 4.4136 - acc: 0.075 - ETA: 4:05 - loss: 4.4052 - acc: 0.077 - ETA: 4:04 - loss: 4.4115 - acc: 0.075 - ETA: 4:03 - loss: 4.4120 - acc: 0.075 - ETA: 4:02 - loss: 4.4099 - acc: 0.074 - ETA: 4:01 - loss: 4.4132 - acc: 0.074 - ETA: 4:00 - loss: 4.4160 - acc: 0.072 - ETA: 4:00 - loss: 4.4179 - acc: 0.073 - ETA: 3:59 - loss: 4.4187 - acc: 0.074 - ETA: 3:58 - loss: 4.4209 - acc: 0.073 - ETA: 3:57 - loss: 4.4213 - acc: 0.072 - ETA: 3:56 - loss: 4.4157 - acc: 0.073 - ETA: 3:55 - loss: 4.4194 - acc: 0.071 - ETA: 3:54 - loss: 4.4186 - acc: 0.070 - ETA: 3:53 - loss: 4.4203 - acc: 0.070 - ETA: 3:53 - loss: 4.4201 - acc: 0.069 - ETA: 3:52 - loss: 4.4258 - acc: 0.068 - ETA: 3:51 - loss: 4.4173 - acc: 0.069 - ETA: 3:50 - loss: 4.4178 - acc: 0.070 - ETA: 3:49 - loss: 4.4158 - acc: 0.069 - ETA: 3:48 - loss: 4.4134 - acc: 0.069 - ETA: 3:47 - loss: 4.4138 - acc: 0.068 - ETA: 3:46 - loss: 4.4260 - acc: 0.070 - ETA: 3:46 - loss: 4.4260 - acc: 0.069 - ETA: 3:45 - loss: 4.4261 - acc: 0.069 - ETA: 3:44 - loss: 4.4237 - acc: 0.069 - ETA: 3:43 - loss: 4.4223 - acc: 0.069 - ETA: 3:42 - loss: 4.4133 - acc: 0.070 - ETA: 3:41 - loss: 4.4129 - acc: 0.070 - ETA: 3:40 - loss: 4.4088 - acc: 0.070 - ETA: 3:40 - loss: 4.4063 - acc: 0.070 - ETA: 3:39 - loss: 4.4078 - acc: 0.070 - ETA: 3:38 - loss: 4.4084 - acc: 0.070 - ETA: 3:37 - loss: 4.4096 - acc: 0.069 - ETA: 3:36 - loss: 4.4069 - acc: 0.068 - ETA: 3:35 - loss: 4.4043 - acc: 0.068 - ETA: 3:35 - loss: 4.4026 - acc: 0.069 - ETA: 3:34 - loss: 4.3987 - acc: 0.069 - ETA: 3:33 - loss: 4.4031 - acc: 0.068 - ETA: 3:32 - loss: 4.4051 - acc: 0.070 - ETA: 3:32 - loss: 4.4088 - acc: 0.069 - ETA: 3:31 - loss: 4.4072 - acc: 0.069 - ETA: 3:30 - loss: 4.4043 - acc: 0.069 - ETA: 3:29 - loss: 4.3995 - acc: 0.070 - ETA: 3:29 - loss: 4.3966 - acc: 0.071 - ETA: 3:28 - loss: 4.3930 - acc: 0.070 - ETA: 3:27 - loss: 4.3940 - acc: 0.070 - ETA: 3:26 - loss: 4.3940 - acc: 0.070 - ETA: 3:25 - loss: 4.3948 - acc: 0.070 - ETA: 3:25 - loss: 4.3914 - acc: 0.071 - ETA: 3:24 - loss: 4.3950 - acc: 0.070 - ETA: 3:23 - loss: 4.3942 - acc: 0.070 - ETA: 3:22 - loss: 4.3976 - acc: 0.069 - ETA: 3:21 - loss: 4.3988 - acc: 0.069 - ETA: 3:20 - loss: 4.3984 - acc: 0.068 - ETA: 3:19 - loss: 4.3998 - acc: 0.067 - ETA: 3:19 - loss: 4.3982 - acc: 0.067 - ETA: 3:18 - loss: 4.3977 - acc: 0.066 - ETA: 3:17 - loss: 4.3982 - acc: 0.065 - ETA: 3:16 - loss: 4.3960 - acc: 0.065 - ETA: 3:15 - loss: 4.3970 - acc: 0.065 - ETA: 3:14 - loss: 4.3960 - acc: 0.065 - ETA: 3:13 - loss: 4.3945 - acc: 0.065 - ETA: 3:12 - loss: 4.3909 - acc: 0.065 - ETA: 3:12 - loss: 4.3850 - acc: 0.065 - ETA: 3:11 - loss: 4.3857 - acc: 0.066 - ETA: 3:10 - loss: 4.3871 - acc: 0.066 - ETA: 3:09 - loss: 4.3896 - acc: 0.065 - ETA: 3:08 - loss: 4.3884 - acc: 0.065 - ETA: 3:07 - loss: 4.3854 - acc: 0.065 - ETA: 3:06 - loss: 4.3813 - acc: 0.065 - ETA: 3:06 - loss: 4.3794 - acc: 0.066 - ETA: 3:05 - loss: 4.3774 - acc: 0.067 - ETA: 3:04 - loss: 4.3781 - acc: 0.066 - ETA: 3:03 - loss: 4.3794 - acc: 0.066 - ETA: 3:02 - loss: 4.3803 - acc: 0.066 - ETA: 3:01 - loss: 4.3797 - acc: 0.066 - ETA: 3:00 - loss: 4.3811 - acc: 0.065 - ETA: 3:00 - loss: 4.3818 - acc: 0.065 - ETA: 2:59 - loss: 4.3810 - acc: 0.065 - ETA: 2:58 - loss: 4.3794 - acc: 0.065 - ETA: 2:57 - loss: 4.3766 - acc: 0.066 - ETA: 2:56 - loss: 4.3783 - acc: 0.065 - ETA: 2:55 - loss: 4.3773 - acc: 0.066 - ETA: 2:54 - loss: 4.3759 - acc: 0.066 - ETA: 2:54 - loss: 4.3740 - acc: 0.066 - ETA: 2:53 - loss: 4.3703 - acc: 0.067 - ETA: 2:52 - loss: 4.3680 - acc: 0.066 - ETA: 2:51 - loss: 4.3658 - acc: 0.067 - ETA: 2:50 - loss: 4.3686 - acc: 0.067 - ETA: 2:49 - loss: 4.3717 - acc: 0.066 - ETA: 2:48 - loss: 4.3712 - acc: 0.066 - ETA: 2:48 - loss: 4.3696 - acc: 0.066 - ETA: 2:47 - loss: 4.3679 - acc: 0.066 - ETA: 2:46 - loss: 4.3650 - acc: 0.066 - ETA: 2:45 - loss: 4.3643 - acc: 0.065 - ETA: 2:44 - loss: 4.3628 - acc: 0.065 - ETA: 2:43 - loss: 4.3616 - acc: 0.066 - ETA: 2:42 - loss: 4.3608 - acc: 0.067 - ETA: 2:42 - loss: 4.3621 - acc: 0.066 - ETA: 2:41 - loss: 4.3614 - acc: 0.067 - ETA: 2:40 - loss: 4.3607 - acc: 0.068 - ETA: 2:39 - loss: 4.3620 - acc: 0.067 - ETA: 2:38 - loss: 4.3617 - acc: 0.068 - ETA: 2:37 - loss: 4.3616 - acc: 0.068 - ETA: 2:36 - loss: 4.3608 - acc: 0.068 - ETA: 2:36 - loss: 4.3627 - acc: 0.067 - ETA: 2:35 - loss: 4.3593 - acc: 0.069 - ETA: 2:34 - loss: 4.3620 - acc: 0.068 - ETA: 2:33 - loss: 4.3620 - acc: 0.068 - ETA: 2:32 - loss: 4.3615 - acc: 0.068 - ETA: 2:31 - loss: 4.3614 - acc: 0.068 - ETA: 2:30 - loss: 4.3566 - acc: 0.068 - ETA: 2:30 - loss: 4.3568 - acc: 0.068 - ETA: 2:29 - loss: 4.3613 - acc: 0.067 - ETA: 2:28 - loss: 4.3601 - acc: 0.068 - ETA: 2:27 - loss: 4.3611 - acc: 0.068 - ETA: 2:26 - loss: 4.3612 - acc: 0.067 - ETA: 2:25 - loss: 4.3587 - acc: 0.067 - ETA: 2:25 - loss: 4.3595 - acc: 0.067 - ETA: 2:24 - loss: 4.3581 - acc: 0.067 - ETA: 2:23 - loss: 4.3596 - acc: 0.067 - ETA: 2:22 - loss: 4.3586 - acc: 0.067 - ETA: 2:21 - loss: 4.3578 - acc: 0.067 - ETA: 2:20 - loss: 4.3555 - acc: 0.067 - ETA: 2:19 - loss: 4.3585 - acc: 0.067 - ETA: 2:19 - loss: 4.3577 - acc: 0.067 - ETA: 2:18 - loss: 4.3597 - acc: 0.067 - ETA: 2:17 - loss: 4.3584 - acc: 0.067 - ETA: 2:16 - loss: 4.3556 - acc: 0.067 - ETA: 2:15 - loss: 4.3576 - acc: 0.067 - ETA: 2:14 - loss: 4.3562 - acc: 0.067 - ETA: 2:13 - loss: 4.3560 - acc: 0.067 - ETA: 2:13 - loss: 4.3543 - acc: 0.067 - ETA: 2:12 - loss: 4.3537 - acc: 0.068 - ETA: 2:11 - loss: 4.3540 - acc: 0.067 - ETA: 2:10 - loss: 4.3528 - acc: 0.067 - ETA: 2:09 - loss: 4.3525 - acc: 0.067 - ETA: 2:09 - loss: 4.3516 - acc: 0.067 - ETA: 2:08 - loss: 4.3502 - acc: 0.067 - ETA: 2:07 - loss: 4.3469 - acc: 0.067 - ETA: 2:06 - loss: 4.3469 - acc: 0.067 - ETA: 2:05 - loss: 4.3471 - acc: 0.068 - ETA: 2:04 - loss: 4.3479 - acc: 0.067 - ETA: 2:04 - loss: 4.3465 - acc: 0.067 - ETA: 2:03 - loss: 4.3458 - acc: 0.067 - ETA: 2:02 - loss: 4.3421 - acc: 0.068 - ETA: 2:01 - loss: 4.3429 - acc: 0.068 - ETA: 2:00 - loss: 4.3436 - acc: 0.068 - ETA: 1:59 - loss: 4.3453 - acc: 0.068 - ETA: 1:59 - loss: 4.3444 - acc: 0.068 - ETA: 1:58 - loss: 4.3435 - acc: 0.068 - ETA: 1:57 - loss: 4.3401 - acc: 0.068 - ETA: 1:56 - loss: 4.3404 - acc: 0.069 - ETA: 1:55 - loss: 4.3400 - acc: 0.069 - ETA: 1:54 - loss: 4.3375 - acc: 0.068 - ETA: 1:53 - loss: 4.3372 - acc: 0.068 - ETA: 1:53 - loss: 4.3375 - acc: 0.068 - ETA: 1:52 - loss: 4.3369 - acc: 0.068 - ETA: 1:51 - loss: 4.3344 - acc: 0.068 - ETA: 1:50 - loss: 4.3334 - acc: 0.068 - ETA: 1:49 - loss: 4.3316 - acc: 0.0686"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1:48 - loss: 4.3298 - acc: 0.069 - ETA: 1:48 - loss: 4.3288 - acc: 0.068 - ETA: 1:47 - loss: 4.3268 - acc: 0.068 - ETA: 1:46 - loss: 4.3268 - acc: 0.068 - ETA: 1:45 - loss: 4.3259 - acc: 0.068 - ETA: 1:44 - loss: 4.3256 - acc: 0.068 - ETA: 1:43 - loss: 4.3251 - acc: 0.068 - ETA: 1:42 - loss: 4.3245 - acc: 0.068 - ETA: 1:42 - loss: 4.3248 - acc: 0.068 - ETA: 1:41 - loss: 4.3245 - acc: 0.068 - ETA: 1:40 - loss: 4.3249 - acc: 0.068 - ETA: 1:39 - loss: 4.3262 - acc: 0.068 - ETA: 1:38 - loss: 4.3251 - acc: 0.069 - ETA: 1:37 - loss: 4.3271 - acc: 0.069 - ETA: 1:37 - loss: 4.3269 - acc: 0.069 - ETA: 1:36 - loss: 4.3268 - acc: 0.069 - ETA: 1:35 - loss: 4.3254 - acc: 0.069 - ETA: 1:34 - loss: 4.3258 - acc: 0.068 - ETA: 1:33 - loss: 4.3264 - acc: 0.068 - ETA: 1:32 - loss: 4.3251 - acc: 0.068 - ETA: 1:31 - loss: 4.3249 - acc: 0.068 - ETA: 1:31 - loss: 4.3250 - acc: 0.068 - ETA: 1:30 - loss: 4.3252 - acc: 0.068 - ETA: 1:29 - loss: 4.3236 - acc: 0.069 - ETA: 1:28 - loss: 4.3227 - acc: 0.069 - ETA: 1:27 - loss: 4.3228 - acc: 0.069 - ETA: 1:26 - loss: 4.3230 - acc: 0.069 - ETA: 1:26 - loss: 4.3220 - acc: 0.069 - ETA: 1:25 - loss: 4.3196 - acc: 0.070 - ETA: 1:24 - loss: 4.3219 - acc: 0.070 - ETA: 1:23 - loss: 4.3229 - acc: 0.070 - ETA: 1:22 - loss: 4.3238 - acc: 0.070 - ETA: 1:21 - loss: 4.3232 - acc: 0.070 - ETA: 1:20 - loss: 4.3234 - acc: 0.069 - ETA: 1:20 - loss: 4.3223 - acc: 0.069 - ETA: 1:19 - loss: 4.3211 - acc: 0.069 - ETA: 1:18 - loss: 4.3214 - acc: 0.069 - ETA: 1:17 - loss: 4.3197 - acc: 0.069 - ETA: 1:16 - loss: 4.3191 - acc: 0.069 - ETA: 1:15 - loss: 4.3187 - acc: 0.069 - ETA: 1:15 - loss: 4.3181 - acc: 0.069 - ETA: 1:14 - loss: 4.3188 - acc: 0.069 - ETA: 1:13 - loss: 4.3182 - acc: 0.069 - ETA: 1:12 - loss: 4.3172 - acc: 0.069 - ETA: 1:11 - loss: 4.3173 - acc: 0.069 - ETA: 1:10 - loss: 4.3166 - acc: 0.070 - ETA: 1:09 - loss: 4.3160 - acc: 0.069 - ETA: 1:09 - loss: 4.3156 - acc: 0.069 - ETA: 1:08 - loss: 4.3151 - acc: 0.069 - ETA: 1:07 - loss: 4.3150 - acc: 0.069 - ETA: 1:06 - loss: 4.3150 - acc: 0.069 - ETA: 1:05 - loss: 4.3155 - acc: 0.068 - ETA: 1:04 - loss: 4.3153 - acc: 0.069 - ETA: 1:04 - loss: 4.3147 - acc: 0.068 - ETA: 1:03 - loss: 4.3128 - acc: 0.069 - ETA: 1:02 - loss: 4.3130 - acc: 0.068 - ETA: 1:01 - loss: 4.3107 - acc: 0.069 - ETA: 1:00 - loss: 4.3120 - acc: 0.068 - ETA: 59s - loss: 4.3120 - acc: 0.068 - ETA: 58s - loss: 4.3103 - acc: 0.06 - ETA: 58s - loss: 4.3082 - acc: 0.06 - ETA: 57s - loss: 4.3085 - acc: 0.06 - ETA: 56s - loss: 4.3090 - acc: 0.06 - ETA: 55s - loss: 4.3091 - acc: 0.06 - ETA: 54s - loss: 4.3077 - acc: 0.06 - ETA: 53s - loss: 4.3069 - acc: 0.06 - ETA: 53s - loss: 4.3059 - acc: 0.06 - ETA: 52s - loss: 4.3042 - acc: 0.06 - ETA: 51s - loss: 4.3049 - acc: 0.06 - ETA: 50s - loss: 4.3032 - acc: 0.07 - ETA: 49s - loss: 4.3028 - acc: 0.06 - ETA: 48s - loss: 4.3016 - acc: 0.06 - ETA: 47s - loss: 4.3003 - acc: 0.06 - ETA: 47s - loss: 4.2990 - acc: 0.06 - ETA: 46s - loss: 4.2979 - acc: 0.06 - ETA: 45s - loss: 4.2979 - acc: 0.07 - ETA: 44s - loss: 4.2991 - acc: 0.06 - ETA: 43s - loss: 4.2993 - acc: 0.06 - ETA: 42s - loss: 4.3011 - acc: 0.06 - ETA: 42s - loss: 4.2994 - acc: 0.06 - ETA: 41s - loss: 4.3004 - acc: 0.06 - ETA: 40s - loss: 4.3003 - acc: 0.06 - ETA: 39s - loss: 4.2994 - acc: 0.07 - ETA: 38s - loss: 4.2982 - acc: 0.07 - ETA: 37s - loss: 4.2992 - acc: 0.07 - ETA: 37s - loss: 4.2987 - acc: 0.07 - ETA: 36s - loss: 4.2973 - acc: 0.07 - ETA: 35s - loss: 4.2975 - acc: 0.07 - ETA: 34s - loss: 4.2981 - acc: 0.07 - ETA: 33s - loss: 4.2972 - acc: 0.07 - ETA: 32s - loss: 4.2953 - acc: 0.07 - ETA: 31s - loss: 4.2951 - acc: 0.07 - ETA: 31s - loss: 4.2942 - acc: 0.07 - ETA: 30s - loss: 4.2943 - acc: 0.07 - ETA: 29s - loss: 4.2931 - acc: 0.07 - ETA: 28s - loss: 4.2928 - acc: 0.07 - ETA: 27s - loss: 4.2928 - acc: 0.07 - ETA: 26s - loss: 4.2926 - acc: 0.07 - ETA: 26s - loss: 4.2939 - acc: 0.07 - ETA: 25s - loss: 4.2917 - acc: 0.07 - ETA: 24s - loss: 4.2914 - acc: 0.07 - ETA: 23s - loss: 4.2931 - acc: 0.07 - ETA: 22s - loss: 4.2929 - acc: 0.07 - ETA: 21s - loss: 4.2919 - acc: 0.07 - ETA: 21s - loss: 4.2905 - acc: 0.07 - ETA: 20s - loss: 4.2890 - acc: 0.07 - ETA: 19s - loss: 4.2887 - acc: 0.07 - ETA: 18s - loss: 4.2872 - acc: 0.07 - ETA: 17s - loss: 4.2865 - acc: 0.07 - ETA: 16s - loss: 4.2857 - acc: 0.07 - ETA: 15s - loss: 4.2858 - acc: 0.07 - ETA: 15s - loss: 4.2862 - acc: 0.07 - ETA: 14s - loss: 4.2859 - acc: 0.07 - ETA: 13s - loss: 4.2849 - acc: 0.07 - ETA: 12s - loss: 4.2849 - acc: 0.07 - ETA: 11s - loss: 4.2842 - acc: 0.07 - ETA: 10s - loss: 4.2817 - acc: 0.07 - ETA: 10s - loss: 4.2820 - acc: 0.07 - ETA: 9s - loss: 4.2809 - acc: 0.0709 - ETA: 8s - loss: 4.2816 - acc: 0.070 - ETA: 7s - loss: 4.2815 - acc: 0.070 - ETA: 6s - loss: 4.2815 - acc: 0.070 - ETA: 5s - loss: 4.2806 - acc: 0.070 - ETA: 5s - loss: 4.2783 - acc: 0.071 - ETA: 4s - loss: 4.2777 - acc: 0.071 - ETA: 3s - loss: 4.2767 - acc: 0.071 - ETA: 2s - loss: 4.2755 - acc: 0.072 - ETA: 1s - loss: 4.2766 - acc: 0.072 - ETA: 0s - loss: 4.2756 - acc: 0.072 - 290s 43ms/step - loss: 4.2737 - acc: 0.0723 - val_loss: 4.2096 - val_acc: 0.0719\n",
      "\n",
      "Epoch 00002: val_loss improved from 4.59629 to 4.20959, saving model to saved_models/weights.best.from_scratch.hdf5\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 4:59 - loss: 4.0232 - acc: 0.150 - ETA: 4:45 - loss: 3.6539 - acc: 0.150 - ETA: 4:42 - loss: 3.5849 - acc: 0.166 - ETA: 4:39 - loss: 3.5977 - acc: 0.187 - ETA: 4:37 - loss: 3.5796 - acc: 0.220 - ETA: 4:36 - loss: 3.5340 - acc: 0.233 - ETA: 4:34 - loss: 3.5527 - acc: 0.214 - ETA: 4:34 - loss: 3.5116 - acc: 0.212 - ETA: 4:33 - loss: 3.5304 - acc: 0.200 - ETA: 4:31 - loss: 3.5328 - acc: 0.195 - ETA: 4:30 - loss: 3.4277 - acc: 0.218 - ETA: 4:30 - loss: 3.4458 - acc: 0.216 - ETA: 4:29 - loss: 3.4725 - acc: 0.215 - ETA: 4:29 - loss: 3.4985 - acc: 0.203 - ETA: 4:28 - loss: 3.4703 - acc: 0.210 - ETA: 4:27 - loss: 3.4284 - acc: 0.218 - ETA: 4:26 - loss: 3.4307 - acc: 0.211 - ETA: 4:25 - loss: 3.4665 - acc: 0.211 - ETA: 4:24 - loss: 3.4473 - acc: 0.207 - ETA: 4:23 - loss: 3.4299 - acc: 0.210 - ETA: 4:22 - loss: 3.4225 - acc: 0.211 - ETA: 4:21 - loss: 3.4350 - acc: 0.209 - ETA: 4:20 - loss: 3.4106 - acc: 0.215 - ETA: 4:19 - loss: 3.4411 - acc: 0.210 - ETA: 4:18 - loss: 3.4841 - acc: 0.204 - ETA: 4:17 - loss: 3.4554 - acc: 0.211 - ETA: 4:16 - loss: 3.4507 - acc: 0.211 - ETA: 4:15 - loss: 3.4419 - acc: 0.210 - ETA: 4:15 - loss: 3.4392 - acc: 0.208 - ETA: 4:14 - loss: 3.4482 - acc: 0.205 - ETA: 4:13 - loss: 3.4462 - acc: 0.201 - ETA: 4:12 - loss: 3.4239 - acc: 0.210 - ETA: 4:11 - loss: 3.4179 - acc: 0.212 - ETA: 4:10 - loss: 3.4193 - acc: 0.213 - ETA: 4:10 - loss: 3.4147 - acc: 0.212 - ETA: 4:09 - loss: 3.3991 - acc: 0.216 - ETA: 4:08 - loss: 3.4035 - acc: 0.214 - ETA: 4:07 - loss: 3.4107 - acc: 0.214 - ETA: 4:06 - loss: 3.3979 - acc: 0.215 - ETA: 4:05 - loss: 3.3865 - acc: 0.221 - ETA: 4:04 - loss: 3.3967 - acc: 0.218 - ETA: 4:04 - loss: 3.4071 - acc: 0.216 - ETA: 4:03 - loss: 3.4117 - acc: 0.215 - ETA: 4:02 - loss: 3.4182 - acc: 0.212 - ETA: 4:01 - loss: 3.4239 - acc: 0.211 - ETA: 4:00 - loss: 3.4242 - acc: 0.209 - ETA: 3:59 - loss: 3.4254 - acc: 0.208 - ETA: 3:58 - loss: 3.4198 - acc: 0.209 - ETA: 3:58 - loss: 3.4096 - acc: 0.211 - ETA: 3:57 - loss: 3.4104 - acc: 0.209 - ETA: 3:56 - loss: 3.3980 - acc: 0.210 - ETA: 3:55 - loss: 3.4038 - acc: 0.210 - ETA: 3:54 - loss: 3.3954 - acc: 0.214 - ETA: 3:53 - loss: 3.4028 - acc: 0.213 - ETA: 3:53 - loss: 3.3932 - acc: 0.215 - ETA: 3:52 - loss: 3.3941 - acc: 0.215 - ETA: 3:51 - loss: 3.3935 - acc: 0.215 - ETA: 3:50 - loss: 3.3833 - acc: 0.216 - ETA: 3:49 - loss: 3.3794 - acc: 0.216 - ETA: 3:48 - loss: 3.3747 - acc: 0.217 - ETA: 3:48 - loss: 3.3813 - acc: 0.216 - ETA: 3:47 - loss: 3.3898 - acc: 0.215 - ETA: 3:46 - loss: 3.3902 - acc: 0.215 - ETA: 3:45 - loss: 3.3877 - acc: 0.214 - ETA: 3:44 - loss: 3.3811 - acc: 0.216 - ETA: 3:43 - loss: 3.3766 - acc: 0.218 - ETA: 3:43 - loss: 3.3737 - acc: 0.218 - ETA: 3:42 - loss: 3.3641 - acc: 0.219 - ETA: 3:41 - loss: 3.3622 - acc: 0.220 - ETA: 3:40 - loss: 3.3659 - acc: 0.220 - ETA: 3:39 - loss: 3.3646 - acc: 0.221 - ETA: 3:38 - loss: 3.3635 - acc: 0.222 - ETA: 3:38 - loss: 3.3630 - acc: 0.222 - ETA: 3:37 - loss: 3.3656 - acc: 0.224 - ETA: 3:36 - loss: 3.3598 - acc: 0.226 - ETA: 3:35 - loss: 3.3530 - acc: 0.229 - ETA: 3:34 - loss: 3.3518 - acc: 0.229 - ETA: 3:33 - loss: 3.3465 - acc: 0.229 - ETA: 3:33 - loss: 3.3448 - acc: 0.229 - ETA: 3:32 - loss: 3.3475 - acc: 0.228 - ETA: 3:31 - loss: 3.3471 - acc: 0.227 - ETA: 3:30 - loss: 3.3509 - acc: 0.226 - ETA: 3:29 - loss: 3.3569 - acc: 0.226 - ETA: 3:28 - loss: 3.3580 - acc: 0.226 - ETA: 3:28 - loss: 3.3566 - acc: 0.224 - ETA: 3:27 - loss: 3.3541 - acc: 0.225 - ETA: 3:26 - loss: 3.3500 - acc: 0.224 - ETA: 3:25 - loss: 3.3459 - acc: 0.225 - ETA: 3:24 - loss: 3.3441 - acc: 0.226 - ETA: 3:23 - loss: 3.3400 - acc: 0.226 - ETA: 3:22 - loss: 3.3354 - acc: 0.226 - ETA: 3:22 - loss: 3.3326 - acc: 0.226 - ETA: 3:21 - loss: 3.3266 - acc: 0.226 - ETA: 3:20 - loss: 3.3275 - acc: 0.225 - ETA: 3:19 - loss: 3.3221 - acc: 0.227 - ETA: 3:18 - loss: 3.3206 - acc: 0.229 - ETA: 3:18 - loss: 3.3203 - acc: 0.228 - ETA: 3:17 - loss: 3.3172 - acc: 0.229 - ETA: 3:16 - loss: 3.3196 - acc: 0.229 - ETA: 3:15 - loss: 3.3227 - acc: 0.230 - ETA: 3:14 - loss: 3.3239 - acc: 0.229 - ETA: 3:13 - loss: 3.3233 - acc: 0.229 - ETA: 3:13 - loss: 3.3237 - acc: 0.229 - ETA: 3:12 - loss: 3.3201 - acc: 0.230 - ETA: 3:11 - loss: 3.3226 - acc: 0.230 - ETA: 3:10 - loss: 3.3224 - acc: 0.230 - ETA: 3:09 - loss: 3.3204 - acc: 0.230 - ETA: 3:08 - loss: 3.3221 - acc: 0.230 - ETA: 3:08 - loss: 3.3204 - acc: 0.232 - ETA: 3:07 - loss: 3.3189 - acc: 0.232 - ETA: 3:06 - loss: 3.3203 - acc: 0.231 - ETA: 3:05 - loss: 3.3240 - acc: 0.229 - ETA: 3:04 - loss: 3.3265 - acc: 0.229 - ETA: 3:03 - loss: 3.3265 - acc: 0.229 - ETA: 3:03 - loss: 3.3251 - acc: 0.230 - ETA: 3:02 - loss: 3.3245 - acc: 0.231 - ETA: 3:01 - loss: 3.3253 - acc: 0.229 - ETA: 3:00 - loss: 3.3219 - acc: 0.229 - ETA: 2:59 - loss: 3.3181 - acc: 0.229 - ETA: 2:58 - loss: 3.3236 - acc: 0.229 - ETA: 2:58 - loss: 3.3235 - acc: 0.229 - ETA: 2:57 - loss: 3.3199 - acc: 0.229 - ETA: 2:56 - loss: 3.3205 - acc: 0.228 - ETA: 2:55 - loss: 3.3161 - acc: 0.230 - ETA: 2:54 - loss: 3.3195 - acc: 0.229 - ETA: 2:53 - loss: 3.3177 - acc: 0.229 - ETA: 2:53 - loss: 3.3181 - acc: 0.230 - ETA: 2:52 - loss: 3.3158 - acc: 0.230 - ETA: 2:51 - loss: 3.3176 - acc: 0.229 - ETA: 2:50 - loss: 3.3181 - acc: 0.228 - ETA: 2:49 - loss: 3.3161 - acc: 0.228 - ETA: 2:48 - loss: 3.3167 - acc: 0.228 - ETA: 2:48 - loss: 3.3172 - acc: 0.228 - ETA: 2:47 - loss: 3.3144 - acc: 0.228 - ETA: 2:46 - loss: 3.3166 - acc: 0.228 - ETA: 2:45 - loss: 3.3130 - acc: 0.228 - ETA: 2:44 - loss: 3.3129 - acc: 0.228 - ETA: 2:43 - loss: 3.3172 - acc: 0.228 - ETA: 2:43 - loss: 3.3170 - acc: 0.229 - ETA: 2:42 - loss: 3.3161 - acc: 0.229 - ETA: 2:41 - loss: 3.3130 - acc: 0.229 - ETA: 2:40 - loss: 3.3099 - acc: 0.229 - ETA: 2:39 - loss: 3.3088 - acc: 0.230 - ETA: 2:38 - loss: 3.3065 - acc: 0.231 - ETA: 2:37 - loss: 3.3062 - acc: 0.231 - ETA: 2:37 - loss: 3.3089 - acc: 0.231 - ETA: 2:36 - loss: 3.3122 - acc: 0.231 - ETA: 2:35 - loss: 3.3147 - acc: 0.230 - ETA: 2:34 - loss: 3.3134 - acc: 0.230 - ETA: 2:33 - loss: 3.3145 - acc: 0.230 - ETA: 2:32 - loss: 3.3165 - acc: 0.230 - ETA: 2:32 - loss: 3.3187 - acc: 0.229 - ETA: 2:31 - loss: 3.3152 - acc: 0.230 - ETA: 2:30 - loss: 3.3131 - acc: 0.231 - ETA: 2:29 - loss: 3.3143 - acc: 0.231 - ETA: 2:28 - loss: 3.3139 - acc: 0.231 - ETA: 2:27 - loss: 3.3104 - acc: 0.231 - ETA: 2:27 - loss: 3.3111 - acc: 0.231 - ETA: 2:26 - loss: 3.3109 - acc: 0.231 - ETA: 2:25 - loss: 3.3162 - acc: 0.230 - ETA: 2:24 - loss: 3.3165 - acc: 0.230 - ETA: 2:23 - loss: 3.3200 - acc: 0.229 - ETA: 2:22 - loss: 3.3212 - acc: 0.230 - ETA: 2:22 - loss: 3.3210 - acc: 0.229 - ETA: 2:21 - loss: 3.3170 - acc: 0.230 - ETA: 2:20 - loss: 3.3187 - acc: 0.230 - ETA: 2:19 - loss: 3.3168 - acc: 0.231 - ETA: 2:18 - loss: 3.3162 - acc: 0.231 - ETA: 2:17 - loss: 3.3163 - acc: 0.231 - ETA: 2:17 - loss: 3.3159 - acc: 0.231 - ETA: 2:16 - loss: 3.3122 - acc: 0.232 - ETA: 2:15 - loss: 3.3103 - acc: 0.232 - ETA: 2:14 - loss: 3.3093 - acc: 0.232 - ETA: 2:13 - loss: 3.3089 - acc: 0.232 - ETA: 2:12 - loss: 3.3093 - acc: 0.232 - ETA: 2:12 - loss: 3.3079 - acc: 0.233 - ETA: 2:11 - loss: 3.3088 - acc: 0.232 - ETA: 2:10 - loss: 3.3052 - acc: 0.233 - ETA: 2:09 - loss: 3.3069 - acc: 0.233 - ETA: 2:08 - loss: 3.3042 - acc: 0.233 - ETA: 2:07 - loss: 3.3036 - acc: 0.234 - ETA: 2:07 - loss: 3.3044 - acc: 0.234 - ETA: 2:06 - loss: 3.3051 - acc: 0.233 - ETA: 2:05 - loss: 3.3039 - acc: 0.233 - ETA: 2:04 - loss: 3.3043 - acc: 0.232 - ETA: 2:03 - loss: 3.3047 - acc: 0.233 - ETA: 2:02 - loss: 3.3082 - acc: 0.232 - ETA: 2:02 - loss: 3.3066 - acc: 0.232 - ETA: 2:01 - loss: 3.3057 - acc: 0.233 - ETA: 2:00 - loss: 3.3116 - acc: 0.233 - ETA: 1:59 - loss: 3.3126 - acc: 0.234 - ETA: 1:58 - loss: 3.3131 - acc: 0.233 - ETA: 1:57 - loss: 3.3131 - acc: 0.233 - ETA: 1:57 - loss: 3.3116 - acc: 0.234 - ETA: 1:56 - loss: 3.3130 - acc: 0.233 - ETA: 1:55 - loss: 3.3140 - acc: 0.233 - ETA: 1:54 - loss: 3.3138 - acc: 0.233 - ETA: 1:53 - loss: 3.3114 - acc: 0.234 - ETA: 1:52 - loss: 3.3079 - acc: 0.234 - ETA: 1:51 - loss: 3.3097 - acc: 0.234 - ETA: 1:51 - loss: 3.3102 - acc: 0.234 - ETA: 1:50 - loss: 3.3098 - acc: 0.234 - ETA: 1:49 - loss: 3.3092 - acc: 0.235 - ETA: 1:48 - loss: 3.3078 - acc: 0.2350"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1:47 - loss: 3.3069 - acc: 0.235 - ETA: 1:46 - loss: 3.3088 - acc: 0.235 - ETA: 1:46 - loss: 3.3084 - acc: 0.235 - ETA: 1:45 - loss: 3.3095 - acc: 0.235 - ETA: 1:44 - loss: 3.3066 - acc: 0.235 - ETA: 1:43 - loss: 3.3037 - acc: 0.235 - ETA: 1:42 - loss: 3.3028 - acc: 0.235 - ETA: 1:41 - loss: 3.3014 - acc: 0.235 - ETA: 1:41 - loss: 3.2969 - acc: 0.237 - ETA: 1:40 - loss: 3.2968 - acc: 0.237 - ETA: 1:39 - loss: 3.2988 - acc: 0.236 - ETA: 1:38 - loss: 3.2979 - acc: 0.236 - ETA: 1:37 - loss: 3.2995 - acc: 0.236 - ETA: 1:36 - loss: 3.3008 - acc: 0.237 - ETA: 1:36 - loss: 3.2998 - acc: 0.237 - ETA: 1:35 - loss: 3.3000 - acc: 0.237 - ETA: 1:34 - loss: 3.2979 - acc: 0.238 - ETA: 1:33 - loss: 3.3011 - acc: 0.238 - ETA: 1:32 - loss: 3.3001 - acc: 0.238 - ETA: 1:31 - loss: 3.2989 - acc: 0.238 - ETA: 1:31 - loss: 3.2970 - acc: 0.238 - ETA: 1:30 - loss: 3.2993 - acc: 0.237 - ETA: 1:29 - loss: 3.2967 - acc: 0.238 - ETA: 1:28 - loss: 3.2970 - acc: 0.239 - ETA: 1:27 - loss: 3.2960 - acc: 0.239 - ETA: 1:26 - loss: 3.2983 - acc: 0.238 - ETA: 1:26 - loss: 3.2968 - acc: 0.239 - ETA: 1:25 - loss: 3.2964 - acc: 0.238 - ETA: 1:24 - loss: 3.2985 - acc: 0.238 - ETA: 1:23 - loss: 3.2977 - acc: 0.238 - ETA: 1:22 - loss: 3.2984 - acc: 0.239 - ETA: 1:21 - loss: 3.2982 - acc: 0.239 - ETA: 1:21 - loss: 3.2975 - acc: 0.239 - ETA: 1:20 - loss: 3.2989 - acc: 0.239 - ETA: 1:19 - loss: 3.2981 - acc: 0.239 - ETA: 1:18 - loss: 3.3031 - acc: 0.239 - ETA: 1:17 - loss: 3.3016 - acc: 0.239 - ETA: 1:16 - loss: 3.3028 - acc: 0.239 - ETA: 1:16 - loss: 3.3029 - acc: 0.239 - ETA: 1:15 - loss: 3.3025 - acc: 0.239 - ETA: 1:14 - loss: 3.3003 - acc: 0.239 - ETA: 1:13 - loss: 3.3013 - acc: 0.239 - ETA: 1:12 - loss: 3.3004 - acc: 0.239 - ETA: 1:11 - loss: 3.3010 - acc: 0.239 - ETA: 1:11 - loss: 3.3012 - acc: 0.239 - ETA: 1:10 - loss: 3.3028 - acc: 0.239 - ETA: 1:09 - loss: 3.3003 - acc: 0.239 - ETA: 1:08 - loss: 3.3003 - acc: 0.239 - ETA: 1:07 - loss: 3.3017 - acc: 0.238 - ETA: 1:06 - loss: 3.3007 - acc: 0.239 - ETA: 1:05 - loss: 3.3001 - acc: 0.239 - ETA: 1:05 - loss: 3.3005 - acc: 0.239 - ETA: 1:04 - loss: 3.2979 - acc: 0.240 - ETA: 1:03 - loss: 3.2985 - acc: 0.239 - ETA: 1:02 - loss: 3.2982 - acc: 0.239 - ETA: 1:01 - loss: 3.2964 - acc: 0.239 - ETA: 1:00 - loss: 3.2947 - acc: 0.239 - ETA: 1:00 - loss: 3.2958 - acc: 0.239 - ETA: 59s - loss: 3.2960 - acc: 0.239 - ETA: 58s - loss: 3.2976 - acc: 0.23 - ETA: 57s - loss: 3.2980 - acc: 0.23 - ETA: 56s - loss: 3.2990 - acc: 0.23 - ETA: 55s - loss: 3.3014 - acc: 0.23 - ETA: 55s - loss: 3.3008 - acc: 0.23 - ETA: 54s - loss: 3.3013 - acc: 0.23 - ETA: 53s - loss: 3.3000 - acc: 0.23 - ETA: 52s - loss: 3.3007 - acc: 0.23 - ETA: 51s - loss: 3.3020 - acc: 0.23 - ETA: 50s - loss: 3.3030 - acc: 0.23 - ETA: 50s - loss: 3.3010 - acc: 0.23 - ETA: 49s - loss: 3.2990 - acc: 0.24 - ETA: 48s - loss: 3.2985 - acc: 0.24 - ETA: 47s - loss: 3.3008 - acc: 0.23 - ETA: 46s - loss: 3.2987 - acc: 0.23 - ETA: 45s - loss: 3.2991 - acc: 0.23 - ETA: 45s - loss: 3.3002 - acc: 0.23 - ETA: 44s - loss: 3.3002 - acc: 0.23 - ETA: 43s - loss: 3.2986 - acc: 0.23 - ETA: 42s - loss: 3.2976 - acc: 0.23 - ETA: 41s - loss: 3.2991 - acc: 0.23 - ETA: 40s - loss: 3.3012 - acc: 0.23 - ETA: 40s - loss: 3.2990 - acc: 0.23 - ETA: 39s - loss: 3.2989 - acc: 0.23 - ETA: 38s - loss: 3.3003 - acc: 0.23 - ETA: 37s - loss: 3.3009 - acc: 0.23 - ETA: 36s - loss: 3.3000 - acc: 0.23 - ETA: 35s - loss: 3.3035 - acc: 0.23 - ETA: 35s - loss: 3.3030 - acc: 0.23 - ETA: 34s - loss: 3.3027 - acc: 0.23 - ETA: 33s - loss: 3.3029 - acc: 0.23 - ETA: 32s - loss: 3.3022 - acc: 0.23 - ETA: 31s - loss: 3.3012 - acc: 0.23 - ETA: 30s - loss: 3.3005 - acc: 0.23 - ETA: 30s - loss: 3.2979 - acc: 0.23 - ETA: 29s - loss: 3.2977 - acc: 0.23 - ETA: 28s - loss: 3.2981 - acc: 0.23 - ETA: 27s - loss: 3.2995 - acc: 0.23 - ETA: 26s - loss: 3.2998 - acc: 0.23 - ETA: 25s - loss: 3.3002 - acc: 0.23 - ETA: 25s - loss: 3.2988 - acc: 0.23 - ETA: 24s - loss: 3.2984 - acc: 0.23 - ETA: 23s - loss: 3.2985 - acc: 0.23 - ETA: 22s - loss: 3.2967 - acc: 0.23 - ETA: 21s - loss: 3.2956 - acc: 0.23 - ETA: 20s - loss: 3.2960 - acc: 0.23 - ETA: 20s - loss: 3.2949 - acc: 0.23 - ETA: 19s - loss: 3.2974 - acc: 0.23 - ETA: 18s - loss: 3.2965 - acc: 0.23 - ETA: 17s - loss: 3.2969 - acc: 0.23 - ETA: 16s - loss: 3.2978 - acc: 0.23 - ETA: 15s - loss: 3.2997 - acc: 0.23 - ETA: 15s - loss: 3.2992 - acc: 0.23 - ETA: 14s - loss: 3.2990 - acc: 0.23 - ETA: 13s - loss: 3.2992 - acc: 0.23 - ETA: 12s - loss: 3.2998 - acc: 0.23 - ETA: 11s - loss: 3.2987 - acc: 0.23 - ETA: 10s - loss: 3.2983 - acc: 0.23 - ETA: 10s - loss: 3.2991 - acc: 0.23 - ETA: 9s - loss: 3.2996 - acc: 0.2375 - ETA: 8s - loss: 3.2984 - acc: 0.237 - ETA: 7s - loss: 3.2985 - acc: 0.237 - ETA: 6s - loss: 3.2971 - acc: 0.237 - ETA: 5s - loss: 3.2969 - acc: 0.237 - ETA: 5s - loss: 3.2953 - acc: 0.237 - ETA: 4s - loss: 3.2962 - acc: 0.237 - ETA: 3s - loss: 3.2966 - acc: 0.237 - ETA: 2s - loss: 3.2956 - acc: 0.237 - ETA: 1s - loss: 3.2954 - acc: 0.237 - ETA: 0s - loss: 3.2943 - acc: 0.237 - 289s 43ms/step - loss: 3.2950 - acc: 0.2374 - val_loss: 4.3692 - val_acc: 0.0802\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 4.20959\n",
      "Epoch 4/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 4:54 - loss: 2.3708 - acc: 0.500 - ETA: 4:46 - loss: 2.0885 - acc: 0.500 - ETA: 4:41 - loss: 2.0400 - acc: 0.533 - ETA: 4:39 - loss: 2.0004 - acc: 0.537 - ETA: 4:37 - loss: 1.9568 - acc: 0.540 - ETA: 4:36 - loss: 1.8923 - acc: 0.541 - ETA: 4:36 - loss: 1.9277 - acc: 0.535 - ETA: 4:35 - loss: 1.8945 - acc: 0.531 - ETA: 4:34 - loss: 1.8390 - acc: 0.533 - ETA: 4:32 - loss: 1.8848 - acc: 0.515 - ETA: 4:31 - loss: 1.8522 - acc: 0.531 - ETA: 4:30 - loss: 1.8345 - acc: 0.537 - ETA: 4:29 - loss: 1.8132 - acc: 0.542 - ETA: 4:28 - loss: 1.7922 - acc: 0.550 - ETA: 4:27 - loss: 1.7869 - acc: 0.553 - ETA: 4:26 - loss: 1.7698 - acc: 0.556 - ETA: 4:25 - loss: 1.7609 - acc: 0.555 - ETA: 4:25 - loss: 1.7464 - acc: 0.561 - ETA: 4:24 - loss: 1.7714 - acc: 0.555 - ETA: 4:23 - loss: 1.7986 - acc: 0.547 - ETA: 4:22 - loss: 1.7948 - acc: 0.550 - ETA: 4:21 - loss: 1.7774 - acc: 0.559 - ETA: 4:20 - loss: 1.7474 - acc: 0.563 - ETA: 4:20 - loss: 1.7261 - acc: 0.572 - ETA: 4:19 - loss: 1.7181 - acc: 0.578 - ETA: 4:18 - loss: 1.7262 - acc: 0.576 - ETA: 4:17 - loss: 1.7367 - acc: 0.574 - ETA: 4:16 - loss: 1.7515 - acc: 0.569 - ETA: 4:15 - loss: 1.7736 - acc: 0.565 - ETA: 4:15 - loss: 1.7595 - acc: 0.570 - ETA: 4:14 - loss: 1.7443 - acc: 0.577 - ETA: 4:13 - loss: 1.7402 - acc: 0.576 - ETA: 4:12 - loss: 1.7421 - acc: 0.575 - ETA: 4:11 - loss: 1.7432 - acc: 0.573 - ETA: 4:10 - loss: 1.7484 - acc: 0.570 - ETA: 4:09 - loss: 1.7336 - acc: 0.575 - ETA: 4:09 - loss: 1.7315 - acc: 0.577 - ETA: 4:08 - loss: 1.7228 - acc: 0.577 - ETA: 4:07 - loss: 1.7120 - acc: 0.582 - ETA: 4:06 - loss: 1.7363 - acc: 0.577 - ETA: 4:05 - loss: 1.7523 - acc: 0.575 - ETA: 4:05 - loss: 1.7493 - acc: 0.578 - ETA: 4:04 - loss: 1.7582 - acc: 0.574 - ETA: 4:03 - loss: 1.7698 - acc: 0.569 - ETA: 4:02 - loss: 1.7773 - acc: 0.568 - ETA: 4:01 - loss: 1.7772 - acc: 0.570 - ETA: 4:00 - loss: 1.7706 - acc: 0.573 - ETA: 3:59 - loss: 1.7854 - acc: 0.568 - ETA: 3:59 - loss: 1.7816 - acc: 0.569 - ETA: 3:58 - loss: 1.7724 - acc: 0.570 - ETA: 3:57 - loss: 1.7697 - acc: 0.570 - ETA: 3:56 - loss: 1.7904 - acc: 0.565 - ETA: 3:55 - loss: 1.7896 - acc: 0.563 - ETA: 3:54 - loss: 1.7841 - acc: 0.563 - ETA: 3:53 - loss: 1.7824 - acc: 0.562 - ETA: 3:53 - loss: 1.7790 - acc: 0.562 - ETA: 3:52 - loss: 1.7723 - acc: 0.564 - ETA: 3:51 - loss: 1.7772 - acc: 0.562 - ETA: 3:50 - loss: 1.7680 - acc: 0.564 - ETA: 3:50 - loss: 1.7712 - acc: 0.563 - ETA: 3:49 - loss: 1.7652 - acc: 0.563 - ETA: 3:48 - loss: 1.7562 - acc: 0.566 - ETA: 3:47 - loss: 1.7444 - acc: 0.570 - ETA: 3:46 - loss: 1.7335 - acc: 0.572 - ETA: 3:45 - loss: 1.7318 - acc: 0.571 - ETA: 3:44 - loss: 1.7238 - acc: 0.572 - ETA: 3:43 - loss: 1.7120 - acc: 0.575 - ETA: 3:43 - loss: 1.7166 - acc: 0.575 - ETA: 3:42 - loss: 1.7207 - acc: 0.574 - ETA: 3:41 - loss: 1.7353 - acc: 0.571 - ETA: 3:40 - loss: 1.7378 - acc: 0.570 - ETA: 3:39 - loss: 1.7289 - acc: 0.571 - ETA: 3:38 - loss: 1.7317 - acc: 0.571 - ETA: 3:38 - loss: 1.7372 - acc: 0.568 - ETA: 3:37 - loss: 1.7317 - acc: 0.568 - ETA: 3:36 - loss: 1.7303 - acc: 0.569 - ETA: 3:35 - loss: 1.7285 - acc: 0.568 - ETA: 3:34 - loss: 1.7293 - acc: 0.567 - ETA: 3:33 - loss: 1.7291 - acc: 0.567 - ETA: 3:32 - loss: 1.7308 - acc: 0.567 - ETA: 3:32 - loss: 1.7287 - acc: 0.568 - ETA: 3:31 - loss: 1.7265 - acc: 0.570 - ETA: 3:30 - loss: 1.7244 - acc: 0.571 - ETA: 3:29 - loss: 1.7234 - acc: 0.570 - ETA: 3:28 - loss: 1.7220 - acc: 0.571 - ETA: 3:27 - loss: 1.7220 - acc: 0.570 - ETA: 3:27 - loss: 1.7263 - acc: 0.571 - ETA: 3:26 - loss: 1.7310 - acc: 0.570 - ETA: 3:25 - loss: 1.7272 - acc: 0.570 - ETA: 3:24 - loss: 1.7314 - acc: 0.570 - ETA: 3:23 - loss: 1.7320 - acc: 0.570 - ETA: 3:22 - loss: 1.7356 - acc: 0.571 - ETA: 3:22 - loss: 1.7403 - acc: 0.572 - ETA: 3:21 - loss: 1.7415 - acc: 0.571 - ETA: 3:20 - loss: 1.7405 - acc: 0.570 - ETA: 3:19 - loss: 1.7450 - acc: 0.568 - ETA: 3:18 - loss: 1.7476 - acc: 0.567 - ETA: 3:17 - loss: 1.7418 - acc: 0.568 - ETA: 3:16 - loss: 1.7369 - acc: 0.569 - ETA: 3:16 - loss: 1.7373 - acc: 0.568 - ETA: 3:15 - loss: 1.7351 - acc: 0.568 - ETA: 3:14 - loss: 1.7389 - acc: 0.568 - ETA: 3:13 - loss: 1.7381 - acc: 0.568 - ETA: 3:12 - loss: 1.7490 - acc: 0.566 - ETA: 3:11 - loss: 1.7466 - acc: 0.566 - ETA: 3:11 - loss: 1.7561 - acc: 0.565 - ETA: 3:10 - loss: 1.7504 - acc: 0.568 - ETA: 3:09 - loss: 1.7562 - acc: 0.568 - ETA: 3:08 - loss: 1.7614 - acc: 0.567 - ETA: 3:07 - loss: 1.7566 - acc: 0.567 - ETA: 3:06 - loss: 1.7573 - acc: 0.566 - ETA: 3:06 - loss: 1.7575 - acc: 0.565 - ETA: 3:05 - loss: 1.7586 - acc: 0.563 - ETA: 3:04 - loss: 1.7584 - acc: 0.563 - ETA: 3:03 - loss: 1.7565 - acc: 0.563 - ETA: 3:02 - loss: 1.7565 - acc: 0.564 - ETA: 3:01 - loss: 1.7550 - acc: 0.563 - ETA: 3:01 - loss: 1.7556 - acc: 0.564 - ETA: 3:00 - loss: 1.7570 - acc: 0.565 - ETA: 2:59 - loss: 1.7570 - acc: 0.566 - ETA: 2:58 - loss: 1.7593 - acc: 0.565 - ETA: 2:57 - loss: 1.7561 - acc: 0.565 - ETA: 2:57 - loss: 1.7576 - acc: 0.565 - ETA: 2:56 - loss: 1.7576 - acc: 0.564 - ETA: 2:55 - loss: 1.7554 - acc: 0.566 - ETA: 2:54 - loss: 1.7578 - acc: 0.564 - ETA: 2:53 - loss: 1.7549 - acc: 0.566 - ETA: 2:53 - loss: 1.7519 - acc: 0.566 - ETA: 2:52 - loss: 1.7528 - acc: 0.566 - ETA: 2:51 - loss: 1.7580 - acc: 0.564 - ETA: 2:50 - loss: 1.7653 - acc: 0.562 - ETA: 2:49 - loss: 1.7657 - acc: 0.562 - ETA: 2:48 - loss: 1.7677 - acc: 0.561 - ETA: 2:47 - loss: 1.7699 - acc: 0.560 - ETA: 2:47 - loss: 1.7743 - acc: 0.558 - ETA: 2:46 - loss: 1.7734 - acc: 0.559 - ETA: 2:45 - loss: 1.7770 - acc: 0.557 - ETA: 2:44 - loss: 1.7794 - acc: 0.557 - ETA: 2:43 - loss: 1.7790 - acc: 0.557 - ETA: 2:42 - loss: 1.7796 - acc: 0.557 - ETA: 2:42 - loss: 1.7778 - acc: 0.557 - ETA: 2:41 - loss: 1.7772 - acc: 0.557 - ETA: 2:40 - loss: 1.7781 - acc: 0.557 - ETA: 2:39 - loss: 1.7798 - acc: 0.556 - ETA: 2:38 - loss: 1.7751 - acc: 0.557 - ETA: 2:37 - loss: 1.7737 - acc: 0.557 - ETA: 2:36 - loss: 1.7718 - acc: 0.558 - ETA: 2:36 - loss: 1.7744 - acc: 0.557 - ETA: 2:35 - loss: 1.7742 - acc: 0.557 - ETA: 2:34 - loss: 1.7768 - acc: 0.556 - ETA: 2:33 - loss: 1.7810 - acc: 0.557 - ETA: 2:32 - loss: 1.7822 - acc: 0.556 - ETA: 2:31 - loss: 1.7826 - acc: 0.556 - ETA: 2:31 - loss: 1.7868 - acc: 0.556 - ETA: 2:30 - loss: 1.7882 - acc: 0.555 - ETA: 2:29 - loss: 1.7852 - acc: 0.556 - ETA: 2:28 - loss: 1.7813 - acc: 0.557 - ETA: 2:27 - loss: 1.7844 - acc: 0.555 - ETA: 2:27 - loss: 1.7829 - acc: 0.555 - ETA: 2:26 - loss: 1.7857 - acc: 0.554 - ETA: 2:25 - loss: 1.7812 - acc: 0.555 - ETA: 2:24 - loss: 1.7795 - acc: 0.555 - ETA: 2:23 - loss: 1.7786 - acc: 0.555 - ETA: 2:23 - loss: 1.7830 - acc: 0.555 - ETA: 2:22 - loss: 1.7824 - acc: 0.555 - ETA: 2:21 - loss: 1.7792 - acc: 0.555 - ETA: 2:20 - loss: 1.7787 - acc: 0.555 - ETA: 2:19 - loss: 1.7813 - acc: 0.555 - ETA: 2:18 - loss: 1.7784 - acc: 0.555 - ETA: 2:18 - loss: 1.7772 - acc: 0.555 - ETA: 2:17 - loss: 1.7773 - acc: 0.555 - ETA: 2:16 - loss: 1.7741 - acc: 0.555 - ETA: 2:15 - loss: 1.7731 - acc: 0.555 - ETA: 2:14 - loss: 1.7744 - acc: 0.555 - ETA: 2:14 - loss: 1.7763 - acc: 0.554 - ETA: 2:13 - loss: 1.7741 - acc: 0.555 - ETA: 2:12 - loss: 1.7763 - acc: 0.554 - ETA: 2:11 - loss: 1.7781 - acc: 0.554 - ETA: 2:10 - loss: 1.7790 - acc: 0.554 - ETA: 2:10 - loss: 1.7826 - acc: 0.553 - ETA: 2:09 - loss: 1.7825 - acc: 0.553 - ETA: 2:08 - loss: 1.7790 - acc: 0.554 - ETA: 2:07 - loss: 1.7792 - acc: 0.554 - ETA: 2:06 - loss: 1.7793 - acc: 0.553 - ETA: 2:05 - loss: 1.7817 - acc: 0.553 - ETA: 2:04 - loss: 1.7871 - acc: 0.553 - ETA: 2:04 - loss: 1.7870 - acc: 0.553 - ETA: 2:03 - loss: 1.7853 - acc: 0.554 - ETA: 2:02 - loss: 1.7873 - acc: 0.553 - ETA: 2:01 - loss: 1.7910 - acc: 0.552 - ETA: 2:00 - loss: 1.7917 - acc: 0.552 - ETA: 1:59 - loss: 1.7901 - acc: 0.553 - ETA: 1:59 - loss: 1.7897 - acc: 0.552 - ETA: 1:58 - loss: 1.7935 - acc: 0.552 - ETA: 1:57 - loss: 1.7967 - acc: 0.551 - ETA: 1:56 - loss: 1.7948 - acc: 0.552 - ETA: 1:55 - loss: 1.7920 - acc: 0.553 - ETA: 1:54 - loss: 1.7893 - acc: 0.553 - ETA: 1:53 - loss: 1.7896 - acc: 0.554 - ETA: 1:53 - loss: 1.7895 - acc: 0.553 - ETA: 1:52 - loss: 1.7866 - acc: 0.554 - ETA: 1:51 - loss: 1.7894 - acc: 0.553 - ETA: 1:50 - loss: 1.7927 - acc: 0.553 - ETA: 1:49 - loss: 1.7943 - acc: 0.5534"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1:48 - loss: 1.7962 - acc: 0.553 - ETA: 1:48 - loss: 1.7976 - acc: 0.552 - ETA: 1:47 - loss: 1.7988 - acc: 0.552 - ETA: 1:46 - loss: 1.7984 - acc: 0.552 - ETA: 1:45 - loss: 1.7960 - acc: 0.553 - ETA: 1:44 - loss: 1.7934 - acc: 0.554 - ETA: 1:43 - loss: 1.7951 - acc: 0.554 - ETA: 1:42 - loss: 1.7938 - acc: 0.554 - ETA: 1:42 - loss: 1.7931 - acc: 0.554 - ETA: 1:41 - loss: 1.7940 - acc: 0.554 - ETA: 1:40 - loss: 1.7949 - acc: 0.554 - ETA: 1:39 - loss: 1.7989 - acc: 0.554 - ETA: 1:38 - loss: 1.8017 - acc: 0.553 - ETA: 1:37 - loss: 1.8002 - acc: 0.553 - ETA: 1:37 - loss: 1.8020 - acc: 0.553 - ETA: 1:36 - loss: 1.8028 - acc: 0.553 - ETA: 1:35 - loss: 1.7991 - acc: 0.555 - ETA: 1:34 - loss: 1.8014 - acc: 0.554 - ETA: 1:33 - loss: 1.8011 - acc: 0.554 - ETA: 1:32 - loss: 1.7990 - acc: 0.554 - ETA: 1:31 - loss: 1.7985 - acc: 0.554 - ETA: 1:31 - loss: 1.8018 - acc: 0.554 - ETA: 1:30 - loss: 1.8008 - acc: 0.554 - ETA: 1:29 - loss: 1.7980 - acc: 0.555 - ETA: 1:28 - loss: 1.7989 - acc: 0.555 - ETA: 1:27 - loss: 1.8008 - acc: 0.554 - ETA: 1:26 - loss: 1.8014 - acc: 0.555 - ETA: 1:26 - loss: 1.8042 - acc: 0.554 - ETA: 1:25 - loss: 1.8043 - acc: 0.554 - ETA: 1:24 - loss: 1.8074 - acc: 0.553 - ETA: 1:23 - loss: 1.8062 - acc: 0.553 - ETA: 1:22 - loss: 1.8096 - acc: 0.553 - ETA: 1:21 - loss: 1.8080 - acc: 0.553 - ETA: 1:20 - loss: 1.8071 - acc: 0.553 - ETA: 1:20 - loss: 1.8074 - acc: 0.552 - ETA: 1:19 - loss: 1.8059 - acc: 0.552 - ETA: 1:18 - loss: 1.8039 - acc: 0.553 - ETA: 1:17 - loss: 1.8043 - acc: 0.553 - ETA: 1:16 - loss: 1.8028 - acc: 0.553 - ETA: 1:15 - loss: 1.8028 - acc: 0.553 - ETA: 1:15 - loss: 1.8064 - acc: 0.553 - ETA: 1:14 - loss: 1.8054 - acc: 0.553 - ETA: 1:13 - loss: 1.8034 - acc: 0.554 - ETA: 1:12 - loss: 1.8005 - acc: 0.554 - ETA: 1:11 - loss: 1.7993 - acc: 0.555 - ETA: 1:10 - loss: 1.7976 - acc: 0.555 - ETA: 1:09 - loss: 1.7980 - acc: 0.555 - ETA: 1:09 - loss: 1.7999 - acc: 0.555 - ETA: 1:08 - loss: 1.7996 - acc: 0.555 - ETA: 1:07 - loss: 1.8004 - acc: 0.555 - ETA: 1:06 - loss: 1.7988 - acc: 0.555 - ETA: 1:05 - loss: 1.7971 - acc: 0.555 - ETA: 1:04 - loss: 1.7970 - acc: 0.555 - ETA: 1:04 - loss: 1.7966 - acc: 0.555 - ETA: 1:03 - loss: 1.7968 - acc: 0.555 - ETA: 1:02 - loss: 1.7978 - acc: 0.555 - ETA: 1:01 - loss: 1.7976 - acc: 0.555 - ETA: 1:00 - loss: 1.7975 - acc: 0.555 - ETA: 59s - loss: 1.7990 - acc: 0.554 - ETA: 58s - loss: 1.7988 - acc: 0.55 - ETA: 58s - loss: 1.7992 - acc: 0.55 - ETA: 57s - loss: 1.8007 - acc: 0.55 - ETA: 56s - loss: 1.7987 - acc: 0.55 - ETA: 55s - loss: 1.8004 - acc: 0.55 - ETA: 54s - loss: 1.7985 - acc: 0.55 - ETA: 53s - loss: 1.7987 - acc: 0.55 - ETA: 53s - loss: 1.7968 - acc: 0.55 - ETA: 52s - loss: 1.7970 - acc: 0.55 - ETA: 51s - loss: 1.7941 - acc: 0.55 - ETA: 50s - loss: 1.7958 - acc: 0.55 - ETA: 49s - loss: 1.7946 - acc: 0.55 - ETA: 48s - loss: 1.7942 - acc: 0.55 - ETA: 48s - loss: 1.7942 - acc: 0.55 - ETA: 47s - loss: 1.7943 - acc: 0.55 - ETA: 46s - loss: 1.7959 - acc: 0.55 - ETA: 45s - loss: 1.7951 - acc: 0.55 - ETA: 44s - loss: 1.7951 - acc: 0.55 - ETA: 43s - loss: 1.7957 - acc: 0.55 - ETA: 42s - loss: 1.7963 - acc: 0.55 - ETA: 42s - loss: 1.7936 - acc: 0.55 - ETA: 41s - loss: 1.7933 - acc: 0.55 - ETA: 40s - loss: 1.7949 - acc: 0.55 - ETA: 39s - loss: 1.7944 - acc: 0.55 - ETA: 38s - loss: 1.7949 - acc: 0.55 - ETA: 37s - loss: 1.7993 - acc: 0.55 - ETA: 37s - loss: 1.7982 - acc: 0.55 - ETA: 36s - loss: 1.7991 - acc: 0.55 - ETA: 35s - loss: 1.7979 - acc: 0.55 - ETA: 34s - loss: 1.7978 - acc: 0.55 - ETA: 33s - loss: 1.7960 - acc: 0.55 - ETA: 32s - loss: 1.7969 - acc: 0.55 - ETA: 31s - loss: 1.8000 - acc: 0.55 - ETA: 31s - loss: 1.8014 - acc: 0.55 - ETA: 30s - loss: 1.7999 - acc: 0.55 - ETA: 29s - loss: 1.7991 - acc: 0.55 - ETA: 28s - loss: 1.7988 - acc: 0.55 - ETA: 27s - loss: 1.7989 - acc: 0.55 - ETA: 26s - loss: 1.7976 - acc: 0.55 - ETA: 26s - loss: 1.7950 - acc: 0.55 - ETA: 25s - loss: 1.7956 - acc: 0.55 - ETA: 24s - loss: 1.7974 - acc: 0.55 - ETA: 23s - loss: 1.7966 - acc: 0.55 - ETA: 22s - loss: 1.7964 - acc: 0.55 - ETA: 21s - loss: 1.7941 - acc: 0.55 - ETA: 21s - loss: 1.7923 - acc: 0.55 - ETA: 20s - loss: 1.7924 - acc: 0.55 - ETA: 19s - loss: 1.7906 - acc: 0.55 - ETA: 18s - loss: 1.7898 - acc: 0.55 - ETA: 17s - loss: 1.7899 - acc: 0.55 - ETA: 16s - loss: 1.7911 - acc: 0.55 - ETA: 15s - loss: 1.7923 - acc: 0.55 - ETA: 15s - loss: 1.7910 - acc: 0.55 - ETA: 14s - loss: 1.7908 - acc: 0.55 - ETA: 13s - loss: 1.7898 - acc: 0.55 - ETA: 12s - loss: 1.7913 - acc: 0.55 - ETA: 11s - loss: 1.7929 - acc: 0.55 - ETA: 10s - loss: 1.7918 - acc: 0.55 - ETA: 10s - loss: 1.7902 - acc: 0.55 - ETA: 9s - loss: 1.7915 - acc: 0.5557 - ETA: 8s - loss: 1.7908 - acc: 0.556 - ETA: 7s - loss: 1.7920 - acc: 0.555 - ETA: 6s - loss: 1.7914 - acc: 0.556 - ETA: 5s - loss: 1.7921 - acc: 0.556 - ETA: 5s - loss: 1.7935 - acc: 0.556 - ETA: 4s - loss: 1.7932 - acc: 0.556 - ETA: 3s - loss: 1.7955 - acc: 0.556 - ETA: 2s - loss: 1.7955 - acc: 0.556 - ETA: 1s - loss: 1.7949 - acc: 0.556 - ETA: 0s - loss: 1.7935 - acc: 0.556 - 290s 43ms/step - loss: 1.7922 - acc: 0.5570 - val_loss: 5.2499 - val_acc: 0.0898\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 4.20959\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4080/6680 [=================>............] - ETA: 4:41 - loss: 0.9481 - acc: 0.900 - ETA: 4:38 - loss: 0.8699 - acc: 0.900 - ETA: 4:38 - loss: 0.7958 - acc: 0.866 - ETA: 4:37 - loss: 0.8175 - acc: 0.862 - ETA: 4:36 - loss: 0.8700 - acc: 0.840 - ETA: 4:35 - loss: 0.8001 - acc: 0.858 - ETA: 4:34 - loss: 0.8482 - acc: 0.835 - ETA: 4:33 - loss: 0.7960 - acc: 0.843 - ETA: 4:32 - loss: 0.7775 - acc: 0.844 - ETA: 4:32 - loss: 0.7298 - acc: 0.850 - ETA: 4:31 - loss: 0.6964 - acc: 0.854 - ETA: 4:30 - loss: 0.7251 - acc: 0.833 - ETA: 4:29 - loss: 0.7082 - acc: 0.842 - ETA: 4:28 - loss: 0.6950 - acc: 0.842 - ETA: 4:27 - loss: 0.6836 - acc: 0.846 - ETA: 4:26 - loss: 0.6634 - acc: 0.850 - ETA: 4:25 - loss: 0.6605 - acc: 0.850 - ETA: 4:24 - loss: 0.6645 - acc: 0.844 - ETA: 4:23 - loss: 0.6626 - acc: 0.844 - ETA: 4:22 - loss: 0.6514 - acc: 0.847 - ETA: 4:21 - loss: 0.6531 - acc: 0.850 - ETA: 4:20 - loss: 0.6504 - acc: 0.850 - ETA: 4:20 - loss: 0.6472 - acc: 0.847 - ETA: 4:19 - loss: 0.6581 - acc: 0.839 - ETA: 4:18 - loss: 0.6597 - acc: 0.836 - ETA: 4:17 - loss: 0.6589 - acc: 0.832 - ETA: 4:16 - loss: 0.6603 - acc: 0.831 - ETA: 4:15 - loss: 0.6623 - acc: 0.832 - ETA: 4:15 - loss: 0.6638 - acc: 0.831 - ETA: 4:14 - loss: 0.6625 - acc: 0.828 - ETA: 4:13 - loss: 0.6529 - acc: 0.830 - ETA: 4:12 - loss: 0.6553 - acc: 0.829 - ETA: 4:11 - loss: 0.6533 - acc: 0.828 - ETA: 4:10 - loss: 0.6432 - acc: 0.830 - ETA: 4:09 - loss: 0.6531 - acc: 0.827 - ETA: 4:09 - loss: 0.6746 - acc: 0.822 - ETA: 4:08 - loss: 0.6775 - acc: 0.821 - ETA: 4:07 - loss: 0.6866 - acc: 0.817 - ETA: 4:06 - loss: 0.6829 - acc: 0.820 - ETA: 4:05 - loss: 0.6899 - acc: 0.818 - ETA: 4:04 - loss: 0.6795 - acc: 0.822 - ETA: 4:04 - loss: 0.6782 - acc: 0.823 - ETA: 4:03 - loss: 0.6861 - acc: 0.820 - ETA: 4:02 - loss: 0.6824 - acc: 0.820 - ETA: 4:01 - loss: 0.7024 - acc: 0.817 - ETA: 4:00 - loss: 0.6986 - acc: 0.818 - ETA: 3:59 - loss: 0.7006 - acc: 0.820 - ETA: 3:59 - loss: 0.7023 - acc: 0.818 - ETA: 3:58 - loss: 0.7039 - acc: 0.817 - ETA: 3:57 - loss: 0.7015 - acc: 0.819 - ETA: 3:56 - loss: 0.7031 - acc: 0.819 - ETA: 3:55 - loss: 0.6992 - acc: 0.822 - ETA: 3:54 - loss: 0.6915 - acc: 0.824 - ETA: 3:53 - loss: 0.6850 - acc: 0.826 - ETA: 3:53 - loss: 0.6836 - acc: 0.826 - ETA: 3:52 - loss: 0.6769 - acc: 0.827 - ETA: 3:51 - loss: 0.6756 - acc: 0.828 - ETA: 3:50 - loss: 0.6708 - acc: 0.829 - ETA: 3:49 - loss: 0.6685 - acc: 0.830 - ETA: 3:48 - loss: 0.6647 - acc: 0.831 - ETA: 3:47 - loss: 0.6613 - acc: 0.831 - ETA: 3:47 - loss: 0.6626 - acc: 0.829 - ETA: 3:46 - loss: 0.6687 - acc: 0.827 - ETA: 3:45 - loss: 0.6638 - acc: 0.828 - ETA: 3:44 - loss: 0.6820 - acc: 0.825 - ETA: 3:43 - loss: 0.6845 - acc: 0.823 - ETA: 3:42 - loss: 0.6810 - acc: 0.824 - ETA: 3:42 - loss: 0.6812 - acc: 0.823 - ETA: 3:41 - loss: 0.6796 - acc: 0.823 - ETA: 3:40 - loss: 0.6757 - acc: 0.825 - ETA: 3:39 - loss: 0.6750 - acc: 0.824 - ETA: 3:38 - loss: 0.6744 - acc: 0.823 - ETA: 3:37 - loss: 0.6724 - acc: 0.824 - ETA: 3:36 - loss: 0.6693 - acc: 0.825 - ETA: 3:36 - loss: 0.6678 - acc: 0.825 - ETA: 3:35 - loss: 0.6662 - acc: 0.825 - ETA: 3:34 - loss: 0.6692 - acc: 0.824 - ETA: 3:33 - loss: 0.6704 - acc: 0.825 - ETA: 3:32 - loss: 0.6650 - acc: 0.826 - ETA: 3:31 - loss: 0.6652 - acc: 0.826 - ETA: 3:31 - loss: 0.6630 - acc: 0.827 - ETA: 3:30 - loss: 0.6624 - acc: 0.826 - ETA: 3:29 - loss: 0.6599 - acc: 0.827 - ETA: 3:28 - loss: 0.6598 - acc: 0.828 - ETA: 3:27 - loss: 0.6686 - acc: 0.826 - ETA: 3:26 - loss: 0.6688 - acc: 0.825 - ETA: 3:26 - loss: 0.6695 - acc: 0.825 - ETA: 3:25 - loss: 0.6705 - acc: 0.825 - ETA: 3:24 - loss: 0.6687 - acc: 0.825 - ETA: 3:23 - loss: 0.6678 - acc: 0.825 - ETA: 3:22 - loss: 0.6701 - acc: 0.825 - ETA: 3:21 - loss: 0.6713 - acc: 0.825 - ETA: 3:21 - loss: 0.6650 - acc: 0.827 - ETA: 3:20 - loss: 0.6645 - acc: 0.828 - ETA: 3:19 - loss: 0.6648 - acc: 0.827 - ETA: 3:18 - loss: 0.6642 - acc: 0.828 - ETA: 3:17 - loss: 0.6642 - acc: 0.828 - ETA: 3:16 - loss: 0.6635 - acc: 0.829 - ETA: 3:15 - loss: 0.6683 - acc: 0.828 - ETA: 3:15 - loss: 0.6642 - acc: 0.829 - ETA: 3:14 - loss: 0.6645 - acc: 0.828 - ETA: 3:13 - loss: 0.6617 - acc: 0.828 - ETA: 3:12 - loss: 0.6625 - acc: 0.828 - ETA: 3:11 - loss: 0.6617 - acc: 0.828 - ETA: 3:11 - loss: 0.6590 - acc: 0.829 - ETA: 3:10 - loss: 0.6565 - acc: 0.830 - ETA: 3:09 - loss: 0.6549 - acc: 0.830 - ETA: 3:08 - loss: 0.6574 - acc: 0.829 - ETA: 3:07 - loss: 0.6574 - acc: 0.830 - ETA: 3:06 - loss: 0.6577 - acc: 0.830 - ETA: 3:06 - loss: 0.6550 - acc: 0.831 - ETA: 3:05 - loss: 0.6540 - acc: 0.832 - ETA: 3:04 - loss: 0.6587 - acc: 0.831 - ETA: 3:03 - loss: 0.6571 - acc: 0.832 - ETA: 3:02 - loss: 0.6603 - acc: 0.831 - ETA: 3:01 - loss: 0.6663 - acc: 0.829 - ETA: 3:01 - loss: 0.6645 - acc: 0.830 - ETA: 3:00 - loss: 0.6650 - acc: 0.830 - ETA: 2:59 - loss: 0.6639 - acc: 0.830 - ETA: 2:58 - loss: 0.6637 - acc: 0.830 - ETA: 2:57 - loss: 0.6638 - acc: 0.831 - ETA: 2:57 - loss: 0.6645 - acc: 0.830 - ETA: 2:56 - loss: 0.6641 - acc: 0.830 - ETA: 2:55 - loss: 0.6631 - acc: 0.830 - ETA: 2:54 - loss: 0.6610 - acc: 0.830 - ETA: 2:53 - loss: 0.6616 - acc: 0.830 - ETA: 2:52 - loss: 0.6614 - acc: 0.830 - ETA: 2:52 - loss: 0.6603 - acc: 0.830 - ETA: 2:51 - loss: 0.6591 - acc: 0.831 - ETA: 2:50 - loss: 0.6615 - acc: 0.830 - ETA: 2:49 - loss: 0.6623 - acc: 0.830 - ETA: 2:48 - loss: 0.6646 - acc: 0.830 - ETA: 2:47 - loss: 0.6621 - acc: 0.830 - ETA: 2:46 - loss: 0.6656 - acc: 0.829 - ETA: 2:46 - loss: 0.6636 - acc: 0.830 - ETA: 2:45 - loss: 0.6624 - acc: 0.829 - ETA: 2:44 - loss: 0.6599 - acc: 0.830 - ETA: 2:43 - loss: 0.6625 - acc: 0.829 - ETA: 2:42 - loss: 0.6635 - acc: 0.829 - ETA: 2:41 - loss: 0.6636 - acc: 0.830 - ETA: 2:41 - loss: 0.6641 - acc: 0.829 - ETA: 2:40 - loss: 0.6662 - acc: 0.828 - ETA: 2:39 - loss: 0.6667 - acc: 0.828 - ETA: 2:38 - loss: 0.6685 - acc: 0.827 - ETA: 2:37 - loss: 0.6680 - acc: 0.827 - ETA: 2:37 - loss: 0.6670 - acc: 0.828 - ETA: 2:36 - loss: 0.6669 - acc: 0.828 - ETA: 2:35 - loss: 0.6673 - acc: 0.828 - ETA: 2:34 - loss: 0.6679 - acc: 0.827 - ETA: 2:33 - loss: 0.6676 - acc: 0.827 - ETA: 2:32 - loss: 0.6677 - acc: 0.827 - ETA: 2:32 - loss: 0.6690 - acc: 0.827 - ETA: 2:31 - loss: 0.6687 - acc: 0.827 - ETA: 2:30 - loss: 0.6665 - acc: 0.827 - ETA: 2:29 - loss: 0.6683 - acc: 0.826 - ETA: 2:28 - loss: 0.6713 - acc: 0.826 - ETA: 2:27 - loss: 0.6713 - acc: 0.826 - ETA: 2:27 - loss: 0.6715 - acc: 0.826 - ETA: 2:26 - loss: 0.6706 - acc: 0.826 - ETA: 2:25 - loss: 0.6741 - acc: 0.825 - ETA: 2:24 - loss: 0.6755 - acc: 0.825 - ETA: 2:23 - loss: 0.6717 - acc: 0.826 - ETA: 2:22 - loss: 0.6720 - acc: 0.825 - ETA: 2:21 - loss: 0.6734 - acc: 0.825 - ETA: 2:21 - loss: 0.6732 - acc: 0.825 - ETA: 2:20 - loss: 0.6772 - acc: 0.824 - ETA: 2:19 - loss: 0.6791 - acc: 0.823 - ETA: 2:18 - loss: 0.6794 - acc: 0.824 - ETA: 2:17 - loss: 0.6795 - acc: 0.824 - ETA: 2:16 - loss: 0.6799 - acc: 0.823 - ETA: 2:16 - loss: 0.6784 - acc: 0.824 - ETA: 2:15 - loss: 0.6773 - acc: 0.824 - ETA: 2:14 - loss: 0.6780 - acc: 0.824 - ETA: 2:13 - loss: 0.6777 - acc: 0.823 - ETA: 2:12 - loss: 0.6778 - acc: 0.823 - ETA: 2:11 - loss: 0.6806 - acc: 0.823 - ETA: 2:11 - loss: 0.6813 - acc: 0.822 - ETA: 2:10 - loss: 0.6857 - acc: 0.821 - ETA: 2:09 - loss: 0.6851 - acc: 0.822 - ETA: 2:08 - loss: 0.6877 - acc: 0.821 - ETA: 2:07 - loss: 0.6887 - acc: 0.820 - ETA: 2:06 - loss: 0.6903 - acc: 0.820 - ETA: 2:06 - loss: 0.6894 - acc: 0.820 - ETA: 2:05 - loss: 0.6899 - acc: 0.820 - ETA: 2:04 - loss: 0.6921 - acc: 0.819 - ETA: 2:03 - loss: 0.6940 - acc: 0.818 - ETA: 2:02 - loss: 0.6946 - acc: 0.818 - ETA: 2:01 - loss: 0.6950 - acc: 0.818 - ETA: 2:01 - loss: 0.6961 - acc: 0.818 - ETA: 2:00 - loss: 0.6943 - acc: 0.819 - ETA: 1:59 - loss: 0.6936 - acc: 0.819 - ETA: 1:58 - loss: 0.6934 - acc: 0.819 - ETA: 1:57 - loss: 0.6950 - acc: 0.818 - ETA: 1:56 - loss: 0.6978 - acc: 0.817 - ETA: 1:56 - loss: 0.7013 - acc: 0.816 - ETA: 1:55 - loss: 0.7096 - acc: 0.814 - ETA: 1:54 - loss: 0.7152 - acc: 0.813 - ETA: 1:53 - loss: 0.7145 - acc: 0.813 - ETA: 1:52 - loss: 0.7115 - acc: 0.814 - ETA: 1:51 - loss: 0.7138 - acc: 0.814 - ETA: 1:51 - loss: 0.7140 - acc: 0.813 - ETA: 1:50 - loss: 0.7148 - acc: 0.813 - ETA: 1:49 - loss: 0.7164 - acc: 0.813 - ETA: 1:48 - loss: 0.7160 - acc: 0.8132"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 1:47 - loss: 0.7184 - acc: 0.812 - ETA: 1:46 - loss: 0.7190 - acc: 0.812 - ETA: 1:46 - loss: 0.7190 - acc: 0.812 - ETA: 1:45 - loss: 0.7180 - acc: 0.812 - ETA: 1:44 - loss: 0.7226 - acc: 0.810 - ETA: 1:43 - loss: 0.7216 - acc: 0.811 - ETA: 1:42 - loss: 0.7238 - acc: 0.810 - ETA: 1:41 - loss: 0.7230 - acc: 0.810 - ETA: 1:41 - loss: 0.7220 - acc: 0.811 - ETA: 1:40 - loss: 0.7207 - acc: 0.811 - ETA: 1:39 - loss: 0.7235 - acc: 0.810 - ETA: 1:38 - loss: 0.7237 - acc: 0.810 - ETA: 1:37 - loss: 0.7259 - acc: 0.810 - ETA: 1:36 - loss: 0.7268 - acc: 0.810 - ETA: 1:36 - loss: 0.7253 - acc: 0.810 - ETA: 1:35 - loss: 0.7271 - acc: 0.810 - ETA: 1:34 - loss: 0.7275 - acc: 0.810 - ETA: 1:33 - loss: 0.7271 - acc: 0.810 - ETA: 1:32 - loss: 0.7300 - acc: 0.810 - ETA: 1:31 - loss: 0.7324 - acc: 0.809 - ETA: 1:31 - loss: 0.7311 - acc: 0.810 - ETA: 1:30 - loss: 0.7321 - acc: 0.810 - ETA: 1:29 - loss: 0.7325 - acc: 0.809 - ETA: 1:28 - loss: 0.7326 - acc: 0.809 - ETA: 1:27 - loss: 0.7329 - acc: 0.809 - ETA: 1:26 - loss: 0.7323 - acc: 0.809 - ETA: 1:26 - loss: 0.7330 - acc: 0.809 - ETA: 1:25 - loss: 0.7323 - acc: 0.810 - ETA: 1:24 - loss: 0.7324 - acc: 0.810 - ETA: 1:23 - loss: 0.7335 - acc: 0.810 - ETA: 1:22 - loss: 0.7324 - acc: 0.810 - ETA: 1:21 - loss: 0.7330 - acc: 0.810 - ETA: 1:21 - loss: 0.7326 - acc: 0.810 - ETA: 1:20 - loss: 0.7328 - acc: 0.810 - ETA: 1:19 - loss: 0.7319 - acc: 0.810 - ETA: 1:18 - loss: 0.7318 - acc: 0.810 - ETA: 1:17 - loss: 0.7346 - acc: 0.810 - ETA: 1:16 - loss: 0.7363 - acc: 0.809 - ETA: 1:16 - loss: 0.7360 - acc: 0.809 - ETA: 1:15 - loss: 0.7353 - acc: 0.809 - ETA: 1:14 - loss: 0.7329 - acc: 0.810 - ETA: 1:13 - loss: 0.7310 - acc: 0.811 - ETA: 1:12 - loss: 0.7302 - acc: 0.811 - ETA: 1:11 - loss: 0.7283 - acc: 0.811 - ETA: 1:10 - loss: 0.7278 - acc: 0.811 - ETA: 1:10 - loss: 0.7309 - acc: 0.811 - ETA: 1:09 - loss: 0.7303 - acc: 0.811 - ETA: 1:08 - loss: 0.7303 - acc: 0.811 - ETA: 1:07 - loss: 0.7295 - acc: 0.811 - ETA: 1:06 - loss: 0.7310 - acc: 0.810 - ETA: 1:05 - loss: 0.7293 - acc: 0.810 - ETA: 1:05 - loss: 0.7284 - acc: 0.810 - ETA: 1:04 - loss: 0.7275 - acc: 0.811 - ETA: 1:03 - loss: 0.7282 - acc: 0.811 - ETA: 1:02 - loss: 0.7268 - acc: 0.811 - ETA: 1:01 - loss: 0.7273 - acc: 0.811 - ETA: 1:00 - loss: 0.7304 - acc: 0.810 - ETA: 1:00 - loss: 0.7330 - acc: 0.809 - ETA: 59s - loss: 0.7323 - acc: 0.809 - ETA: 58s - loss: 0.7335 - acc: 0.80 - ETA: 57s - loss: 0.7325 - acc: 0.80 - ETA: 56s - loss: 0.7344 - acc: 0.80 - ETA: 55s - loss: 0.7342 - acc: 0.80 - ETA: 55s - loss: 0.7356 - acc: 0.80 - ETA: 54s - loss: 0.7367 - acc: 0.80 - ETA: 53s - loss: 0.7375 - acc: 0.80 - ETA: 52s - loss: 0.7361 - acc: 0.80 - ETA: 51s - loss: 0.7376 - acc: 0.80 - ETA: 50s - loss: 0.7388 - acc: 0.80 - ETA: 50s - loss: 0.7389 - acc: 0.80 - ETA: 49s - loss: 0.7405 - acc: 0.80 - ETA: 48s - loss: 0.7403 - acc: 0.80 - ETA: 47s - loss: 0.7403 - acc: 0.80 - ETA: 46s - loss: 0.7467 - acc: 0.80 - ETA: 45s - loss: 0.7467 - acc: 0.80 - ETA: 45s - loss: 0.7465 - acc: 0.80 - ETA: 44s - loss: 0.7463 - acc: 0.80 - ETA: 43s - loss: 0.7452 - acc: 0.80 - ETA: 42s - loss: 0.7441 - acc: 0.80 - ETA: 41s - loss: 0.7441 - acc: 0.80 - ETA: 40s - loss: 0.7460 - acc: 0.80 - ETA: 40s - loss: 0.7466 - acc: 0.80 - ETA: 39s - loss: 0.7456 - acc: 0.80 - ETA: 38s - loss: 0.7462 - acc: 0.80 - ETA: 37s - loss: 0.7476 - acc: 0.80 - ETA: 36s - loss: 0.7482 - acc: 0.80 - ETA: 35s - loss: 0.7479 - acc: 0.80 - ETA: 35s - loss: 0.7485 - acc: 0.80 - ETA: 34s - loss: 0.7474 - acc: 0.80 - ETA: 33s - loss: 0.7472 - acc: 0.80 - ETA: 32s - loss: 0.7480 - acc: 0.80 - ETA: 31s - loss: 0.7472 - acc: 0.80 - ETA: 30s - loss: 0.7469 - acc: 0.80 - ETA: 30s - loss: 0.7476 - acc: 0.80 - ETA: 29s - loss: 0.7490 - acc: 0.80 - ETA: 28s - loss: 0.7488 - acc: 0.80 - ETA: 27s - loss: 0.7490 - acc: 0.80 - ETA: 26s - loss: 0.7474 - acc: 0.80 - ETA: 25s - loss: 0.7471 - acc: 0.80 - ETA: 25s - loss: 0.7464 - acc: 0.80 - ETA: 24s - loss: 0.7450 - acc: 0.80 - ETA: 23s - loss: 0.7437 - acc: 0.80 - ETA: 22s - loss: 0.7451 - acc: 0.80 - ETA: 21s - loss: 0.7470 - acc: 0.80 - ETA: 20s - loss: 0.7479 - acc: 0.80 - ETA: 20s - loss: 0.7482 - acc: 0.80 - ETA: 19s - loss: 0.7495 - acc: 0.80 - ETA: 18s - loss: 0.7499 - acc: 0.80 - ETA: 17s - loss: 0.7500 - acc: 0.80 - ETA: 16s - loss: 0.7488 - acc: 0.80 - ETA: 15s - loss: 0.7484 - acc: 0.80 - ETA: 15s - loss: 0.7496 - acc: 0.80 - ETA: 14s - loss: 0.7506 - acc: 0.80 - ETA: 13s - loss: 0.7511 - acc: 0.80 - ETA: 12s - loss: 0.7513 - acc: 0.80 - ETA: 11s - loss: 0.7517 - acc: 0.80 - ETA: 10s - loss: 0.7513 - acc: 0.80 - ETA: 10s - loss: 0.7508 - acc: 0.80 - ETA: 9s - loss: 0.7503 - acc: 0.8042 - ETA: 8s - loss: 0.7493 - acc: 0.804 - ETA: 7s - loss: 0.7489 - acc: 0.804 - ETA: 6s - loss: 0.7481 - acc: 0.804 - ETA: 5s - loss: 0.7484 - acc: 0.805 - ETA: 5s - loss: 0.7481 - acc: 0.805 - ETA: 4s - loss: 0.7489 - acc: 0.804 - ETA: 3s - loss: 0.7489 - acc: 0.804 - ETA: 2s - loss: 0.7488 - acc: 0.804 - ETA: 1s - loss: 0.7481 - acc: 0.804 - ETA: 0s - loss: 0.7481 - acc: 0.804 - 288s 43ms/step - loss: 0.7482 - acc: 0.8040 - val_loss: 7.2829 - val_acc: 0.0671\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 4.20959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x39019b358>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint  \n",
    "\n",
    "### TODO: specify the number of epochs that you would like to use to train the model.\n",
    "\n",
    "epochs = 5\n",
    "\n",
    "### Do NOT modify the code below this line.\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.from_scratch.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets),\n",
    "          epochs=epochs, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('saved_models/weights.best.from_scratch.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images.  Ensure that your test accuracy is greater than 1%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 6.6986%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "dog_breed_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(dog_breed_predictions)==np.argmax(test_targets, axis=1))/len(dog_breed_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step4'></a>\n",
    "## Step 4: Use a CNN to Classify Dog Breeds\n",
    "\n",
    "To reduce training time without sacrificing accuracy, we show you how to train a CNN using transfer learning.  In the following step, you will get a chance to use transfer learning to train your own CNN.\n",
    "\n",
    "### Obtain Bottleneck Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "bottleneck_features = np.load('bottleneck_features/DogVGG16Data.npz')\n",
    "train_VGG16 = bottleneck_features['train']\n",
    "valid_VGG16 = bottleneck_features['valid']\n",
    "test_VGG16 = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "The model uses the the pre-trained VGG-16 model as a fixed feature extractor, where the last convolutional output of VGG-16 is fed as input to our model.  We only add a global average pooling layer and a fully connected layer, where the latter contains one node for each dog category and is equipped with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_3 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 133)               68229     \n",
      "=================================================================\n",
      "Total params: 68,229\n",
      "Trainable params: 68,229\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "VGG16_model = Sequential()\n",
    "VGG16_model.add(GlobalAveragePooling2D(input_shape=train_VGG16.shape[1:]))\n",
    "VGG16_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "VGG16_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      "6680/6680 [==============================] - ETA: 1:27:48 - loss: 13.6978 - acc: 0.0000e+0 - ETA: 29:12 - loss: 14.3592 - acc: 0.0000e+00  - ETA: 14:31 - loss: 14.6396 - acc: 0.0167    - ETA: 8:39 - loss: 14.3059 - acc: 0.025 - ETA: 6:07 - loss: 14.5289 - acc: 0.02 - ETA: 5:01 - loss: 14.4345 - acc: 0.02 - ETA: 4:01 - loss: 14.3355 - acc: 0.03 - ETA: 3:30 - loss: 14.3322 - acc: 0.03 - ETA: 3:13 - loss: 14.3227 - acc: 0.03 - ETA: 2:52 - loss: 14.2756 - acc: 0.03 - ETA: 2:41 - loss: 14.2858 - acc: 0.03 - ETA: 2:25 - loss: 14.1629 - acc: 0.03 - ETA: 2:09 - loss: 14.1405 - acc: 0.03 - ETA: 1:55 - loss: 14.0995 - acc: 0.03 - ETA: 1:44 - loss: 14.0343 - acc: 0.03 - ETA: 1:37 - loss: 14.0190 - acc: 0.03 - ETA: 1:29 - loss: 13.9649 - acc: 0.03 - ETA: 1:22 - loss: 13.8961 - acc: 0.03 - ETA: 1:15 - loss: 13.8363 - acc: 0.03 - ETA: 1:11 - loss: 13.7849 - acc: 0.03 - ETA: 1:07 - loss: 13.7846 - acc: 0.03 - ETA: 1:04 - loss: 13.7081 - acc: 0.04 - ETA: 1:01 - loss: 13.7075 - acc: 0.03 - ETA: 57s - loss: 13.6440 - acc: 0.0409 - ETA: 53s - loss: 13.5766 - acc: 0.042 - ETA: 50s - loss: 13.5251 - acc: 0.046 - ETA: 47s - loss: 13.4936 - acc: 0.048 - ETA: 45s - loss: 13.4676 - acc: 0.048 - ETA: 42s - loss: 13.4003 - acc: 0.052 - ETA: 40s - loss: 13.3548 - acc: 0.053 - ETA: 38s - loss: 13.3003 - acc: 0.054 - ETA: 36s - loss: 13.2947 - acc: 0.054 - ETA: 35s - loss: 13.2547 - acc: 0.057 - ETA: 33s - loss: 13.2258 - acc: 0.058 - ETA: 32s - loss: 13.1817 - acc: 0.059 - ETA: 30s - loss: 13.1382 - acc: 0.059 - ETA: 29s - loss: 13.1085 - acc: 0.061 - ETA: 28s - loss: 13.0849 - acc: 0.061 - ETA: 27s - loss: 13.0413 - acc: 0.062 - ETA: 26s - loss: 13.0030 - acc: 0.064 - ETA: 24s - loss: 12.9433 - acc: 0.065 - ETA: 23s - loss: 12.9052 - acc: 0.066 - ETA: 22s - loss: 12.8486 - acc: 0.069 - ETA: 22s - loss: 12.8150 - acc: 0.070 - ETA: 21s - loss: 12.7304 - acc: 0.072 - ETA: 20s - loss: 12.6950 - acc: 0.074 - ETA: 19s - loss: 12.6356 - acc: 0.075 - ETA: 18s - loss: 12.6165 - acc: 0.076 - ETA: 17s - loss: 12.5602 - acc: 0.078 - ETA: 17s - loss: 12.5249 - acc: 0.080 - ETA: 16s - loss: 12.4798 - acc: 0.082 - ETA: 15s - loss: 12.4171 - acc: 0.084 - ETA: 15s - loss: 12.3907 - acc: 0.084 - ETA: 14s - loss: 12.3725 - acc: 0.085 - ETA: 14s - loss: 12.3534 - acc: 0.085 - ETA: 13s - loss: 12.2883 - acc: 0.089 - ETA: 12s - loss: 12.2594 - acc: 0.090 - ETA: 12s - loss: 12.2224 - acc: 0.091 - ETA: 11s - loss: 12.1969 - acc: 0.092 - ETA: 11s - loss: 12.1618 - acc: 0.094 - ETA: 10s - loss: 12.1263 - acc: 0.096 - ETA: 10s - loss: 12.0966 - acc: 0.098 - ETA: 9s - loss: 12.0438 - acc: 0.100 - ETA: 9s - loss: 11.9866 - acc: 0.10 - ETA: 8s - loss: 11.9546 - acc: 0.10 - ETA: 8s - loss: 11.9371 - acc: 0.10 - ETA: 7s - loss: 11.9209 - acc: 0.10 - ETA: 7s - loss: 11.9026 - acc: 0.10 - ETA: 7s - loss: 11.8704 - acc: 0.11 - ETA: 6s - loss: 11.8378 - acc: 0.11 - ETA: 6s - loss: 11.7910 - acc: 0.11 - ETA: 6s - loss: 11.7588 - acc: 0.11 - ETA: 5s - loss: 11.7190 - acc: 0.11 - ETA: 5s - loss: 11.7138 - acc: 0.11 - ETA: 5s - loss: 11.6846 - acc: 0.11 - ETA: 4s - loss: 11.6556 - acc: 0.12 - ETA: 4s - loss: 11.6234 - acc: 0.12 - ETA: 4s - loss: 11.5934 - acc: 0.12 - ETA: 3s - loss: 11.5510 - acc: 0.12 - ETA: 3s - loss: 11.5054 - acc: 0.12 - ETA: 3s - loss: 11.4818 - acc: 0.13 - ETA: 2s - loss: 11.4514 - acc: 0.13 - ETA: 2s - loss: 11.4239 - acc: 0.13 - ETA: 2s - loss: 11.4070 - acc: 0.13 - ETA: 1s - loss: 11.3905 - acc: 0.13 - ETA: 1s - loss: 11.3735 - acc: 0.13 - ETA: 1s - loss: 11.3499 - acc: 0.13 - ETA: 1s - loss: 11.3222 - acc: 0.14 - ETA: 0s - loss: 11.3006 - acc: 0.14 - ETA: 0s - loss: 11.2747 - acc: 0.14 - ETA: 0s - loss: 11.2472 - acc: 0.14 - ETA: 0s - loss: 11.1996 - acc: 0.14 - 22s 3ms/step - loss: 11.1844 - acc: 0.1493 - val_loss: 9.0739 - val_acc: 0.2850\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.07389, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 2/20\n",
      "6680/6680 [==============================] - ETA: 8s - loss: 9.5622 - acc: 0.350 - ETA: 7s - loss: 8.0990 - acc: 0.375 - ETA: 6s - loss: 7.8023 - acc: 0.385 - ETA: 6s - loss: 8.6683 - acc: 0.331 - ETA: 5s - loss: 8.4555 - acc: 0.336 - ETA: 5s - loss: 8.7402 - acc: 0.326 - ETA: 5s - loss: 8.7487 - acc: 0.326 - ETA: 5s - loss: 8.8186 - acc: 0.320 - ETA: 5s - loss: 8.8512 - acc: 0.321 - ETA: 4s - loss: 8.8943 - acc: 0.323 - ETA: 4s - loss: 8.8672 - acc: 0.322 - ETA: 4s - loss: 8.8303 - acc: 0.322 - ETA: 4s - loss: 8.7795 - acc: 0.322 - ETA: 4s - loss: 8.6822 - acc: 0.329 - ETA: 4s - loss: 8.6493 - acc: 0.331 - ETA: 4s - loss: 8.6228 - acc: 0.331 - ETA: 4s - loss: 8.6778 - acc: 0.327 - ETA: 4s - loss: 8.6342 - acc: 0.329 - ETA: 4s - loss: 8.6213 - acc: 0.330 - ETA: 4s - loss: 8.6369 - acc: 0.332 - ETA: 4s - loss: 8.6505 - acc: 0.334 - ETA: 4s - loss: 8.6019 - acc: 0.338 - ETA: 4s - loss: 8.5478 - acc: 0.341 - ETA: 3s - loss: 8.5139 - acc: 0.342 - ETA: 3s - loss: 8.5029 - acc: 0.345 - ETA: 3s - loss: 8.4776 - acc: 0.346 - ETA: 3s - loss: 8.4849 - acc: 0.346 - ETA: 3s - loss: 8.5187 - acc: 0.343 - ETA: 3s - loss: 8.5072 - acc: 0.342 - ETA: 3s - loss: 8.4917 - acc: 0.343 - ETA: 3s - loss: 8.4868 - acc: 0.343 - ETA: 3s - loss: 8.4919 - acc: 0.343 - ETA: 3s - loss: 8.4669 - acc: 0.344 - ETA: 3s - loss: 8.4799 - acc: 0.344 - ETA: 3s - loss: 8.4951 - acc: 0.341 - ETA: 3s - loss: 8.4671 - acc: 0.345 - ETA: 3s - loss: 8.4593 - acc: 0.346 - ETA: 3s - loss: 8.4623 - acc: 0.347 - ETA: 2s - loss: 8.5082 - acc: 0.346 - ETA: 2s - loss: 8.4604 - acc: 0.349 - ETA: 2s - loss: 8.4447 - acc: 0.351 - ETA: 2s - loss: 8.4078 - acc: 0.353 - ETA: 2s - loss: 8.3817 - acc: 0.354 - ETA: 2s - loss: 8.3552 - acc: 0.355 - ETA: 2s - loss: 8.3635 - acc: 0.354 - ETA: 2s - loss: 8.3428 - acc: 0.356 - ETA: 2s - loss: 8.3334 - acc: 0.357 - ETA: 2s - loss: 8.3353 - acc: 0.356 - ETA: 2s - loss: 8.3184 - acc: 0.357 - ETA: 2s - loss: 8.3148 - acc: 0.358 - ETA: 2s - loss: 8.3156 - acc: 0.358 - ETA: 2s - loss: 8.3097 - acc: 0.359 - ETA: 2s - loss: 8.3056 - acc: 0.359 - ETA: 2s - loss: 8.3106 - acc: 0.359 - ETA: 2s - loss: 8.3208 - acc: 0.359 - ETA: 1s - loss: 8.3352 - acc: 0.358 - ETA: 1s - loss: 8.3294 - acc: 0.359 - ETA: 1s - loss: 8.3412 - acc: 0.359 - ETA: 1s - loss: 8.3041 - acc: 0.361 - ETA: 1s - loss: 8.3135 - acc: 0.361 - ETA: 1s - loss: 8.3187 - acc: 0.361 - ETA: 1s - loss: 8.3228 - acc: 0.361 - ETA: 1s - loss: 8.3092 - acc: 0.362 - ETA: 1s - loss: 8.2892 - acc: 0.363 - ETA: 1s - loss: 8.2836 - acc: 0.363 - ETA: 1s - loss: 8.2744 - acc: 0.363 - ETA: 1s - loss: 8.2786 - acc: 0.364 - ETA: 1s - loss: 8.2916 - acc: 0.363 - ETA: 1s - loss: 8.2773 - acc: 0.364 - ETA: 1s - loss: 8.2735 - acc: 0.365 - ETA: 1s - loss: 8.2593 - acc: 0.366 - ETA: 1s - loss: 8.2346 - acc: 0.369 - ETA: 0s - loss: 8.2209 - acc: 0.370 - ETA: 0s - loss: 8.2084 - acc: 0.370 - ETA: 0s - loss: 8.2033 - acc: 0.371 - ETA: 0s - loss: 8.1979 - acc: 0.371 - ETA: 0s - loss: 8.2046 - acc: 0.371 - ETA: 0s - loss: 8.1939 - acc: 0.371 - ETA: 0s - loss: 8.1830 - acc: 0.371 - ETA: 0s - loss: 8.1911 - acc: 0.371 - ETA: 0s - loss: 8.1839 - acc: 0.371 - ETA: 0s - loss: 8.1803 - acc: 0.372 - ETA: 0s - loss: 8.1869 - acc: 0.371 - ETA: 0s - loss: 8.1939 - acc: 0.371 - ETA: 0s - loss: 8.1876 - acc: 0.372 - ETA: 0s - loss: 8.1762 - acc: 0.373 - ETA: 0s - loss: 8.1848 - acc: 0.372 - ETA: 0s - loss: 8.1936 - acc: 0.372 - 6s 845us/step - loss: 8.1868 - acc: 0.3725 - val_loss: 8.0588 - val_acc: 0.3820\n",
      "\n",
      "Epoch 00002: val_loss improved from 9.07389 to 8.05877, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 3/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 8.2986 - acc: 0.350 - ETA: 5s - loss: 7.0053 - acc: 0.470 - ETA: 4s - loss: 7.2724 - acc: 0.450 - ETA: 5s - loss: 6.8672 - acc: 0.475 - ETA: 5s - loss: 6.7848 - acc: 0.480 - ETA: 5s - loss: 7.0697 - acc: 0.468 - ETA: 4s - loss: 7.3462 - acc: 0.454 - ETA: 4s - loss: 7.3277 - acc: 0.457 - ETA: 4s - loss: 7.2036 - acc: 0.465 - ETA: 4s - loss: 7.0781 - acc: 0.472 - ETA: 4s - loss: 7.0703 - acc: 0.472 - ETA: 4s - loss: 7.1789 - acc: 0.468 - ETA: 4s - loss: 7.2681 - acc: 0.467 - ETA: 4s - loss: 7.2060 - acc: 0.471 - ETA: 4s - loss: 7.0519 - acc: 0.478 - ETA: 4s - loss: 7.1052 - acc: 0.472 - ETA: 4s - loss: 7.1623 - acc: 0.467 - ETA: 4s - loss: 7.2190 - acc: 0.461 - ETA: 4s - loss: 7.3133 - acc: 0.457 - ETA: 4s - loss: 7.2897 - acc: 0.456 - ETA: 4s - loss: 7.2873 - acc: 0.455 - ETA: 4s - loss: 7.2604 - acc: 0.458 - ETA: 4s - loss: 7.2499 - acc: 0.459 - ETA: 4s - loss: 7.2097 - acc: 0.463 - ETA: 3s - loss: 7.2234 - acc: 0.461 - ETA: 3s - loss: 7.2516 - acc: 0.461 - ETA: 3s - loss: 7.2741 - acc: 0.461 - ETA: 3s - loss: 7.3167 - acc: 0.458 - ETA: 3s - loss: 7.2933 - acc: 0.459 - ETA: 3s - loss: 7.3220 - acc: 0.456 - ETA: 3s - loss: 7.3199 - acc: 0.455 - ETA: 3s - loss: 7.3338 - acc: 0.454 - ETA: 3s - loss: 7.3625 - acc: 0.453 - ETA: 3s - loss: 7.3723 - acc: 0.453 - ETA: 3s - loss: 7.3734 - acc: 0.453 - ETA: 3s - loss: 7.3682 - acc: 0.454 - ETA: 3s - loss: 7.3748 - acc: 0.454 - ETA: 3s - loss: 7.3960 - acc: 0.453 - ETA: 3s - loss: 7.4080 - acc: 0.453 - ETA: 3s - loss: 7.4444 - acc: 0.450 - ETA: 3s - loss: 7.4496 - acc: 0.448 - ETA: 3s - loss: 7.4454 - acc: 0.448 - ETA: 3s - loss: 7.4456 - acc: 0.448 - ETA: 2s - loss: 7.4375 - acc: 0.449 - ETA: 2s - loss: 7.4256 - acc: 0.450 - ETA: 2s - loss: 7.4496 - acc: 0.448 - ETA: 2s - loss: 7.4757 - acc: 0.447 - ETA: 2s - loss: 7.4481 - acc: 0.448 - ETA: 2s - loss: 7.4036 - acc: 0.452 - ETA: 2s - loss: 7.4239 - acc: 0.451 - ETA: 2s - loss: 7.4356 - acc: 0.450 - ETA: 2s - loss: 7.4049 - acc: 0.450 - ETA: 2s - loss: 7.4147 - acc: 0.451 - ETA: 2s - loss: 7.4066 - acc: 0.451 - ETA: 2s - loss: 7.4133 - acc: 0.451 - ETA: 2s - loss: 7.4223 - acc: 0.450 - ETA: 2s - loss: 7.4225 - acc: 0.451 - ETA: 2s - loss: 7.4271 - acc: 0.451 - ETA: 2s - loss: 7.4493 - acc: 0.450 - ETA: 1s - loss: 7.4624 - acc: 0.450 - ETA: 1s - loss: 7.4587 - acc: 0.450 - ETA: 1s - loss: 7.4404 - acc: 0.451 - ETA: 1s - loss: 7.4428 - acc: 0.450 - ETA: 1s - loss: 7.4411 - acc: 0.450 - ETA: 1s - loss: 7.4507 - acc: 0.450 - ETA: 1s - loss: 7.4635 - acc: 0.449 - ETA: 1s - loss: 7.4723 - acc: 0.449 - ETA: 1s - loss: 7.4632 - acc: 0.450 - ETA: 1s - loss: 7.4704 - acc: 0.450 - ETA: 1s - loss: 7.4790 - acc: 0.449 - ETA: 1s - loss: 7.4801 - acc: 0.449 - ETA: 1s - loss: 7.4768 - acc: 0.449 - ETA: 1s - loss: 7.4582 - acc: 0.450 - ETA: 1s - loss: 7.4631 - acc: 0.451 - ETA: 0s - loss: 7.4381 - acc: 0.453 - ETA: 0s - loss: 7.4555 - acc: 0.452 - ETA: 0s - loss: 7.4507 - acc: 0.452 - ETA: 0s - loss: 7.4414 - acc: 0.452 - ETA: 0s - loss: 7.4276 - acc: 0.453 - ETA: 0s - loss: 7.4107 - acc: 0.453 - ETA: 0s - loss: 7.4076 - acc: 0.453 - ETA: 0s - loss: 7.4163 - acc: 0.453 - ETA: 0s - loss: 7.4228 - acc: 0.453 - ETA: 0s - loss: 7.4007 - acc: 0.454 - ETA: 0s - loss: 7.3898 - acc: 0.455 - ETA: 0s - loss: 7.4041 - acc: 0.454 - ETA: 0s - loss: 7.4081 - acc: 0.454 - ETA: 0s - loss: 7.4346 - acc: 0.453 - ETA: 0s - loss: 7.4386 - acc: 0.453 - ETA: 0s - loss: 7.4344 - acc: 0.452 - ETA: 0s - loss: 7.4322 - acc: 0.452 - ETA: 0s - loss: 7.4219 - acc: 0.453 - 6s 852us/step - loss: 7.4178 - acc: 0.4531 - val_loss: 7.6813 - val_acc: 0.4168\n",
      "\n",
      "Epoch 00003: val_loss improved from 8.05877 to 7.68134, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 4/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 6.6773 - acc: 0.550 - ETA: 5s - loss: 7.1607 - acc: 0.520 - ETA: 5s - loss: 7.1210 - acc: 0.522 - ETA: 4s - loss: 6.8962 - acc: 0.519 - ETA: 4s - loss: 7.0439 - acc: 0.508 - ETA: 4s - loss: 7.1462 - acc: 0.495 - ETA: 4s - loss: 7.2012 - acc: 0.493 - ETA: 4s - loss: 7.0807 - acc: 0.500 - ETA: 4s - loss: 6.9711 - acc: 0.508 - ETA: 4s - loss: 6.9796 - acc: 0.508 - ETA: 4s - loss: 6.8557 - acc: 0.515 - ETA: 4s - loss: 6.7881 - acc: 0.523 - ETA: 4s - loss: 6.8314 - acc: 0.520 - ETA: 4s - loss: 6.9666 - acc: 0.510 - ETA: 4s - loss: 6.9511 - acc: 0.510 - ETA: 4s - loss: 6.9224 - acc: 0.514 - ETA: 4s - loss: 6.9001 - acc: 0.513 - ETA: 4s - loss: 7.0032 - acc: 0.506 - ETA: 4s - loss: 6.9807 - acc: 0.506 - ETA: 4s - loss: 6.9694 - acc: 0.506 - ETA: 4s - loss: 6.9139 - acc: 0.511 - ETA: 3s - loss: 6.9477 - acc: 0.508 - ETA: 3s - loss: 6.9351 - acc: 0.506 - ETA: 3s - loss: 6.9392 - acc: 0.506 - ETA: 3s - loss: 6.9866 - acc: 0.502 - ETA: 3s - loss: 6.9567 - acc: 0.504 - ETA: 3s - loss: 7.0219 - acc: 0.501 - ETA: 3s - loss: 7.0491 - acc: 0.499 - ETA: 3s - loss: 7.0332 - acc: 0.501 - ETA: 3s - loss: 7.0044 - acc: 0.504 - ETA: 3s - loss: 6.9972 - acc: 0.505 - ETA: 3s - loss: 7.0284 - acc: 0.503 - ETA: 3s - loss: 6.9785 - acc: 0.506 - ETA: 3s - loss: 6.9394 - acc: 0.509 - ETA: 3s - loss: 6.9397 - acc: 0.508 - ETA: 3s - loss: 6.9439 - acc: 0.507 - ETA: 3s - loss: 6.9354 - acc: 0.508 - ETA: 2s - loss: 6.9689 - acc: 0.506 - ETA: 2s - loss: 6.9903 - acc: 0.504 - ETA: 2s - loss: 7.0216 - acc: 0.502 - ETA: 2s - loss: 7.0227 - acc: 0.502 - ETA: 2s - loss: 7.0158 - acc: 0.503 - ETA: 2s - loss: 6.9872 - acc: 0.505 - ETA: 2s - loss: 6.9937 - acc: 0.504 - ETA: 2s - loss: 7.0099 - acc: 0.503 - ETA: 2s - loss: 7.0309 - acc: 0.501 - ETA: 2s - loss: 7.0317 - acc: 0.501 - ETA: 2s - loss: 7.0067 - acc: 0.503 - ETA: 2s - loss: 7.0015 - acc: 0.503 - ETA: 2s - loss: 6.9846 - acc: 0.503 - ETA: 2s - loss: 7.0005 - acc: 0.502 - ETA: 2s - loss: 6.9977 - acc: 0.502 - ETA: 2s - loss: 6.9869 - acc: 0.503 - ETA: 2s - loss: 6.9472 - acc: 0.505 - ETA: 1s - loss: 6.9596 - acc: 0.504 - ETA: 1s - loss: 6.9321 - acc: 0.506 - ETA: 1s - loss: 6.9214 - acc: 0.506 - ETA: 1s - loss: 6.9209 - acc: 0.506 - ETA: 1s - loss: 6.9388 - acc: 0.505 - ETA: 1s - loss: 6.9490 - acc: 0.505 - ETA: 1s - loss: 6.9389 - acc: 0.504 - ETA: 1s - loss: 6.9336 - acc: 0.503 - ETA: 1s - loss: 6.9133 - acc: 0.504 - ETA: 1s - loss: 6.8927 - acc: 0.506 - ETA: 1s - loss: 6.8900 - acc: 0.505 - ETA: 1s - loss: 6.8957 - acc: 0.505 - ETA: 1s - loss: 6.9072 - acc: 0.505 - ETA: 1s - loss: 6.9061 - acc: 0.505 - ETA: 1s - loss: 6.9070 - acc: 0.505 - ETA: 1s - loss: 6.9188 - acc: 0.504 - ETA: 1s - loss: 6.9418 - acc: 0.502 - ETA: 0s - loss: 6.9412 - acc: 0.502 - ETA: 0s - loss: 6.9361 - acc: 0.502 - ETA: 0s - loss: 6.9282 - acc: 0.502 - ETA: 0s - loss: 6.9022 - acc: 0.504 - ETA: 0s - loss: 6.9087 - acc: 0.504 - ETA: 0s - loss: 6.9177 - acc: 0.503 - ETA: 0s - loss: 6.9072 - acc: 0.504 - ETA: 0s - loss: 6.8917 - acc: 0.505 - ETA: 0s - loss: 6.8898 - acc: 0.504 - ETA: 0s - loss: 6.8771 - acc: 0.505 - ETA: 0s - loss: 6.8714 - acc: 0.506 - ETA: 0s - loss: 6.8600 - acc: 0.506 - ETA: 0s - loss: 6.8633 - acc: 0.507 - ETA: 0s - loss: 6.8555 - acc: 0.507 - ETA: 0s - loss: 6.8713 - acc: 0.506 - ETA: 0s - loss: 6.8732 - acc: 0.506 - 6s 926us/step - loss: 6.8874 - acc: 0.5049 - val_loss: 7.2540 - val_acc: 0.4407\n",
      "\n",
      "Epoch 00004: val_loss improved from 7.68134 to 7.25397, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 5/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 4s - loss: 4.9319 - acc: 0.650 - ETA: 4s - loss: 6.4404 - acc: 0.560 - ETA: 4s - loss: 6.4600 - acc: 0.527 - ETA: 4s - loss: 6.2604 - acc: 0.534 - ETA: 4s - loss: 6.2130 - acc: 0.541 - ETA: 4s - loss: 6.5263 - acc: 0.533 - ETA: 4s - loss: 6.7115 - acc: 0.528 - ETA: 4s - loss: 6.5596 - acc: 0.536 - ETA: 4s - loss: 6.5888 - acc: 0.531 - ETA: 4s - loss: 6.5833 - acc: 0.533 - ETA: 4s - loss: 6.5247 - acc: 0.531 - ETA: 4s - loss: 6.5444 - acc: 0.528 - ETA: 4s - loss: 6.4529 - acc: 0.533 - ETA: 4s - loss: 6.3817 - acc: 0.539 - ETA: 4s - loss: 6.4827 - acc: 0.532 - ETA: 4s - loss: 6.4658 - acc: 0.533 - ETA: 3s - loss: 6.4258 - acc: 0.536 - ETA: 3s - loss: 6.4584 - acc: 0.534 - ETA: 3s - loss: 6.4402 - acc: 0.537 - ETA: 3s - loss: 6.4735 - acc: 0.536 - ETA: 3s - loss: 6.5830 - acc: 0.532 - ETA: 3s - loss: 6.6032 - acc: 0.530 - ETA: 3s - loss: 6.6452 - acc: 0.527 - ETA: 3s - loss: 6.6597 - acc: 0.526 - ETA: 3s - loss: 6.6652 - acc: 0.526 - ETA: 3s - loss: 6.6712 - acc: 0.526 - ETA: 3s - loss: 6.6782 - acc: 0.526 - ETA: 3s - loss: 6.6558 - acc: 0.528 - ETA: 3s - loss: 6.6816 - acc: 0.526 - ETA: 3s - loss: 6.7301 - acc: 0.523 - ETA: 3s - loss: 6.6933 - acc: 0.525 - ETA: 3s - loss: 6.7184 - acc: 0.523 - ETA: 3s - loss: 6.6719 - acc: 0.526 - ETA: 2s - loss: 6.6696 - acc: 0.526 - ETA: 2s - loss: 6.6675 - acc: 0.526 - ETA: 2s - loss: 6.6807 - acc: 0.526 - ETA: 2s - loss: 6.6554 - acc: 0.527 - ETA: 2s - loss: 6.6671 - acc: 0.526 - ETA: 2s - loss: 6.6792 - acc: 0.526 - ETA: 2s - loss: 6.6626 - acc: 0.526 - ETA: 2s - loss: 6.6309 - acc: 0.528 - ETA: 2s - loss: 6.6244 - acc: 0.528 - ETA: 2s - loss: 6.6596 - acc: 0.526 - ETA: 2s - loss: 6.6556 - acc: 0.526 - ETA: 2s - loss: 6.6307 - acc: 0.528 - ETA: 2s - loss: 6.6231 - acc: 0.528 - ETA: 2s - loss: 6.6029 - acc: 0.530 - ETA: 2s - loss: 6.5952 - acc: 0.530 - ETA: 2s - loss: 6.5413 - acc: 0.534 - ETA: 2s - loss: 6.5414 - acc: 0.534 - ETA: 1s - loss: 6.5429 - acc: 0.534 - ETA: 1s - loss: 6.5234 - acc: 0.536 - ETA: 1s - loss: 6.4876 - acc: 0.538 - ETA: 1s - loss: 6.5120 - acc: 0.536 - ETA: 1s - loss: 6.4867 - acc: 0.537 - ETA: 1s - loss: 6.5022 - acc: 0.535 - ETA: 1s - loss: 6.5155 - acc: 0.535 - ETA: 1s - loss: 6.5240 - acc: 0.534 - ETA: 1s - loss: 6.5209 - acc: 0.534 - ETA: 1s - loss: 6.5012 - acc: 0.535 - ETA: 1s - loss: 6.5131 - acc: 0.535 - ETA: 1s - loss: 6.5190 - acc: 0.534 - ETA: 1s - loss: 6.5311 - acc: 0.533 - ETA: 1s - loss: 6.5099 - acc: 0.534 - ETA: 1s - loss: 6.5168 - acc: 0.534 - ETA: 1s - loss: 6.5365 - acc: 0.533 - ETA: 1s - loss: 6.5454 - acc: 0.533 - ETA: 0s - loss: 6.5472 - acc: 0.533 - ETA: 0s - loss: 6.5594 - acc: 0.531 - ETA: 0s - loss: 6.5595 - acc: 0.531 - ETA: 0s - loss: 6.5429 - acc: 0.532 - ETA: 0s - loss: 6.5316 - acc: 0.533 - ETA: 0s - loss: 6.5410 - acc: 0.532 - ETA: 0s - loss: 6.5453 - acc: 0.532 - ETA: 0s - loss: 6.5481 - acc: 0.532 - ETA: 0s - loss: 6.5380 - acc: 0.532 - ETA: 0s - loss: 6.5310 - acc: 0.532 - ETA: 0s - loss: 6.5232 - acc: 0.532 - ETA: 0s - loss: 6.5433 - acc: 0.531 - ETA: 0s - loss: 6.5522 - acc: 0.531 - ETA: 0s - loss: 6.5421 - acc: 0.531 - ETA: 0s - loss: 6.5275 - acc: 0.531 - ETA: 0s - loss: 6.5270 - acc: 0.531 - ETA: 0s - loss: 6.5041 - acc: 0.532 - ETA: 0s - loss: 6.5047 - acc: 0.533 - 5s 817us/step - loss: 6.5026 - acc: 0.5332 - val_loss: 6.9884 - val_acc: 0.4551\n",
      "\n",
      "Epoch 00005: val_loss improved from 7.25397 to 6.98840, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 6/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 7.4396 - acc: 0.500 - ETA: 5s - loss: 7.5331 - acc: 0.462 - ETA: 5s - loss: 6.9446 - acc: 0.514 - ETA: 5s - loss: 6.6638 - acc: 0.536 - ETA: 5s - loss: 7.0292 - acc: 0.516 - ETA: 5s - loss: 6.6318 - acc: 0.528 - ETA: 4s - loss: 6.4083 - acc: 0.541 - ETA: 4s - loss: 6.5413 - acc: 0.537 - ETA: 4s - loss: 6.4003 - acc: 0.543 - ETA: 4s - loss: 6.3091 - acc: 0.552 - ETA: 4s - loss: 6.3796 - acc: 0.547 - ETA: 4s - loss: 6.3174 - acc: 0.552 - ETA: 4s - loss: 6.3264 - acc: 0.554 - ETA: 4s - loss: 6.2537 - acc: 0.560 - ETA: 4s - loss: 6.2333 - acc: 0.562 - ETA: 4s - loss: 6.2024 - acc: 0.565 - ETA: 4s - loss: 6.1259 - acc: 0.570 - ETA: 4s - loss: 6.1694 - acc: 0.568 - ETA: 3s - loss: 6.2432 - acc: 0.564 - ETA: 3s - loss: 6.2160 - acc: 0.566 - ETA: 3s - loss: 6.2295 - acc: 0.567 - ETA: 3s - loss: 6.2225 - acc: 0.568 - ETA: 3s - loss: 6.2596 - acc: 0.564 - ETA: 3s - loss: 6.2428 - acc: 0.566 - ETA: 3s - loss: 6.1918 - acc: 0.570 - ETA: 3s - loss: 6.1745 - acc: 0.571 - ETA: 3s - loss: 6.1459 - acc: 0.571 - ETA: 3s - loss: 6.1649 - acc: 0.569 - ETA: 3s - loss: 6.1430 - acc: 0.571 - ETA: 3s - loss: 6.1748 - acc: 0.570 - ETA: 3s - loss: 6.1304 - acc: 0.571 - ETA: 3s - loss: 6.0906 - acc: 0.574 - ETA: 3s - loss: 6.1484 - acc: 0.571 - ETA: 3s - loss: 6.1437 - acc: 0.571 - ETA: 2s - loss: 6.1206 - acc: 0.573 - ETA: 2s - loss: 6.1260 - acc: 0.572 - ETA: 2s - loss: 6.1530 - acc: 0.572 - ETA: 2s - loss: 6.1641 - acc: 0.571 - ETA: 2s - loss: 6.1728 - acc: 0.571 - ETA: 2s - loss: 6.1927 - acc: 0.570 - ETA: 2s - loss: 6.2124 - acc: 0.568 - ETA: 2s - loss: 6.1963 - acc: 0.567 - ETA: 2s - loss: 6.1739 - acc: 0.569 - ETA: 2s - loss: 6.1958 - acc: 0.568 - ETA: 2s - loss: 6.2356 - acc: 0.566 - ETA: 2s - loss: 6.2226 - acc: 0.566 - ETA: 2s - loss: 6.2069 - acc: 0.566 - ETA: 2s - loss: 6.1807 - acc: 0.569 - ETA: 2s - loss: 6.2059 - acc: 0.568 - ETA: 2s - loss: 6.2124 - acc: 0.567 - ETA: 2s - loss: 6.2135 - acc: 0.568 - ETA: 1s - loss: 6.1958 - acc: 0.569 - ETA: 1s - loss: 6.2292 - acc: 0.567 - ETA: 1s - loss: 6.2234 - acc: 0.568 - ETA: 1s - loss: 6.1997 - acc: 0.569 - ETA: 1s - loss: 6.2298 - acc: 0.566 - ETA: 1s - loss: 6.2366 - acc: 0.567 - ETA: 1s - loss: 6.2288 - acc: 0.568 - ETA: 1s - loss: 6.2144 - acc: 0.569 - ETA: 1s - loss: 6.2039 - acc: 0.569 - ETA: 1s - loss: 6.2165 - acc: 0.569 - ETA: 1s - loss: 6.2092 - acc: 0.568 - ETA: 1s - loss: 6.2150 - acc: 0.568 - ETA: 1s - loss: 6.2200 - acc: 0.567 - ETA: 1s - loss: 6.2190 - acc: 0.566 - ETA: 1s - loss: 6.2159 - acc: 0.567 - ETA: 1s - loss: 6.2138 - acc: 0.567 - ETA: 1s - loss: 6.2088 - acc: 0.568 - ETA: 0s - loss: 6.2208 - acc: 0.567 - ETA: 0s - loss: 6.2211 - acc: 0.567 - ETA: 0s - loss: 6.2089 - acc: 0.568 - ETA: 0s - loss: 6.2031 - acc: 0.568 - ETA: 0s - loss: 6.1956 - acc: 0.569 - ETA: 0s - loss: 6.1855 - acc: 0.569 - ETA: 0s - loss: 6.1771 - acc: 0.570 - ETA: 0s - loss: 6.1762 - acc: 0.569 - ETA: 0s - loss: 6.1673 - acc: 0.570 - ETA: 0s - loss: 6.1720 - acc: 0.569 - ETA: 0s - loss: 6.1916 - acc: 0.568 - ETA: 0s - loss: 6.1810 - acc: 0.568 - ETA: 0s - loss: 6.1811 - acc: 0.568 - ETA: 0s - loss: 6.1994 - acc: 0.567 - ETA: 0s - loss: 6.2120 - acc: 0.566 - ETA: 0s - loss: 6.2182 - acc: 0.566 - 5s 811us/step - loss: 6.2187 - acc: 0.5659 - val_loss: 6.8885 - val_acc: 0.4659\n",
      "\n",
      "Epoch 00006: val_loss improved from 6.98840 to 6.88849, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 7/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 7.4197 - acc: 0.500 - ETA: 5s - loss: 6.6681 - acc: 0.520 - ETA: 4s - loss: 6.7782 - acc: 0.533 - ETA: 5s - loss: 6.2279 - acc: 0.570 - ETA: 4s - loss: 6.0519 - acc: 0.587 - ETA: 4s - loss: 6.2809 - acc: 0.575 - ETA: 4s - loss: 6.1612 - acc: 0.585 - ETA: 4s - loss: 6.1094 - acc: 0.591 - ETA: 4s - loss: 6.0751 - acc: 0.587 - ETA: 4s - loss: 6.1051 - acc: 0.587 - ETA: 4s - loss: 5.9544 - acc: 0.596 - ETA: 4s - loss: 5.9197 - acc: 0.597 - ETA: 4s - loss: 6.0630 - acc: 0.590 - ETA: 4s - loss: 6.0675 - acc: 0.590 - ETA: 4s - loss: 6.0841 - acc: 0.590 - ETA: 4s - loss: 6.0614 - acc: 0.590 - ETA: 4s - loss: 5.9850 - acc: 0.595 - ETA: 3s - loss: 6.0297 - acc: 0.592 - ETA: 3s - loss: 6.1116 - acc: 0.588 - ETA: 3s - loss: 6.0761 - acc: 0.590 - ETA: 3s - loss: 6.1440 - acc: 0.586 - ETA: 3s - loss: 6.1699 - acc: 0.586 - ETA: 3s - loss: 6.1776 - acc: 0.585 - ETA: 3s - loss: 6.1835 - acc: 0.585 - ETA: 3s - loss: 6.1372 - acc: 0.587 - ETA: 3s - loss: 6.1530 - acc: 0.586 - ETA: 3s - loss: 6.1544 - acc: 0.585 - ETA: 3s - loss: 6.1870 - acc: 0.582 - ETA: 3s - loss: 6.1779 - acc: 0.582 - ETA: 3s - loss: 6.1713 - acc: 0.582 - ETA: 3s - loss: 6.1321 - acc: 0.585 - ETA: 3s - loss: 6.1498 - acc: 0.585 - ETA: 3s - loss: 6.1270 - acc: 0.585 - ETA: 3s - loss: 6.1042 - acc: 0.586 - ETA: 3s - loss: 6.0515 - acc: 0.589 - ETA: 3s - loss: 6.0660 - acc: 0.589 - ETA: 2s - loss: 6.0807 - acc: 0.588 - ETA: 2s - loss: 6.0600 - acc: 0.590 - ETA: 2s - loss: 6.0281 - acc: 0.591 - ETA: 2s - loss: 6.0109 - acc: 0.593 - ETA: 2s - loss: 6.0176 - acc: 0.592 - ETA: 2s - loss: 6.0034 - acc: 0.593 - ETA: 2s - loss: 6.0347 - acc: 0.591 - ETA: 2s - loss: 6.0536 - acc: 0.590 - ETA: 2s - loss: 6.0247 - acc: 0.591 - ETA: 2s - loss: 6.0032 - acc: 0.593 - ETA: 2s - loss: 5.9928 - acc: 0.593 - ETA: 2s - loss: 5.9966 - acc: 0.593 - ETA: 2s - loss: 5.9764 - acc: 0.594 - ETA: 2s - loss: 5.9644 - acc: 0.594 - ETA: 2s - loss: 5.9880 - acc: 0.593 - ETA: 2s - loss: 5.9648 - acc: 0.594 - ETA: 2s - loss: 5.9695 - acc: 0.595 - ETA: 1s - loss: 5.9921 - acc: 0.593 - ETA: 1s - loss: 6.0095 - acc: 0.592 - ETA: 1s - loss: 5.9964 - acc: 0.592 - ETA: 1s - loss: 6.0054 - acc: 0.591 - ETA: 1s - loss: 6.0065 - acc: 0.591 - ETA: 1s - loss: 6.0153 - acc: 0.590 - ETA: 1s - loss: 6.0091 - acc: 0.591 - ETA: 1s - loss: 6.0198 - acc: 0.590 - ETA: 1s - loss: 6.0198 - acc: 0.590 - ETA: 1s - loss: 6.0425 - acc: 0.589 - ETA: 1s - loss: 6.0542 - acc: 0.588 - ETA: 1s - loss: 6.0691 - acc: 0.587 - ETA: 1s - loss: 6.0620 - acc: 0.587 - ETA: 1s - loss: 6.0542 - acc: 0.588 - ETA: 1s - loss: 6.0733 - acc: 0.587 - ETA: 1s - loss: 6.0802 - acc: 0.586 - ETA: 1s - loss: 6.0994 - acc: 0.585 - ETA: 0s - loss: 6.1172 - acc: 0.583 - ETA: 0s - loss: 6.1212 - acc: 0.583 - ETA: 0s - loss: 6.1132 - acc: 0.584 - ETA: 0s - loss: 6.0995 - acc: 0.585 - ETA: 0s - loss: 6.0856 - acc: 0.586 - ETA: 0s - loss: 6.0893 - acc: 0.585 - ETA: 0s - loss: 6.0891 - acc: 0.586 - ETA: 0s - loss: 6.0906 - acc: 0.586 - ETA: 0s - loss: 6.0932 - acc: 0.585 - ETA: 0s - loss: 6.0762 - acc: 0.586 - ETA: 0s - loss: 6.0629 - acc: 0.587 - ETA: 0s - loss: 6.0722 - acc: 0.586 - ETA: 0s - loss: 6.0710 - acc: 0.585 - ETA: 0s - loss: 6.0764 - acc: 0.585 - ETA: 0s - loss: 6.0670 - acc: 0.585 - ETA: 0s - loss: 6.0661 - acc: 0.585 - 5s 815us/step - loss: 6.0584 - acc: 0.5852 - val_loss: 6.8085 - val_acc: 0.4814\n",
      "\n",
      "Epoch 00007: val_loss improved from 6.88849 to 6.80854, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 8/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 4.2754 - acc: 0.700 - ETA: 5s - loss: 5.3361 - acc: 0.640 - ETA: 4s - loss: 5.2278 - acc: 0.650 - ETA: 4s - loss: 5.5585 - acc: 0.630 - ETA: 4s - loss: 5.7828 - acc: 0.608 - ETA: 4s - loss: 5.6971 - acc: 0.610 - ETA: 4s - loss: 5.6459 - acc: 0.617 - ETA: 4s - loss: 5.7285 - acc: 0.613 - ETA: 4s - loss: 5.8223 - acc: 0.611 - ETA: 4s - loss: 5.6774 - acc: 0.618 - ETA: 4s - loss: 5.6085 - acc: 0.619 - ETA: 4s - loss: 5.4942 - acc: 0.625 - ETA: 4s - loss: 5.5208 - acc: 0.625 - ETA: 4s - loss: 5.6366 - acc: 0.619 - ETA: 4s - loss: 5.5953 - acc: 0.623 - ETA: 4s - loss: 5.6271 - acc: 0.622 - ETA: 4s - loss: 5.7615 - acc: 0.614 - ETA: 4s - loss: 5.7141 - acc: 0.614 - ETA: 4s - loss: 5.7353 - acc: 0.612 - ETA: 3s - loss: 5.7490 - acc: 0.609 - ETA: 3s - loss: 5.8065 - acc: 0.606 - ETA: 3s - loss: 5.8132 - acc: 0.606 - ETA: 3s - loss: 5.7531 - acc: 0.608 - ETA: 3s - loss: 5.7078 - acc: 0.610 - ETA: 3s - loss: 5.6924 - acc: 0.611 - ETA: 3s - loss: 5.7161 - acc: 0.609 - ETA: 3s - loss: 5.6805 - acc: 0.611 - ETA: 3s - loss: 5.6711 - acc: 0.612 - ETA: 3s - loss: 5.6743 - acc: 0.611 - ETA: 3s - loss: 5.6431 - acc: 0.614 - ETA: 3s - loss: 5.6895 - acc: 0.610 - ETA: 3s - loss: 5.6788 - acc: 0.610 - ETA: 3s - loss: 5.6616 - acc: 0.610 - ETA: 3s - loss: 5.6727 - acc: 0.609 - ETA: 3s - loss: 5.6555 - acc: 0.610 - ETA: 3s - loss: 5.6583 - acc: 0.610 - ETA: 2s - loss: 5.7085 - acc: 0.606 - ETA: 2s - loss: 5.6921 - acc: 0.608 - ETA: 2s - loss: 5.6934 - acc: 0.608 - ETA: 2s - loss: 5.7061 - acc: 0.607 - ETA: 2s - loss: 5.6713 - acc: 0.608 - ETA: 2s - loss: 5.6766 - acc: 0.607 - ETA: 2s - loss: 5.6762 - acc: 0.607 - ETA: 2s - loss: 5.6615 - acc: 0.608 - ETA: 2s - loss: 5.6309 - acc: 0.610 - ETA: 2s - loss: 5.6634 - acc: 0.608 - ETA: 2s - loss: 5.6890 - acc: 0.607 - ETA: 2s - loss: 5.7267 - acc: 0.605 - ETA: 2s - loss: 5.7214 - acc: 0.605 - ETA: 2s - loss: 5.7257 - acc: 0.604 - ETA: 2s - loss: 5.7135 - acc: 0.605 - ETA: 2s - loss: 5.7196 - acc: 0.605 - ETA: 1s - loss: 5.7245 - acc: 0.605 - ETA: 1s - loss: 5.7399 - acc: 0.603 - ETA: 1s - loss: 5.7595 - acc: 0.603 - ETA: 1s - loss: 5.7728 - acc: 0.602 - ETA: 1s - loss: 5.7733 - acc: 0.602 - ETA: 1s - loss: 5.8076 - acc: 0.600 - ETA: 1s - loss: 5.8210 - acc: 0.599 - ETA: 1s - loss: 5.8177 - acc: 0.599 - ETA: 1s - loss: 5.8290 - acc: 0.598 - ETA: 1s - loss: 5.8421 - acc: 0.598 - ETA: 1s - loss: 5.8451 - acc: 0.598 - ETA: 1s - loss: 5.8359 - acc: 0.598 - ETA: 1s - loss: 5.8244 - acc: 0.599 - ETA: 1s - loss: 5.8298 - acc: 0.599 - ETA: 1s - loss: 5.8320 - acc: 0.598 - ETA: 1s - loss: 5.8366 - acc: 0.597 - ETA: 0s - loss: 5.8462 - acc: 0.597 - ETA: 0s - loss: 5.8565 - acc: 0.596 - ETA: 0s - loss: 5.8694 - acc: 0.595 - ETA: 0s - loss: 5.8887 - acc: 0.594 - ETA: 0s - loss: 5.8758 - acc: 0.595 - ETA: 0s - loss: 5.9050 - acc: 0.593 - ETA: 0s - loss: 5.9001 - acc: 0.594 - ETA: 0s - loss: 5.8790 - acc: 0.595 - ETA: 0s - loss: 5.8790 - acc: 0.595 - ETA: 0s - loss: 5.8946 - acc: 0.594 - ETA: 0s - loss: 5.9000 - acc: 0.594 - ETA: 0s - loss: 5.8818 - acc: 0.595 - ETA: 0s - loss: 5.8677 - acc: 0.596 - ETA: 0s - loss: 5.8612 - acc: 0.597 - ETA: 0s - loss: 5.8547 - acc: 0.597 - ETA: 0s - loss: 5.8657 - acc: 0.597 - ETA: 0s - loss: 5.8663 - acc: 0.597 - 5s 812us/step - loss: 5.8570 - acc: 0.5978 - val_loss: 6.6518 - val_acc: 0.4958\n",
      "\n",
      "Epoch 00008: val_loss improved from 6.80854 to 6.65180, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 9/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 2.4563 - acc: 0.800 - ETA: 5s - loss: 4.7044 - acc: 0.690 - ETA: 4s - loss: 5.0568 - acc: 0.655 - ETA: 4s - loss: 5.4826 - acc: 0.626 - ETA: 4s - loss: 5.4455 - acc: 0.629 - ETA: 4s - loss: 5.3124 - acc: 0.642 - ETA: 4s - loss: 5.5793 - acc: 0.624 - ETA: 4s - loss: 5.7326 - acc: 0.617 - ETA: 4s - loss: 5.5055 - acc: 0.633 - ETA: 4s - loss: 5.6090 - acc: 0.629 - ETA: 4s - loss: 5.6933 - acc: 0.625 - ETA: 4s - loss: 5.6219 - acc: 0.631 - ETA: 4s - loss: 5.4642 - acc: 0.639 - ETA: 4s - loss: 5.4711 - acc: 0.637 - ETA: 4s - loss: 5.4855 - acc: 0.636 - ETA: 4s - loss: 5.4964 - acc: 0.637 - ETA: 3s - loss: 5.4505 - acc: 0.640 - ETA: 3s - loss: 5.5794 - acc: 0.632 - ETA: 3s - loss: 5.6443 - acc: 0.627 - ETA: 3s - loss: 5.6005 - acc: 0.629 - ETA: 3s - loss: 5.5870 - acc: 0.629 - ETA: 3s - loss: 5.5862 - acc: 0.628 - ETA: 3s - loss: 5.5497 - acc: 0.631 - ETA: 3s - loss: 5.5481 - acc: 0.631 - ETA: 3s - loss: 5.5848 - acc: 0.628 - ETA: 3s - loss: 5.5796 - acc: 0.627 - ETA: 3s - loss: 5.5428 - acc: 0.629 - ETA: 3s - loss: 5.5180 - acc: 0.631 - ETA: 3s - loss: 5.5616 - acc: 0.629 - ETA: 3s - loss: 5.5928 - acc: 0.627 - ETA: 3s - loss: 5.5559 - acc: 0.630 - ETA: 3s - loss: 5.6397 - acc: 0.625 - ETA: 3s - loss: 5.6318 - acc: 0.625 - ETA: 2s - loss: 5.6471 - acc: 0.624 - ETA: 2s - loss: 5.6693 - acc: 0.622 - ETA: 2s - loss: 5.6636 - acc: 0.623 - ETA: 2s - loss: 5.6387 - acc: 0.624 - ETA: 2s - loss: 5.6411 - acc: 0.623 - ETA: 2s - loss: 5.6419 - acc: 0.622 - ETA: 2s - loss: 5.6512 - acc: 0.622 - ETA: 2s - loss: 5.6560 - acc: 0.622 - ETA: 2s - loss: 5.6371 - acc: 0.623 - ETA: 2s - loss: 5.6212 - acc: 0.625 - ETA: 2s - loss: 5.6240 - acc: 0.624 - ETA: 2s - loss: 5.5897 - acc: 0.626 - ETA: 2s - loss: 5.5943 - acc: 0.626 - ETA: 2s - loss: 5.6147 - acc: 0.625 - ETA: 2s - loss: 5.6020 - acc: 0.625 - ETA: 2s - loss: 5.6013 - acc: 0.625 - ETA: 2s - loss: 5.6096 - acc: 0.624 - ETA: 1s - loss: 5.6165 - acc: 0.624 - ETA: 1s - loss: 5.6271 - acc: 0.623 - ETA: 1s - loss: 5.6346 - acc: 0.622 - ETA: 1s - loss: 5.6362 - acc: 0.621 - ETA: 1s - loss: 5.6475 - acc: 0.621 - ETA: 1s - loss: 5.6522 - acc: 0.621 - ETA: 1s - loss: 5.6729 - acc: 0.619 - ETA: 1s - loss: 5.6695 - acc: 0.620 - ETA: 1s - loss: 5.6814 - acc: 0.619 - ETA: 1s - loss: 5.7015 - acc: 0.618 - ETA: 1s - loss: 5.6919 - acc: 0.619 - ETA: 1s - loss: 5.6895 - acc: 0.619 - ETA: 1s - loss: 5.6905 - acc: 0.619 - ETA: 1s - loss: 5.7003 - acc: 0.618 - ETA: 1s - loss: 5.6887 - acc: 0.618 - ETA: 1s - loss: 5.6793 - acc: 0.619 - ETA: 1s - loss: 5.6831 - acc: 0.619 - ETA: 0s - loss: 5.6791 - acc: 0.619 - ETA: 0s - loss: 5.6718 - acc: 0.619 - ETA: 0s - loss: 5.6726 - acc: 0.619 - ETA: 0s - loss: 5.6806 - acc: 0.619 - ETA: 0s - loss: 5.7106 - acc: 0.617 - ETA: 0s - loss: 5.7132 - acc: 0.617 - ETA: 0s - loss: 5.7127 - acc: 0.617 - ETA: 0s - loss: 5.7209 - acc: 0.616 - ETA: 0s - loss: 5.7150 - acc: 0.617 - ETA: 0s - loss: 5.7211 - acc: 0.617 - ETA: 0s - loss: 5.7183 - acc: 0.617 - ETA: 0s - loss: 5.7208 - acc: 0.617 - ETA: 0s - loss: 5.7124 - acc: 0.617 - ETA: 0s - loss: 5.7136 - acc: 0.617 - ETA: 0s - loss: 5.7290 - acc: 0.616 - ETA: 0s - loss: 5.7325 - acc: 0.616 - ETA: 0s - loss: 5.7341 - acc: 0.616 - 5s 813us/step - loss: 5.7377 - acc: 0.6162 - val_loss: 6.6021 - val_acc: 0.4922\n",
      "\n",
      "Epoch 00009: val_loss improved from 6.65180 to 6.60206, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 10/20\n",
      "6680/6680 [==============================] - ETA: 4s - loss: 3.9362 - acc: 0.750 - ETA: 5s - loss: 5.0183 - acc: 0.675 - ETA: 5s - loss: 5.1949 - acc: 0.657 - ETA: 5s - loss: 5.4922 - acc: 0.645 - ETA: 5s - loss: 5.9668 - acc: 0.620 - ETA: 5s - loss: 6.1073 - acc: 0.605 - ETA: 5s - loss: 6.0665 - acc: 0.606 - ETA: 4s - loss: 6.0170 - acc: 0.607 - ETA: 4s - loss: 5.9818 - acc: 0.611 - ETA: 4s - loss: 5.8981 - acc: 0.615 - ETA: 4s - loss: 5.9223 - acc: 0.610 - ETA: 4s - loss: 5.8346 - acc: 0.614 - ETA: 4s - loss: 5.7301 - acc: 0.621 - ETA: 4s - loss: 5.7995 - acc: 0.616 - ETA: 4s - loss: 5.7003 - acc: 0.621 - ETA: 4s - loss: 5.6623 - acc: 0.623 - ETA: 4s - loss: 5.6308 - acc: 0.626 - ETA: 4s - loss: 5.5012 - acc: 0.634 - ETA: 4s - loss: 5.4656 - acc: 0.636 - ETA: 4s - loss: 5.5371 - acc: 0.632 - ETA: 4s - loss: 5.5503 - acc: 0.631 - ETA: 3s - loss: 5.5689 - acc: 0.631 - ETA: 3s - loss: 5.5389 - acc: 0.633 - ETA: 3s - loss: 5.5228 - acc: 0.633 - ETA: 3s - loss: 5.5269 - acc: 0.633 - ETA: 3s - loss: 5.4742 - acc: 0.636 - ETA: 3s - loss: 5.4885 - acc: 0.634 - ETA: 3s - loss: 5.5189 - acc: 0.631 - ETA: 3s - loss: 5.4596 - acc: 0.635 - ETA: 3s - loss: 5.5106 - acc: 0.632 - ETA: 3s - loss: 5.4826 - acc: 0.634 - ETA: 3s - loss: 5.5186 - acc: 0.632 - ETA: 3s - loss: 5.5347 - acc: 0.631 - ETA: 3s - loss: 5.5706 - acc: 0.629 - ETA: 3s - loss: 5.6047 - acc: 0.628 - ETA: 3s - loss: 5.6080 - acc: 0.628 - ETA: 2s - loss: 5.5742 - acc: 0.631 - ETA: 2s - loss: 5.5995 - acc: 0.628 - ETA: 2s - loss: 5.6118 - acc: 0.627 - ETA: 2s - loss: 5.6417 - acc: 0.625 - ETA: 2s - loss: 5.6741 - acc: 0.622 - ETA: 2s - loss: 5.6843 - acc: 0.621 - ETA: 2s - loss: 5.6956 - acc: 0.621 - ETA: 2s - loss: 5.7459 - acc: 0.618 - ETA: 2s - loss: 5.7170 - acc: 0.620 - ETA: 2s - loss: 5.7045 - acc: 0.621 - ETA: 2s - loss: 5.6999 - acc: 0.621 - ETA: 2s - loss: 5.7188 - acc: 0.620 - ETA: 2s - loss: 5.6936 - acc: 0.621 - ETA: 2s - loss: 5.6944 - acc: 0.621 - ETA: 2s - loss: 5.6907 - acc: 0.621 - ETA: 2s - loss: 5.6761 - acc: 0.622 - ETA: 2s - loss: 5.6767 - acc: 0.622 - ETA: 2s - loss: 5.6617 - acc: 0.623 - ETA: 1s - loss: 5.6794 - acc: 0.622 - ETA: 1s - loss: 5.6509 - acc: 0.624 - ETA: 1s - loss: 5.6632 - acc: 0.622 - ETA: 1s - loss: 5.6726 - acc: 0.622 - ETA: 1s - loss: 5.6569 - acc: 0.623 - ETA: 1s - loss: 5.6462 - acc: 0.623 - ETA: 1s - loss: 5.6420 - acc: 0.623 - ETA: 1s - loss: 5.6680 - acc: 0.621 - ETA: 1s - loss: 5.6621 - acc: 0.621 - ETA: 1s - loss: 5.6572 - acc: 0.622 - ETA: 1s - loss: 5.6347 - acc: 0.623 - ETA: 1s - loss: 5.6040 - acc: 0.625 - ETA: 1s - loss: 5.6016 - acc: 0.625 - ETA: 1s - loss: 5.6263 - acc: 0.624 - ETA: 1s - loss: 5.6400 - acc: 0.623 - ETA: 1s - loss: 5.6397 - acc: 0.623 - ETA: 0s - loss: 5.6576 - acc: 0.622 - ETA: 0s - loss: 5.6577 - acc: 0.622 - ETA: 0s - loss: 5.6541 - acc: 0.622 - ETA: 0s - loss: 5.6399 - acc: 0.623 - ETA: 0s - loss: 5.6384 - acc: 0.623 - ETA: 0s - loss: 5.6359 - acc: 0.624 - ETA: 0s - loss: 5.6307 - acc: 0.624 - ETA: 0s - loss: 5.6237 - acc: 0.624 - ETA: 0s - loss: 5.6367 - acc: 0.624 - ETA: 0s - loss: 5.6348 - acc: 0.623 - ETA: 0s - loss: 5.6483 - acc: 0.622 - ETA: 0s - loss: 5.6465 - acc: 0.622 - ETA: 0s - loss: 5.6416 - acc: 0.622 - ETA: 0s - loss: 5.6541 - acc: 0.621 - ETA: 0s - loss: 5.6510 - acc: 0.621 - ETA: 0s - loss: 5.6495 - acc: 0.621 - 5s 813us/step - loss: 5.6356 - acc: 0.6222 - val_loss: 6.4613 - val_acc: 0.5042\n",
      "\n",
      "Epoch 00010: val_loss improved from 6.60206 to 6.46131, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 11/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 6s - loss: 4.9277 - acc: 0.650 - ETA: 5s - loss: 4.6687 - acc: 0.700 - ETA: 5s - loss: 5.1584 - acc: 0.668 - ETA: 5s - loss: 5.2885 - acc: 0.658 - ETA: 4s - loss: 4.8871 - acc: 0.684 - ETA: 4s - loss: 5.1649 - acc: 0.665 - ETA: 4s - loss: 5.0479 - acc: 0.672 - ETA: 4s - loss: 4.7947 - acc: 0.687 - ETA: 4s - loss: 4.8748 - acc: 0.681 - ETA: 4s - loss: 4.9673 - acc: 0.673 - ETA: 4s - loss: 4.9942 - acc: 0.671 - ETA: 4s - loss: 5.1169 - acc: 0.663 - ETA: 4s - loss: 5.2291 - acc: 0.655 - ETA: 4s - loss: 5.2117 - acc: 0.654 - ETA: 4s - loss: 5.2957 - acc: 0.650 - ETA: 4s - loss: 5.2925 - acc: 0.651 - ETA: 4s - loss: 5.2804 - acc: 0.652 - ETA: 3s - loss: 5.3331 - acc: 0.646 - ETA: 3s - loss: 5.4315 - acc: 0.640 - ETA: 3s - loss: 5.4017 - acc: 0.642 - ETA: 3s - loss: 5.4180 - acc: 0.640 - ETA: 3s - loss: 5.4097 - acc: 0.642 - ETA: 3s - loss: 5.3569 - acc: 0.646 - ETA: 3s - loss: 5.3610 - acc: 0.647 - ETA: 3s - loss: 5.3480 - acc: 0.649 - ETA: 3s - loss: 5.3015 - acc: 0.651 - ETA: 3s - loss: 5.3192 - acc: 0.649 - ETA: 3s - loss: 5.3413 - acc: 0.647 - ETA: 3s - loss: 5.3181 - acc: 0.649 - ETA: 3s - loss: 5.3475 - acc: 0.646 - ETA: 3s - loss: 5.3546 - acc: 0.645 - ETA: 3s - loss: 5.3848 - acc: 0.644 - ETA: 3s - loss: 5.4260 - acc: 0.642 - ETA: 3s - loss: 5.4384 - acc: 0.641 - ETA: 2s - loss: 5.4450 - acc: 0.641 - ETA: 2s - loss: 5.4444 - acc: 0.641 - ETA: 2s - loss: 5.4361 - acc: 0.641 - ETA: 2s - loss: 5.3940 - acc: 0.644 - ETA: 2s - loss: 5.3809 - acc: 0.645 - ETA: 2s - loss: 5.3861 - acc: 0.644 - ETA: 2s - loss: 5.3626 - acc: 0.645 - ETA: 2s - loss: 5.3490 - acc: 0.645 - ETA: 2s - loss: 5.3195 - acc: 0.647 - ETA: 2s - loss: 5.3085 - acc: 0.649 - ETA: 2s - loss: 5.3289 - acc: 0.648 - ETA: 2s - loss: 5.3577 - acc: 0.646 - ETA: 2s - loss: 5.3751 - acc: 0.645 - ETA: 2s - loss: 5.3818 - acc: 0.644 - ETA: 2s - loss: 5.3792 - acc: 0.645 - ETA: 2s - loss: 5.3811 - acc: 0.644 - ETA: 1s - loss: 5.3742 - acc: 0.645 - ETA: 1s - loss: 5.3977 - acc: 0.643 - ETA: 1s - loss: 5.4315 - acc: 0.639 - ETA: 1s - loss: 5.4375 - acc: 0.639 - ETA: 1s - loss: 5.4221 - acc: 0.639 - ETA: 1s - loss: 5.4272 - acc: 0.639 - ETA: 1s - loss: 5.4377 - acc: 0.638 - ETA: 1s - loss: 5.4363 - acc: 0.638 - ETA: 1s - loss: 5.4493 - acc: 0.637 - ETA: 1s - loss: 5.4394 - acc: 0.638 - ETA: 1s - loss: 5.4351 - acc: 0.638 - ETA: 1s - loss: 5.4470 - acc: 0.638 - ETA: 1s - loss: 5.4470 - acc: 0.638 - ETA: 1s - loss: 5.4380 - acc: 0.639 - ETA: 1s - loss: 5.4349 - acc: 0.639 - ETA: 1s - loss: 5.4336 - acc: 0.639 - ETA: 1s - loss: 5.4388 - acc: 0.638 - ETA: 0s - loss: 5.4470 - acc: 0.637 - ETA: 0s - loss: 5.4514 - acc: 0.637 - ETA: 0s - loss: 5.4580 - acc: 0.636 - ETA: 0s - loss: 5.4592 - acc: 0.636 - ETA: 0s - loss: 5.4782 - acc: 0.635 - ETA: 0s - loss: 5.4704 - acc: 0.635 - ETA: 0s - loss: 5.4628 - acc: 0.636 - ETA: 0s - loss: 5.4470 - acc: 0.637 - ETA: 0s - loss: 5.4641 - acc: 0.636 - ETA: 0s - loss: 5.4660 - acc: 0.636 - ETA: 0s - loss: 5.4570 - acc: 0.636 - ETA: 0s - loss: 5.4709 - acc: 0.636 - ETA: 0s - loss: 5.4847 - acc: 0.635 - ETA: 0s - loss: 5.4958 - acc: 0.634 - ETA: 0s - loss: 5.5027 - acc: 0.634 - ETA: 0s - loss: 5.5124 - acc: 0.633 - ETA: 0s - loss: 5.5109 - acc: 0.633 - 5s 810us/step - loss: 5.4950 - acc: 0.6346 - val_loss: 6.3765 - val_acc: 0.5126\n",
      "\n",
      "Epoch 00011: val_loss improved from 6.46131 to 6.37647, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 12/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 8.1277 - acc: 0.450 - ETA: 5s - loss: 4.4616 - acc: 0.710 - ETA: 5s - loss: 4.8150 - acc: 0.688 - ETA: 4s - loss: 5.5189 - acc: 0.646 - ETA: 4s - loss: 5.4689 - acc: 0.650 - ETA: 4s - loss: 5.3884 - acc: 0.657 - ETA: 4s - loss: 5.4618 - acc: 0.650 - ETA: 4s - loss: 5.3307 - acc: 0.651 - ETA: 4s - loss: 5.1738 - acc: 0.663 - ETA: 4s - loss: 5.1468 - acc: 0.664 - ETA: 4s - loss: 5.1396 - acc: 0.664 - ETA: 4s - loss: 5.2440 - acc: 0.657 - ETA: 4s - loss: 5.2158 - acc: 0.659 - ETA: 4s - loss: 5.1141 - acc: 0.665 - ETA: 4s - loss: 5.2226 - acc: 0.659 - ETA: 4s - loss: 5.2962 - acc: 0.653 - ETA: 4s - loss: 5.1855 - acc: 0.660 - ETA: 4s - loss: 5.2546 - acc: 0.656 - ETA: 3s - loss: 5.2876 - acc: 0.655 - ETA: 3s - loss: 5.2535 - acc: 0.658 - ETA: 3s - loss: 5.3239 - acc: 0.655 - ETA: 3s - loss: 5.3435 - acc: 0.653 - ETA: 3s - loss: 5.3024 - acc: 0.655 - ETA: 3s - loss: 5.2632 - acc: 0.656 - ETA: 3s - loss: 5.2892 - acc: 0.654 - ETA: 3s - loss: 5.2501 - acc: 0.656 - ETA: 3s - loss: 5.2513 - acc: 0.656 - ETA: 3s - loss: 5.2381 - acc: 0.657 - ETA: 3s - loss: 5.2487 - acc: 0.657 - ETA: 3s - loss: 5.2975 - acc: 0.654 - ETA: 3s - loss: 5.2771 - acc: 0.655 - ETA: 3s - loss: 5.2770 - acc: 0.655 - ETA: 3s - loss: 5.2590 - acc: 0.656 - ETA: 3s - loss: 5.2587 - acc: 0.656 - ETA: 2s - loss: 5.2963 - acc: 0.654 - ETA: 2s - loss: 5.3255 - acc: 0.652 - ETA: 2s - loss: 5.3407 - acc: 0.651 - ETA: 2s - loss: 5.2953 - acc: 0.654 - ETA: 2s - loss: 5.2862 - acc: 0.654 - ETA: 2s - loss: 5.2667 - acc: 0.654 - ETA: 2s - loss: 5.2720 - acc: 0.654 - ETA: 2s - loss: 5.2831 - acc: 0.653 - ETA: 2s - loss: 5.2487 - acc: 0.656 - ETA: 2s - loss: 5.2549 - acc: 0.655 - ETA: 2s - loss: 5.2478 - acc: 0.656 - ETA: 2s - loss: 5.2467 - acc: 0.655 - ETA: 2s - loss: 5.2602 - acc: 0.654 - ETA: 2s - loss: 5.2646 - acc: 0.654 - ETA: 2s - loss: 5.2631 - acc: 0.654 - ETA: 2s - loss: 5.2514 - acc: 0.655 - ETA: 2s - loss: 5.2807 - acc: 0.653 - ETA: 1s - loss: 5.2793 - acc: 0.653 - ETA: 1s - loss: 5.2890 - acc: 0.652 - ETA: 1s - loss: 5.2966 - acc: 0.651 - ETA: 1s - loss: 5.3252 - acc: 0.650 - ETA: 1s - loss: 5.3387 - acc: 0.649 - ETA: 1s - loss: 5.3135 - acc: 0.650 - ETA: 1s - loss: 5.3165 - acc: 0.650 - ETA: 1s - loss: 5.3380 - acc: 0.649 - ETA: 1s - loss: 5.3439 - acc: 0.648 - ETA: 1s - loss: 5.3453 - acc: 0.649 - ETA: 1s - loss: 5.3352 - acc: 0.649 - ETA: 1s - loss: 5.3440 - acc: 0.649 - ETA: 1s - loss: 5.3522 - acc: 0.649 - ETA: 1s - loss: 5.3497 - acc: 0.649 - ETA: 1s - loss: 5.3496 - acc: 0.649 - ETA: 1s - loss: 5.3396 - acc: 0.649 - ETA: 1s - loss: 5.3605 - acc: 0.647 - ETA: 0s - loss: 5.3637 - acc: 0.647 - ETA: 0s - loss: 5.3478 - acc: 0.648 - ETA: 0s - loss: 5.3299 - acc: 0.649 - ETA: 0s - loss: 5.3344 - acc: 0.649 - ETA: 0s - loss: 5.3367 - acc: 0.649 - ETA: 0s - loss: 5.3442 - acc: 0.649 - ETA: 0s - loss: 5.3692 - acc: 0.647 - ETA: 0s - loss: 5.3785 - acc: 0.647 - ETA: 0s - loss: 5.3742 - acc: 0.647 - ETA: 0s - loss: 5.3776 - acc: 0.647 - ETA: 0s - loss: 5.3737 - acc: 0.647 - ETA: 0s - loss: 5.3755 - acc: 0.647 - ETA: 0s - loss: 5.3725 - acc: 0.648 - ETA: 0s - loss: 5.3717 - acc: 0.647 - ETA: 0s - loss: 5.3877 - acc: 0.646 - ETA: 0s - loss: 5.3986 - acc: 0.646 - 5s 811us/step - loss: 5.3987 - acc: 0.6461 - val_loss: 6.4199 - val_acc: 0.5114\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 6.37647\n",
      "Epoch 13/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 3.4447 - acc: 0.750 - ETA: 5s - loss: 5.5936 - acc: 0.630 - ETA: 4s - loss: 5.5856 - acc: 0.627 - ETA: 4s - loss: 5.3474 - acc: 0.646 - ETA: 4s - loss: 5.7086 - acc: 0.620 - ETA: 4s - loss: 5.3243 - acc: 0.642 - ETA: 4s - loss: 5.4762 - acc: 0.638 - ETA: 4s - loss: 5.3797 - acc: 0.646 - ETA: 4s - loss: 5.3403 - acc: 0.651 - ETA: 4s - loss: 5.3611 - acc: 0.650 - ETA: 4s - loss: 5.3899 - acc: 0.648 - ETA: 4s - loss: 5.3393 - acc: 0.651 - ETA: 4s - loss: 5.3668 - acc: 0.650 - ETA: 4s - loss: 5.4348 - acc: 0.646 - ETA: 4s - loss: 5.4041 - acc: 0.646 - ETA: 4s - loss: 5.3566 - acc: 0.649 - ETA: 3s - loss: 5.3627 - acc: 0.650 - ETA: 3s - loss: 5.3780 - acc: 0.648 - ETA: 3s - loss: 5.3607 - acc: 0.650 - ETA: 3s - loss: 5.4107 - acc: 0.646 - ETA: 3s - loss: 5.4382 - acc: 0.645 - ETA: 3s - loss: 5.4489 - acc: 0.644 - ETA: 3s - loss: 5.4587 - acc: 0.643 - ETA: 3s - loss: 5.4730 - acc: 0.641 - ETA: 3s - loss: 5.5018 - acc: 0.640 - ETA: 3s - loss: 5.5451 - acc: 0.636 - ETA: 3s - loss: 5.5199 - acc: 0.638 - ETA: 3s - loss: 5.5392 - acc: 0.636 - ETA: 3s - loss: 5.4701 - acc: 0.641 - ETA: 3s - loss: 5.4369 - acc: 0.643 - ETA: 3s - loss: 5.4128 - acc: 0.644 - ETA: 3s - loss: 5.3976 - acc: 0.645 - ETA: 3s - loss: 5.4287 - acc: 0.643 - ETA: 2s - loss: 5.4545 - acc: 0.641 - ETA: 2s - loss: 5.4230 - acc: 0.643 - ETA: 2s - loss: 5.4346 - acc: 0.642 - ETA: 2s - loss: 5.4318 - acc: 0.643 - ETA: 2s - loss: 5.4492 - acc: 0.642 - ETA: 2s - loss: 5.4362 - acc: 0.643 - ETA: 2s - loss: 5.3855 - acc: 0.646 - ETA: 2s - loss: 5.3667 - acc: 0.647 - ETA: 2s - loss: 5.3923 - acc: 0.645 - ETA: 2s - loss: 5.3841 - acc: 0.646 - ETA: 2s - loss: 5.3533 - acc: 0.648 - ETA: 2s - loss: 5.3619 - acc: 0.648 - ETA: 2s - loss: 5.3830 - acc: 0.646 - ETA: 2s - loss: 5.3637 - acc: 0.647 - ETA: 2s - loss: 5.3510 - acc: 0.648 - ETA: 2s - loss: 5.3195 - acc: 0.650 - ETA: 2s - loss: 5.3500 - acc: 0.649 - ETA: 1s - loss: 5.3616 - acc: 0.648 - ETA: 1s - loss: 5.3472 - acc: 0.649 - ETA: 1s - loss: 5.3318 - acc: 0.650 - ETA: 1s - loss: 5.3308 - acc: 0.650 - ETA: 1s - loss: 5.3159 - acc: 0.650 - ETA: 1s - loss: 5.3203 - acc: 0.650 - ETA: 1s - loss: 5.3222 - acc: 0.650 - ETA: 1s - loss: 5.3273 - acc: 0.649 - ETA: 1s - loss: 5.3269 - acc: 0.649 - ETA: 1s - loss: 5.3286 - acc: 0.648 - ETA: 1s - loss: 5.3204 - acc: 0.649 - ETA: 1s - loss: 5.3028 - acc: 0.650 - ETA: 1s - loss: 5.3052 - acc: 0.650 - ETA: 1s - loss: 5.3021 - acc: 0.650 - ETA: 1s - loss: 5.3015 - acc: 0.650 - ETA: 1s - loss: 5.3071 - acc: 0.650 - ETA: 1s - loss: 5.3192 - acc: 0.649 - ETA: 0s - loss: 5.3189 - acc: 0.649 - ETA: 0s - loss: 5.3213 - acc: 0.649 - ETA: 0s - loss: 5.3393 - acc: 0.648 - ETA: 0s - loss: 5.3447 - acc: 0.648 - ETA: 0s - loss: 5.3466 - acc: 0.648 - ETA: 0s - loss: 5.3438 - acc: 0.647 - ETA: 0s - loss: 5.3372 - acc: 0.648 - ETA: 0s - loss: 5.3375 - acc: 0.647 - ETA: 0s - loss: 5.3481 - acc: 0.647 - ETA: 0s - loss: 5.3443 - acc: 0.647 - ETA: 0s - loss: 5.3490 - acc: 0.647 - ETA: 0s - loss: 5.3474 - acc: 0.647 - ETA: 0s - loss: 5.3210 - acc: 0.648 - ETA: 0s - loss: 5.3415 - acc: 0.647 - ETA: 0s - loss: 5.3334 - acc: 0.647 - ETA: 0s - loss: 5.3166 - acc: 0.648 - ETA: 0s - loss: 5.2968 - acc: 0.649 - 5s 792us/step - loss: 5.2980 - acc: 0.6497 - val_loss: 6.1612 - val_acc: 0.5305\n",
      "\n",
      "Epoch 00013: val_loss improved from 6.37647 to 6.16117, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 14/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 5.6423 - acc: 0.650 - ETA: 5s - loss: 5.3400 - acc: 0.660 - ETA: 5s - loss: 5.6062 - acc: 0.638 - ETA: 4s - loss: 5.0360 - acc: 0.669 - ETA: 4s - loss: 5.4160 - acc: 0.650 - ETA: 4s - loss: 5.5868 - acc: 0.638 - ETA: 4s - loss: 5.7394 - acc: 0.628 - ETA: 4s - loss: 5.6461 - acc: 0.634 - ETA: 4s - loss: 5.4579 - acc: 0.647 - ETA: 4s - loss: 5.3161 - acc: 0.655 - ETA: 4s - loss: 5.2513 - acc: 0.659 - ETA: 4s - loss: 5.2690 - acc: 0.660 - ETA: 4s - loss: 5.1690 - acc: 0.666 - ETA: 4s - loss: 5.2207 - acc: 0.664 - ETA: 4s - loss: 5.1939 - acc: 0.666 - ETA: 3s - loss: 5.1596 - acc: 0.668 - ETA: 3s - loss: 5.2110 - acc: 0.665 - ETA: 3s - loss: 5.1564 - acc: 0.668 - ETA: 3s - loss: 5.1481 - acc: 0.669 - ETA: 3s - loss: 5.1772 - acc: 0.666 - ETA: 3s - loss: 5.2376 - acc: 0.663 - ETA: 3s - loss: 5.1399 - acc: 0.668 - ETA: 3s - loss: 5.1355 - acc: 0.666 - ETA: 3s - loss: 5.1411 - acc: 0.666 - ETA: 3s - loss: 5.1104 - acc: 0.668 - ETA: 3s - loss: 5.1644 - acc: 0.664 - ETA: 3s - loss: 5.1788 - acc: 0.663 - ETA: 3s - loss: 5.1882 - acc: 0.662 - ETA: 3s - loss: 5.1520 - acc: 0.664 - ETA: 3s - loss: 5.1990 - acc: 0.661 - ETA: 3s - loss: 5.2270 - acc: 0.659 - ETA: 3s - loss: 5.2148 - acc: 0.661 - ETA: 2s - loss: 5.2106 - acc: 0.661 - ETA: 2s - loss: 5.2195 - acc: 0.660 - ETA: 2s - loss: 5.2132 - acc: 0.660 - ETA: 2s - loss: 5.1975 - acc: 0.662 - ETA: 2s - loss: 5.2100 - acc: 0.661 - ETA: 2s - loss: 5.1540 - acc: 0.664 - ETA: 2s - loss: 5.2190 - acc: 0.660 - ETA: 2s - loss: 5.2253 - acc: 0.660 - ETA: 2s - loss: 5.2291 - acc: 0.660 - ETA: 2s - loss: 5.1857 - acc: 0.662 - ETA: 2s - loss: 5.1908 - acc: 0.662 - ETA: 2s - loss: 5.1699 - acc: 0.663 - ETA: 2s - loss: 5.1825 - acc: 0.663 - ETA: 2s - loss: 5.1991 - acc: 0.661 - ETA: 2s - loss: 5.2430 - acc: 0.659 - ETA: 2s - loss: 5.2479 - acc: 0.659 - ETA: 2s - loss: 5.2314 - acc: 0.660 - ETA: 1s - loss: 5.2016 - acc: 0.662 - ETA: 1s - loss: 5.2012 - acc: 0.662 - ETA: 1s - loss: 5.2374 - acc: 0.660 - ETA: 1s - loss: 5.2417 - acc: 0.660 - ETA: 1s - loss: 5.2812 - acc: 0.657 - ETA: 1s - loss: 5.2960 - acc: 0.656 - ETA: 1s - loss: 5.2666 - acc: 0.658 - ETA: 1s - loss: 5.2707 - acc: 0.657 - ETA: 1s - loss: 5.2710 - acc: 0.657 - ETA: 1s - loss: 5.2765 - acc: 0.656 - ETA: 1s - loss: 5.2898 - acc: 0.656 - ETA: 1s - loss: 5.2929 - acc: 0.656 - ETA: 1s - loss: 5.2892 - acc: 0.656 - ETA: 1s - loss: 5.2825 - acc: 0.656 - ETA: 1s - loss: 5.2914 - acc: 0.656 - ETA: 1s - loss: 5.2972 - acc: 0.656 - ETA: 1s - loss: 5.2719 - acc: 0.657 - ETA: 0s - loss: 5.2509 - acc: 0.659 - ETA: 0s - loss: 5.2330 - acc: 0.660 - ETA: 0s - loss: 5.2223 - acc: 0.660 - ETA: 0s - loss: 5.2267 - acc: 0.660 - ETA: 0s - loss: 5.2279 - acc: 0.660 - ETA: 0s - loss: 5.2236 - acc: 0.660 - ETA: 0s - loss: 5.2212 - acc: 0.660 - ETA: 0s - loss: 5.2240 - acc: 0.660 - ETA: 0s - loss: 5.2371 - acc: 0.659 - ETA: 0s - loss: 5.2282 - acc: 0.660 - ETA: 0s - loss: 5.2378 - acc: 0.659 - ETA: 0s - loss: 5.2264 - acc: 0.660 - ETA: 0s - loss: 5.2016 - acc: 0.661 - ETA: 0s - loss: 5.2147 - acc: 0.661 - ETA: 0s - loss: 5.2160 - acc: 0.661 - ETA: 0s - loss: 5.1967 - acc: 0.662 - ETA: 0s - loss: 5.1753 - acc: 0.663 - ETA: 0s - loss: 5.1817 - acc: 0.663 - 5s 785us/step - loss: 5.1926 - acc: 0.6624 - val_loss: 6.1997 - val_acc: 0.5210\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 6.16117\n",
      "Epoch 15/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 6.6652 - acc: 0.550 - ETA: 5s - loss: 6.1002 - acc: 0.612 - ETA: 5s - loss: 5.8100 - acc: 0.625 - ETA: 5s - loss: 6.0605 - acc: 0.612 - ETA: 4s - loss: 6.0123 - acc: 0.618 - ETA: 4s - loss: 5.9524 - acc: 0.620 - ETA: 4s - loss: 5.7348 - acc: 0.633 - ETA: 4s - loss: 5.7100 - acc: 0.633 - ETA: 4s - loss: 5.6011 - acc: 0.640 - ETA: 4s - loss: 5.5888 - acc: 0.641 - ETA: 4s - loss: 5.4367 - acc: 0.650 - ETA: 4s - loss: 5.3714 - acc: 0.654 - ETA: 4s - loss: 5.2662 - acc: 0.660 - ETA: 4s - loss: 5.2023 - acc: 0.665 - ETA: 4s - loss: 5.1059 - acc: 0.671 - ETA: 4s - loss: 5.2659 - acc: 0.661 - ETA: 4s - loss: 5.2401 - acc: 0.663 - ETA: 4s - loss: 5.2296 - acc: 0.664 - ETA: 4s - loss: 5.2641 - acc: 0.662 - ETA: 3s - loss: 5.2471 - acc: 0.664 - ETA: 3s - loss: 5.2734 - acc: 0.662 - ETA: 3s - loss: 5.2816 - acc: 0.661 - ETA: 3s - loss: 5.2692 - acc: 0.662 - ETA: 3s - loss: 5.2377 - acc: 0.662 - ETA: 3s - loss: 5.2376 - acc: 0.663 - ETA: 3s - loss: 5.2480 - acc: 0.662 - ETA: 3s - loss: 5.2191 - acc: 0.664 - ETA: 3s - loss: 5.1730 - acc: 0.667 - ETA: 3s - loss: 5.1523 - acc: 0.669 - ETA: 3s - loss: 5.1444 - acc: 0.668 - ETA: 3s - loss: 5.1233 - acc: 0.669 - ETA: 3s - loss: 5.0980 - acc: 0.671 - ETA: 3s - loss: 5.0782 - acc: 0.672 - ETA: 3s - loss: 5.0858 - acc: 0.671 - ETA: 3s - loss: 5.0923 - acc: 0.670 - ETA: 3s - loss: 5.0782 - acc: 0.671 - ETA: 3s - loss: 5.1050 - acc: 0.669 - ETA: 2s - loss: 5.1257 - acc: 0.667 - ETA: 2s - loss: 5.0876 - acc: 0.669 - ETA: 2s - loss: 5.1187 - acc: 0.668 - ETA: 2s - loss: 5.1232 - acc: 0.668 - ETA: 2s - loss: 5.0881 - acc: 0.669 - ETA: 2s - loss: 5.1331 - acc: 0.666 - ETA: 2s - loss: 5.1307 - acc: 0.667 - ETA: 2s - loss: 5.1158 - acc: 0.668 - ETA: 2s - loss: 5.1158 - acc: 0.668 - ETA: 2s - loss: 5.1249 - acc: 0.667 - ETA: 2s - loss: 5.0829 - acc: 0.670 - ETA: 2s - loss: 5.1038 - acc: 0.669 - ETA: 2s - loss: 5.0933 - acc: 0.669 - ETA: 2s - loss: 5.1186 - acc: 0.668 - ETA: 2s - loss: 5.1049 - acc: 0.669 - ETA: 2s - loss: 5.1076 - acc: 0.669 - ETA: 2s - loss: 5.1241 - acc: 0.668 - ETA: 1s - loss: 5.1228 - acc: 0.668 - ETA: 1s - loss: 5.1227 - acc: 0.668 - ETA: 1s - loss: 5.1406 - acc: 0.667 - ETA: 1s - loss: 5.1452 - acc: 0.666 - ETA: 1s - loss: 5.1297 - acc: 0.667 - ETA: 1s - loss: 5.1234 - acc: 0.667 - ETA: 1s - loss: 5.0982 - acc: 0.669 - ETA: 1s - loss: 5.1027 - acc: 0.668 - ETA: 1s - loss: 5.0990 - acc: 0.669 - ETA: 1s - loss: 5.0814 - acc: 0.670 - ETA: 1s - loss: 5.1024 - acc: 0.669 - ETA: 1s - loss: 5.0932 - acc: 0.670 - ETA: 1s - loss: 5.1091 - acc: 0.668 - ETA: 1s - loss: 5.0876 - acc: 0.670 - ETA: 1s - loss: 5.0639 - acc: 0.671 - ETA: 1s - loss: 5.0743 - acc: 0.670 - ETA: 1s - loss: 5.0745 - acc: 0.671 - ETA: 0s - loss: 5.0653 - acc: 0.671 - ETA: 0s - loss: 5.0679 - acc: 0.671 - ETA: 0s - loss: 5.0473 - acc: 0.673 - ETA: 0s - loss: 5.0477 - acc: 0.673 - ETA: 0s - loss: 5.0508 - acc: 0.672 - ETA: 0s - loss: 5.0551 - acc: 0.672 - ETA: 0s - loss: 5.0516 - acc: 0.673 - ETA: 0s - loss: 5.0490 - acc: 0.673 - ETA: 0s - loss: 5.0577 - acc: 0.672 - ETA: 0s - loss: 5.0637 - acc: 0.672 - ETA: 0s - loss: 5.0752 - acc: 0.671 - ETA: 0s - loss: 5.0810 - acc: 0.671 - ETA: 0s - loss: 5.0669 - acc: 0.672 - ETA: 0s - loss: 5.0796 - acc: 0.671 - ETA: 0s - loss: 5.0876 - acc: 0.670 - ETA: 0s - loss: 5.0967 - acc: 0.670 - ETA: 0s - loss: 5.0932 - acc: 0.670 - 6s 836us/step - loss: 5.1013 - acc: 0.6699 - val_loss: 6.1247 - val_acc: 0.5257\n",
      "\n",
      "Epoch 00015: val_loss improved from 6.16117 to 6.12474, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 16/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 5.7658 - acc: 0.600 - ETA: 5s - loss: 4.7198 - acc: 0.675 - ETA: 5s - loss: 4.2266 - acc: 0.706 - ETA: 5s - loss: 4.3342 - acc: 0.695 - ETA: 5s - loss: 4.2737 - acc: 0.696 - ETA: 4s - loss: 4.3672 - acc: 0.694 - ETA: 4s - loss: 4.4442 - acc: 0.691 - ETA: 4s - loss: 4.7256 - acc: 0.677 - ETA: 4s - loss: 4.7570 - acc: 0.675 - ETA: 4s - loss: 4.9034 - acc: 0.665 - ETA: 4s - loss: 4.9088 - acc: 0.665 - ETA: 4s - loss: 4.9215 - acc: 0.667 - ETA: 4s - loss: 4.9830 - acc: 0.666 - ETA: 4s - loss: 5.0349 - acc: 0.664 - ETA: 4s - loss: 4.9778 - acc: 0.670 - ETA: 4s - loss: 4.9796 - acc: 0.670 - ETA: 4s - loss: 4.9223 - acc: 0.674 - ETA: 4s - loss: 4.8512 - acc: 0.677 - ETA: 3s - loss: 4.8252 - acc: 0.678 - ETA: 3s - loss: 4.7766 - acc: 0.682 - ETA: 3s - loss: 4.8304 - acc: 0.679 - ETA: 3s - loss: 4.8778 - acc: 0.675 - ETA: 3s - loss: 4.9496 - acc: 0.671 - ETA: 3s - loss: 4.9423 - acc: 0.671 - ETA: 3s - loss: 4.9591 - acc: 0.670 - ETA: 3s - loss: 4.9206 - acc: 0.671 - ETA: 3s - loss: 4.9103 - acc: 0.672 - ETA: 3s - loss: 4.9467 - acc: 0.670 - ETA: 3s - loss: 4.9627 - acc: 0.669 - ETA: 3s - loss: 4.9666 - acc: 0.669 - ETA: 3s - loss: 4.9574 - acc: 0.669 - ETA: 3s - loss: 4.9278 - acc: 0.671 - ETA: 3s - loss: 4.8889 - acc: 0.674 - ETA: 3s - loss: 4.8939 - acc: 0.674 - ETA: 3s - loss: 4.8880 - acc: 0.674 - ETA: 3s - loss: 4.9007 - acc: 0.674 - ETA: 2s - loss: 4.9274 - acc: 0.672 - ETA: 2s - loss: 4.9648 - acc: 0.670 - ETA: 2s - loss: 4.9506 - acc: 0.671 - ETA: 2s - loss: 4.9322 - acc: 0.672 - ETA: 2s - loss: 4.9337 - acc: 0.672 - ETA: 2s - loss: 4.9193 - acc: 0.673 - ETA: 2s - loss: 4.9263 - acc: 0.671 - ETA: 2s - loss: 4.9185 - acc: 0.672 - ETA: 2s - loss: 4.9222 - acc: 0.672 - ETA: 2s - loss: 4.9035 - acc: 0.673 - ETA: 2s - loss: 4.8898 - acc: 0.674 - ETA: 2s - loss: 4.8933 - acc: 0.673 - ETA: 2s - loss: 4.8877 - acc: 0.673 - ETA: 2s - loss: 4.9003 - acc: 0.672 - ETA: 2s - loss: 4.8925 - acc: 0.672 - ETA: 2s - loss: 4.8925 - acc: 0.672 - ETA: 1s - loss: 4.9364 - acc: 0.669 - ETA: 1s - loss: 4.9205 - acc: 0.670 - ETA: 1s - loss: 4.8943 - acc: 0.672 - ETA: 1s - loss: 4.9029 - acc: 0.672 - ETA: 1s - loss: 4.9291 - acc: 0.670 - ETA: 1s - loss: 4.9075 - acc: 0.671 - ETA: 1s - loss: 4.9050 - acc: 0.671 - ETA: 1s - loss: 4.9082 - acc: 0.671 - ETA: 1s - loss: 4.9144 - acc: 0.671 - ETA: 1s - loss: 4.9037 - acc: 0.672 - ETA: 1s - loss: 4.9062 - acc: 0.672 - ETA: 1s - loss: 4.9219 - acc: 0.670 - ETA: 1s - loss: 4.9114 - acc: 0.671 - ETA: 1s - loss: 4.9207 - acc: 0.670 - ETA: 1s - loss: 4.9310 - acc: 0.670 - ETA: 1s - loss: 4.9217 - acc: 0.670 - ETA: 1s - loss: 4.9346 - acc: 0.670 - ETA: 0s - loss: 4.9381 - acc: 0.670 - ETA: 0s - loss: 4.9353 - acc: 0.670 - ETA: 0s - loss: 4.9327 - acc: 0.670 - ETA: 0s - loss: 4.9558 - acc: 0.669 - ETA: 0s - loss: 4.9478 - acc: 0.669 - ETA: 0s - loss: 4.9411 - acc: 0.669 - ETA: 0s - loss: 4.9365 - acc: 0.669 - ETA: 0s - loss: 4.9469 - acc: 0.668 - ETA: 0s - loss: 4.9329 - acc: 0.669 - ETA: 0s - loss: 4.9298 - acc: 0.669 - ETA: 0s - loss: 4.9299 - acc: 0.670 - ETA: 0s - loss: 4.8950 - acc: 0.672 - ETA: 0s - loss: 4.8892 - acc: 0.672 - ETA: 0s - loss: 4.8817 - acc: 0.673 - ETA: 0s - loss: 4.8843 - acc: 0.673 - ETA: 0s - loss: 4.8688 - acc: 0.674 - ETA: 0s - loss: 4.8639 - acc: 0.674 - 6s 828us/step - loss: 4.8565 - acc: 0.6750 - val_loss: 5.7724 - val_acc: 0.5521\n",
      "\n",
      "Epoch 00016: val_loss improved from 6.12474 to 5.77243, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 17/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 1.2252 - acc: 0.900 - ETA: 5s - loss: 3.9579 - acc: 0.737 - ETA: 5s - loss: 3.6454 - acc: 0.764 - ETA: 5s - loss: 3.8612 - acc: 0.754 - ETA: 5s - loss: 4.2197 - acc: 0.726 - ETA: 5s - loss: 4.0591 - acc: 0.736 - ETA: 4s - loss: 4.1265 - acc: 0.730 - ETA: 4s - loss: 4.3826 - acc: 0.716 - ETA: 4s - loss: 4.5193 - acc: 0.709 - ETA: 4s - loss: 4.6337 - acc: 0.700 - ETA: 4s - loss: 4.6818 - acc: 0.697 - ETA: 4s - loss: 4.7735 - acc: 0.691 - ETA: 4s - loss: 4.6561 - acc: 0.698 - ETA: 4s - loss: 4.6289 - acc: 0.701 - ETA: 4s - loss: 4.6184 - acc: 0.700 - ETA: 4s - loss: 4.6227 - acc: 0.699 - ETA: 4s - loss: 4.6477 - acc: 0.697 - ETA: 4s - loss: 4.6877 - acc: 0.695 - ETA: 4s - loss: 4.6867 - acc: 0.694 - ETA: 3s - loss: 4.6076 - acc: 0.697 - ETA: 3s - loss: 4.6226 - acc: 0.696 - ETA: 3s - loss: 4.6638 - acc: 0.694 - ETA: 3s - loss: 4.6014 - acc: 0.698 - ETA: 3s - loss: 4.6126 - acc: 0.697 - ETA: 3s - loss: 4.6779 - acc: 0.693 - ETA: 3s - loss: 4.6692 - acc: 0.693 - ETA: 3s - loss: 4.6839 - acc: 0.693 - ETA: 3s - loss: 4.6926 - acc: 0.693 - ETA: 3s - loss: 4.6432 - acc: 0.696 - ETA: 3s - loss: 4.6370 - acc: 0.696 - ETA: 3s - loss: 4.6383 - acc: 0.696 - ETA: 3s - loss: 4.5930 - acc: 0.698 - ETA: 3s - loss: 4.6217 - acc: 0.696 - ETA: 3s - loss: 4.6600 - acc: 0.694 - ETA: 2s - loss: 4.6371 - acc: 0.695 - ETA: 2s - loss: 4.6696 - acc: 0.693 - ETA: 2s - loss: 4.6975 - acc: 0.691 - ETA: 2s - loss: 4.7023 - acc: 0.691 - ETA: 2s - loss: 4.6908 - acc: 0.691 - ETA: 2s - loss: 4.7159 - acc: 0.690 - ETA: 2s - loss: 4.7187 - acc: 0.690 - ETA: 2s - loss: 4.7320 - acc: 0.689 - ETA: 2s - loss: 4.7019 - acc: 0.691 - ETA: 2s - loss: 4.7006 - acc: 0.691 - ETA: 2s - loss: 4.7105 - acc: 0.690 - ETA: 2s - loss: 4.7280 - acc: 0.689 - ETA: 2s - loss: 4.7210 - acc: 0.689 - ETA: 2s - loss: 4.7274 - acc: 0.689 - ETA: 2s - loss: 4.7140 - acc: 0.690 - ETA: 2s - loss: 4.7136 - acc: 0.689 - ETA: 2s - loss: 4.7410 - acc: 0.688 - ETA: 2s - loss: 4.7593 - acc: 0.687 - ETA: 2s - loss: 4.7770 - acc: 0.686 - ETA: 2s - loss: 4.7738 - acc: 0.687 - ETA: 2s - loss: 4.7718 - acc: 0.686 - ETA: 1s - loss: 4.7451 - acc: 0.688 - ETA: 1s - loss: 4.7313 - acc: 0.689 - ETA: 1s - loss: 4.7262 - acc: 0.690 - ETA: 1s - loss: 4.7211 - acc: 0.690 - ETA: 1s - loss: 4.7140 - acc: 0.691 - ETA: 1s - loss: 4.6858 - acc: 0.692 - ETA: 1s - loss: 4.7100 - acc: 0.691 - ETA: 1s - loss: 4.7228 - acc: 0.690 - ETA: 1s - loss: 4.7194 - acc: 0.690 - ETA: 1s - loss: 4.7253 - acc: 0.690 - ETA: 1s - loss: 4.7705 - acc: 0.687 - ETA: 1s - loss: 4.7554 - acc: 0.688 - ETA: 1s - loss: 4.7599 - acc: 0.688 - ETA: 1s - loss: 4.7678 - acc: 0.687 - ETA: 1s - loss: 4.7505 - acc: 0.688 - ETA: 1s - loss: 4.7429 - acc: 0.689 - ETA: 1s - loss: 4.7355 - acc: 0.689 - ETA: 0s - loss: 4.7134 - acc: 0.691 - ETA: 0s - loss: 4.7158 - acc: 0.691 - ETA: 0s - loss: 4.7135 - acc: 0.691 - ETA: 0s - loss: 4.7228 - acc: 0.690 - ETA: 0s - loss: 4.7147 - acc: 0.691 - ETA: 0s - loss: 4.7019 - acc: 0.692 - ETA: 0s - loss: 4.6929 - acc: 0.692 - ETA: 0s - loss: 4.7070 - acc: 0.691 - ETA: 0s - loss: 4.7110 - acc: 0.691 - ETA: 0s - loss: 4.7205 - acc: 0.691 - ETA: 0s - loss: 4.7221 - acc: 0.691 - ETA: 0s - loss: 4.7290 - acc: 0.690 - ETA: 0s - loss: 4.7391 - acc: 0.690 - ETA: 0s - loss: 4.7431 - acc: 0.690 - ETA: 0s - loss: 4.7351 - acc: 0.690 - ETA: 0s - loss: 4.7244 - acc: 0.690 - 6s 839us/step - loss: 4.7299 - acc: 0.6906 - val_loss: 5.7636 - val_acc: 0.5545\n",
      "\n",
      "Epoch 00017: val_loss improved from 5.77243 to 5.76359, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 18/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 4.8433 - acc: 0.700 - ETA: 5s - loss: 5.1501 - acc: 0.675 - ETA: 5s - loss: 4.7903 - acc: 0.700 - ETA: 5s - loss: 4.8819 - acc: 0.695 - ETA: 5s - loss: 4.7638 - acc: 0.700 - ETA: 5s - loss: 4.7654 - acc: 0.697 - ETA: 5s - loss: 4.5728 - acc: 0.706 - ETA: 4s - loss: 4.6643 - acc: 0.698 - ETA: 4s - loss: 4.6319 - acc: 0.698 - ETA: 4s - loss: 4.5545 - acc: 0.702 - ETA: 4s - loss: 4.7048 - acc: 0.689 - ETA: 4s - loss: 4.8422 - acc: 0.681 - ETA: 4s - loss: 4.8426 - acc: 0.679 - ETA: 4s - loss: 4.7305 - acc: 0.688 - ETA: 4s - loss: 4.6451 - acc: 0.691 - ETA: 4s - loss: 4.5681 - acc: 0.694 - ETA: 4s - loss: 4.5183 - acc: 0.697 - ETA: 4s - loss: 4.5517 - acc: 0.695 - ETA: 4s - loss: 4.5929 - acc: 0.693 - ETA: 4s - loss: 4.5908 - acc: 0.694 - ETA: 3s - loss: 4.6043 - acc: 0.694 - ETA: 3s - loss: 4.6454 - acc: 0.692 - ETA: 3s - loss: 4.6362 - acc: 0.692 - ETA: 3s - loss: 4.6081 - acc: 0.692 - ETA: 3s - loss: 4.5791 - acc: 0.694 - ETA: 3s - loss: 4.5583 - acc: 0.696 - ETA: 3s - loss: 4.6156 - acc: 0.693 - ETA: 3s - loss: 4.6280 - acc: 0.691 - ETA: 3s - loss: 4.6251 - acc: 0.690 - ETA: 3s - loss: 4.5957 - acc: 0.692 - ETA: 3s - loss: 4.5631 - acc: 0.694 - ETA: 3s - loss: 4.5738 - acc: 0.694 - ETA: 3s - loss: 4.5540 - acc: 0.696 - ETA: 3s - loss: 4.5374 - acc: 0.697 - ETA: 3s - loss: 4.5040 - acc: 0.700 - ETA: 3s - loss: 4.5087 - acc: 0.700 - ETA: 2s - loss: 4.5073 - acc: 0.700 - ETA: 2s - loss: 4.5292 - acc: 0.699 - ETA: 2s - loss: 4.5092 - acc: 0.700 - ETA: 2s - loss: 4.5216 - acc: 0.699 - ETA: 2s - loss: 4.5047 - acc: 0.700 - ETA: 2s - loss: 4.5334 - acc: 0.698 - ETA: 2s - loss: 4.5518 - acc: 0.697 - ETA: 2s - loss: 4.5638 - acc: 0.697 - ETA: 2s - loss: 4.6028 - acc: 0.695 - ETA: 2s - loss: 4.5924 - acc: 0.696 - ETA: 2s - loss: 4.5830 - acc: 0.696 - ETA: 2s - loss: 4.5595 - acc: 0.698 - ETA: 2s - loss: 4.5700 - acc: 0.697 - ETA: 2s - loss: 4.5628 - acc: 0.697 - ETA: 2s - loss: 4.5616 - acc: 0.697 - ETA: 2s - loss: 4.5706 - acc: 0.697 - ETA: 2s - loss: 4.5693 - acc: 0.696 - ETA: 1s - loss: 4.5787 - acc: 0.696 - ETA: 1s - loss: 4.5977 - acc: 0.695 - ETA: 1s - loss: 4.5949 - acc: 0.696 - ETA: 1s - loss: 4.5889 - acc: 0.696 - ETA: 1s - loss: 4.5891 - acc: 0.695 - ETA: 1s - loss: 4.5641 - acc: 0.696 - ETA: 1s - loss: 4.5856 - acc: 0.695 - ETA: 1s - loss: 4.5798 - acc: 0.695 - ETA: 1s - loss: 4.5961 - acc: 0.694 - ETA: 1s - loss: 4.5779 - acc: 0.695 - ETA: 1s - loss: 4.5781 - acc: 0.695 - ETA: 1s - loss: 4.5875 - acc: 0.695 - ETA: 1s - loss: 4.6076 - acc: 0.693 - ETA: 1s - loss: 4.5993 - acc: 0.694 - ETA: 1s - loss: 4.6106 - acc: 0.693 - ETA: 1s - loss: 4.5842 - acc: 0.694 - ETA: 0s - loss: 4.5845 - acc: 0.694 - ETA: 0s - loss: 4.5943 - acc: 0.694 - ETA: 0s - loss: 4.5805 - acc: 0.695 - ETA: 0s - loss: 4.5969 - acc: 0.694 - ETA: 0s - loss: 4.5921 - acc: 0.694 - ETA: 0s - loss: 4.5827 - acc: 0.694 - ETA: 0s - loss: 4.5836 - acc: 0.694 - ETA: 0s - loss: 4.5966 - acc: 0.694 - ETA: 0s - loss: 4.6036 - acc: 0.693 - ETA: 0s - loss: 4.5990 - acc: 0.693 - ETA: 0s - loss: 4.5967 - acc: 0.693 - ETA: 0s - loss: 4.5860 - acc: 0.694 - ETA: 0s - loss: 4.5872 - acc: 0.694 - ETA: 0s - loss: 4.5932 - acc: 0.694 - ETA: 0s - loss: 4.5937 - acc: 0.694 - ETA: 0s - loss: 4.5931 - acc: 0.694 - 6s 830us/step - loss: 4.5833 - acc: 0.6952 - val_loss: 5.6092 - val_acc: 0.5581\n",
      "\n",
      "Epoch 00018: val_loss improved from 5.76359 to 5.60924, saving model to saved_models/weights.best.VGG16.hdf5\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6680/6680 [==============================] - ETA: 5s - loss: 4.8357 - acc: 0.700 - ETA: 5s - loss: 4.8356 - acc: 0.700 - ETA: 5s - loss: 4.3600 - acc: 0.718 - ETA: 4s - loss: 4.1676 - acc: 0.725 - ETA: 4s - loss: 4.2849 - acc: 0.721 - ETA: 4s - loss: 4.2752 - acc: 0.725 - ETA: 4s - loss: 4.0256 - acc: 0.739 - ETA: 4s - loss: 4.2697 - acc: 0.723 - ETA: 4s - loss: 4.1528 - acc: 0.728 - ETA: 4s - loss: 3.9684 - acc: 0.740 - ETA: 4s - loss: 4.0043 - acc: 0.737 - ETA: 4s - loss: 4.0105 - acc: 0.737 - ETA: 4s - loss: 4.0059 - acc: 0.737 - ETA: 4s - loss: 4.0550 - acc: 0.734 - ETA: 4s - loss: 4.1159 - acc: 0.730 - ETA: 4s - loss: 4.2181 - acc: 0.725 - ETA: 3s - loss: 4.2070 - acc: 0.726 - ETA: 3s - loss: 4.2233 - acc: 0.725 - ETA: 3s - loss: 4.2772 - acc: 0.720 - ETA: 3s - loss: 4.2857 - acc: 0.721 - ETA: 3s - loss: 4.3372 - acc: 0.718 - ETA: 3s - loss: 4.3130 - acc: 0.719 - ETA: 3s - loss: 4.3487 - acc: 0.717 - ETA: 3s - loss: 4.4150 - acc: 0.713 - ETA: 3s - loss: 4.4418 - acc: 0.711 - ETA: 3s - loss: 4.4878 - acc: 0.708 - ETA: 3s - loss: 4.4582 - acc: 0.710 - ETA: 3s - loss: 4.4518 - acc: 0.710 - ETA: 3s - loss: 4.4553 - acc: 0.710 - ETA: 3s - loss: 4.4501 - acc: 0.710 - ETA: 3s - loss: 4.4966 - acc: 0.708 - ETA: 3s - loss: 4.4973 - acc: 0.708 - ETA: 3s - loss: 4.4890 - acc: 0.709 - ETA: 2s - loss: 4.4758 - acc: 0.710 - ETA: 2s - loss: 4.5054 - acc: 0.708 - ETA: 2s - loss: 4.4778 - acc: 0.709 - ETA: 2s - loss: 4.5133 - acc: 0.706 - ETA: 2s - loss: 4.5060 - acc: 0.707 - ETA: 2s - loss: 4.5365 - acc: 0.705 - ETA: 2s - loss: 4.5546 - acc: 0.704 - ETA: 2s - loss: 4.5473 - acc: 0.705 - ETA: 2s - loss: 4.5575 - acc: 0.704 - ETA: 2s - loss: 4.5434 - acc: 0.705 - ETA: 2s - loss: 4.5505 - acc: 0.704 - ETA: 2s - loss: 4.5617 - acc: 0.704 - ETA: 2s - loss: 4.5297 - acc: 0.705 - ETA: 2s - loss: 4.5291 - acc: 0.705 - ETA: 2s - loss: 4.5241 - acc: 0.705 - ETA: 2s - loss: 4.5388 - acc: 0.705 - ETA: 2s - loss: 4.5493 - acc: 0.704 - ETA: 1s - loss: 4.5439 - acc: 0.705 - ETA: 1s - loss: 4.5260 - acc: 0.706 - ETA: 1s - loss: 4.5312 - acc: 0.706 - ETA: 1s - loss: 4.5301 - acc: 0.706 - ETA: 1s - loss: 4.5211 - acc: 0.706 - ETA: 1s - loss: 4.4938 - acc: 0.708 - ETA: 1s - loss: 4.5183 - acc: 0.707 - ETA: 1s - loss: 4.5312 - acc: 0.706 - ETA: 1s - loss: 4.5332 - acc: 0.706 - ETA: 1s - loss: 4.5257 - acc: 0.707 - ETA: 1s - loss: 4.5395 - acc: 0.705 - ETA: 1s - loss: 4.5387 - acc: 0.705 - ETA: 1s - loss: 4.5241 - acc: 0.706 - ETA: 1s - loss: 4.5207 - acc: 0.707 - ETA: 1s - loss: 4.5267 - acc: 0.706 - ETA: 1s - loss: 4.5275 - acc: 0.706 - ETA: 1s - loss: 4.5334 - acc: 0.706 - ETA: 0s - loss: 4.5246 - acc: 0.706 - ETA: 0s - loss: 4.5321 - acc: 0.706 - ETA: 0s - loss: 4.5249 - acc: 0.707 - ETA: 0s - loss: 4.5248 - acc: 0.706 - ETA: 0s - loss: 4.5300 - acc: 0.706 - ETA: 0s - loss: 4.5259 - acc: 0.706 - ETA: 0s - loss: 4.5249 - acc: 0.706 - ETA: 0s - loss: 4.5215 - acc: 0.707 - ETA: 0s - loss: 4.5065 - acc: 0.708 - ETA: 0s - loss: 4.5230 - acc: 0.707 - ETA: 0s - loss: 4.5023 - acc: 0.708 - ETA: 0s - loss: 4.5009 - acc: 0.708 - ETA: 0s - loss: 4.5081 - acc: 0.707 - ETA: 0s - loss: 4.4946 - acc: 0.708 - ETA: 0s - loss: 4.4919 - acc: 0.709 - ETA: 0s - loss: 4.5002 - acc: 0.708 - ETA: 0s - loss: 4.5012 - acc: 0.708 - 5s 802us/step - loss: 4.4928 - acc: 0.7088 - val_loss: 5.6993 - val_acc: 0.5629\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.60924\n",
      "Epoch 20/20\n",
      "6680/6680 [==============================] - ETA: 5s - loss: 3.2388 - acc: 0.800 - ETA: 5s - loss: 3.4436 - acc: 0.775 - ETA: 5s - loss: 4.3479 - acc: 0.725 - ETA: 5s - loss: 4.5785 - acc: 0.712 - ETA: 5s - loss: 4.5475 - acc: 0.715 - ETA: 4s - loss: 4.5107 - acc: 0.712 - ETA: 4s - loss: 4.5903 - acc: 0.704 - ETA: 4s - loss: 4.6542 - acc: 0.701 - ETA: 4s - loss: 4.5587 - acc: 0.707 - ETA: 4s - loss: 4.5743 - acc: 0.705 - ETA: 4s - loss: 4.5440 - acc: 0.707 - ETA: 4s - loss: 4.4608 - acc: 0.713 - ETA: 4s - loss: 4.4317 - acc: 0.713 - ETA: 4s - loss: 4.4449 - acc: 0.713 - ETA: 4s - loss: 4.3577 - acc: 0.719 - ETA: 4s - loss: 4.3425 - acc: 0.720 - ETA: 4s - loss: 4.3018 - acc: 0.722 - ETA: 3s - loss: 4.2752 - acc: 0.724 - ETA: 3s - loss: 4.2844 - acc: 0.724 - ETA: 3s - loss: 4.2720 - acc: 0.725 - ETA: 3s - loss: 4.2646 - acc: 0.725 - ETA: 3s - loss: 4.3058 - acc: 0.723 - ETA: 3s - loss: 4.3207 - acc: 0.722 - ETA: 3s - loss: 4.3177 - acc: 0.722 - ETA: 3s - loss: 4.3485 - acc: 0.720 - ETA: 3s - loss: 4.3397 - acc: 0.720 - ETA: 3s - loss: 4.3561 - acc: 0.719 - ETA: 3s - loss: 4.3670 - acc: 0.718 - ETA: 3s - loss: 4.3496 - acc: 0.719 - ETA: 3s - loss: 4.3687 - acc: 0.717 - ETA: 3s - loss: 4.4187 - acc: 0.714 - ETA: 3s - loss: 4.4391 - acc: 0.713 - ETA: 3s - loss: 4.4722 - acc: 0.710 - ETA: 2s - loss: 4.4929 - acc: 0.709 - ETA: 2s - loss: 4.4631 - acc: 0.711 - ETA: 2s - loss: 4.4165 - acc: 0.714 - ETA: 2s - loss: 4.4000 - acc: 0.714 - ETA: 2s - loss: 4.4081 - acc: 0.714 - ETA: 2s - loss: 4.4068 - acc: 0.714 - ETA: 2s - loss: 4.3945 - acc: 0.714 - ETA: 2s - loss: 4.3556 - acc: 0.717 - ETA: 2s - loss: 4.3772 - acc: 0.716 - ETA: 2s - loss: 4.3785 - acc: 0.716 - ETA: 2s - loss: 4.3767 - acc: 0.716 - ETA: 2s - loss: 4.4018 - acc: 0.714 - ETA: 2s - loss: 4.3811 - acc: 0.715 - ETA: 2s - loss: 4.3959 - acc: 0.714 - ETA: 2s - loss: 4.3497 - acc: 0.717 - ETA: 2s - loss: 4.3408 - acc: 0.718 - ETA: 2s - loss: 4.3599 - acc: 0.717 - ETA: 1s - loss: 4.3494 - acc: 0.717 - ETA: 1s - loss: 4.3859 - acc: 0.715 - ETA: 1s - loss: 4.3600 - acc: 0.717 - ETA: 1s - loss: 4.3793 - acc: 0.716 - ETA: 1s - loss: 4.3810 - acc: 0.716 - ETA: 1s - loss: 4.4116 - acc: 0.714 - ETA: 1s - loss: 4.4021 - acc: 0.714 - ETA: 1s - loss: 4.4110 - acc: 0.713 - ETA: 1s - loss: 4.3907 - acc: 0.715 - ETA: 1s - loss: 4.3916 - acc: 0.714 - ETA: 1s - loss: 4.3890 - acc: 0.715 - ETA: 1s - loss: 4.3948 - acc: 0.714 - ETA: 1s - loss: 4.4075 - acc: 0.714 - ETA: 1s - loss: 4.4047 - acc: 0.714 - ETA: 1s - loss: 4.4023 - acc: 0.714 - ETA: 1s - loss: 4.4152 - acc: 0.714 - ETA: 1s - loss: 4.4048 - acc: 0.714 - ETA: 0s - loss: 4.3983 - acc: 0.714 - ETA: 0s - loss: 4.3892 - acc: 0.715 - ETA: 0s - loss: 4.3967 - acc: 0.714 - ETA: 0s - loss: 4.4105 - acc: 0.713 - ETA: 0s - loss: 4.4273 - acc: 0.713 - ETA: 0s - loss: 4.4193 - acc: 0.713 - ETA: 0s - loss: 4.4221 - acc: 0.713 - ETA: 0s - loss: 4.4125 - acc: 0.713 - ETA: 0s - loss: 4.4179 - acc: 0.712 - ETA: 0s - loss: 4.4166 - acc: 0.713 - ETA: 0s - loss: 4.4215 - acc: 0.712 - ETA: 0s - loss: 4.4357 - acc: 0.711 - ETA: 0s - loss: 4.4487 - acc: 0.711 - ETA: 0s - loss: 4.4566 - acc: 0.710 - ETA: 0s - loss: 4.4672 - acc: 0.709 - ETA: 0s - loss: 4.4588 - acc: 0.710 - ETA: 0s - loss: 4.4592 - acc: 0.709 - 5s 799us/step - loss: 4.4499 - acc: 0.7103 - val_loss: 5.7753 - val_acc: 0.5557\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.60924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d715668>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.VGG16.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "VGG16_model.fit(train_VGG16, train_targets, \n",
    "          validation_data=(valid_VGG16, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "VGG16_model.load_weights('saved_models/weights.best.VGG16.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model\n",
    "\n",
    "Now, we can use the CNN to test how well it identifies breed within our test dataset of dog images.  We print the test accuracy below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 56.1005%\n"
     ]
    }
   ],
   "source": [
    "# get index of predicted dog breed for each image in test set\n",
    "VGG16_predictions = [np.argmax(VGG16_model.predict(np.expand_dims(feature, axis=0))) for feature in test_VGG16]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(VGG16_predictions)==np.argmax(test_targets, axis=1))/len(VGG16_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Dog Breed with the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from extract_bottleneck_features import *\n",
    "\n",
    "def VGG16_predict_breed(img_path):\n",
    "    # extract bottleneck features\n",
    "    bottleneck_feature = extract_VGG16(path_to_tensor(img_path))\n",
    "    # obtain predicted vector\n",
    "    predicted_vector = VGG16_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step5'></a>\n",
    "## Step 5: Create a CNN to Classify Dog Breeds (using Transfer Learning)\n",
    "\n",
    "You will now use transfer learning to create a CNN that can identify dog breed from images.  Your CNN must attain at least 60% accuracy on the test set.\n",
    "\n",
    "In Step 4, we used transfer learning to create a CNN using VGG-16 bottleneck features.  In this section, you must use the bottleneck features from a different pre-trained model.  To make things easier for you, we have pre-computed the features for all of the networks that are currently available in Keras:\n",
    "- [VGG-19](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogVGG19Data.npz) bottleneck features\n",
    "- [ResNet-50](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogResnet50Data.npz) bottleneck features\n",
    "- [Inception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogInceptionV3Data.npz) bottleneck features\n",
    "- [Xception](https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/DogXceptionData.npz) bottleneck features\n",
    "\n",
    "The files are encoded as such:\n",
    "\n",
    "    Dog{network}Data.npz\n",
    "    \n",
    "where `{network}`, in the above filename, can be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`.  Pick one of the above architectures, download the corresponding bottleneck features, and store the downloaded file in the `bottleneck_features/` folder in the repository.\n",
    "\n",
    "### (IMPLEMENTATION) Obtain Bottleneck Features\n",
    "\n",
    "In the code block below, extract the bottleneck features corresponding to the train, test, and validation sets by running the following:\n",
    "\n",
    "    bottleneck_features = np.load('bottleneck_features/Dog{network}Data.npz')\n",
    "    train_{network} = bottleneck_features['train']\n",
    "    valid_{network} = bottleneck_features['valid']\n",
    "    test_{network} = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Obtain bottleneck features from another pre-trained CNN.\n",
    "bottleneck_features = np.load('bottleneck_features/DogInceptionV3Data.npz')\n",
    "train_inception = bottleneck_features['train']\n",
    "valid_inception = bottleneck_features['valid']\n",
    "test_inception = bottleneck_features['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6680, 5, 5, 2048)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(836, 5, 5, 2048)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_inception.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(835, 5, 5, 2048)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_inception.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Model Architecture\n",
    "\n",
    "Create a CNN to classify dog breed.  At the end of your code cell block, summarize the layers of your model by executing the line:\n",
    "    \n",
    "        <your model's name>.summary()\n",
    "   \n",
    "__Question 5:__ Outline the steps you took to get to your final CNN architecture and your reasoning at each step.  Describe why you think the architecture is suitable for the current problem.\n",
    "\n",
    "__Answer:__ Initially I have tried with Resnet which threw an exception when I try to predict the dog breed of input images which I pass after spending sometime in debugging the issue, I opened slack channel to see if anyone has faced this kind of issue and then I found few of them faced the similar issue and then they have moved to inception model which lead me to use inception model. Then I have followed the same steps as followed for VGG16 model. My model has given 77.5% accuracy which is greater than VGG16 model. I have also used softmax to classify the output into 133 categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "global_average_pooling2d_4 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 133)               272517    \n",
      "=================================================================\n",
      "Total params: 272,517\n",
      "Trainable params: 272,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "### TODO: Define your architecture.\n",
    "\n",
    "DogInception_model  = Sequential()\n",
    "DogInception_model.add(GlobalAveragePooling2D(input_shape=train_inception.shape[1:]))\n",
    "DogInception_model.add(Dense(133, activation='softmax'))\n",
    "\n",
    "DogInception_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Compile the model.\n",
    "\n",
    "DogInception_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Train the Model\n",
    "\n",
    "Train your model in the code cell below.  Use model checkpointing to save the model that attains the best validation loss.  \n",
    "\n",
    "You are welcome to [augment the training data](https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html), but this is not a requirement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6680 samples, validate on 835 samples\n",
      "Epoch 1/20\n",
      " - 22s - loss: 1.1736 - acc: 0.7048 - val_loss: 0.6590 - val_acc: 0.8072\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.65902, saving model to saved_models/weights.best.InceptionV3.hdf5\n",
      "Epoch 2/20\n",
      " - 6s - loss: 0.4704 - acc: 0.8591 - val_loss: 0.6685 - val_acc: 0.8144\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 0.65902\n",
      "Epoch 3/20\n",
      " - 6s - loss: 0.3775 - acc: 0.8859 - val_loss: 0.6654 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.65902\n",
      "Epoch 4/20\n",
      " - 6s - loss: 0.2971 - acc: 0.9081 - val_loss: 0.7147 - val_acc: 0.8395\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.65902\n",
      "Epoch 5/20\n",
      " - 6s - loss: 0.2451 - acc: 0.9254 - val_loss: 0.6981 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.65902\n",
      "Epoch 6/20\n",
      " - 6s - loss: 0.2025 - acc: 0.9398 - val_loss: 0.7276 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.65902\n",
      "Epoch 7/20\n",
      " - 6s - loss: 0.1728 - acc: 0.9455 - val_loss: 0.7780 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.65902\n",
      "Epoch 8/20\n",
      " - 6s - loss: 0.1480 - acc: 0.9524 - val_loss: 0.7420 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.65902\n",
      "Epoch 9/20\n",
      " - 6s - loss: 0.1265 - acc: 0.9599 - val_loss: 0.7552 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.65902\n",
      "Epoch 10/20\n",
      " - 6s - loss: 0.1167 - acc: 0.9656 - val_loss: 0.7987 - val_acc: 0.8623\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.65902\n",
      "Epoch 11/20\n",
      " - 6s - loss: 0.0953 - acc: 0.9710 - val_loss: 0.8403 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.65902\n",
      "Epoch 12/20\n",
      " - 6s - loss: 0.0839 - acc: 0.9744 - val_loss: 0.8180 - val_acc: 0.8611\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.65902\n",
      "Epoch 13/20\n",
      " - 6s - loss: 0.0728 - acc: 0.9792 - val_loss: 0.8408 - val_acc: 0.8515\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.65902\n",
      "Epoch 14/20\n",
      " - 6s - loss: 0.0647 - acc: 0.9813 - val_loss: 0.8580 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.65902\n",
      "Epoch 15/20\n",
      " - 6s - loss: 0.0574 - acc: 0.9831 - val_loss: 0.8803 - val_acc: 0.8575\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.65902\n",
      "Epoch 16/20\n",
      " - 6s - loss: 0.0477 - acc: 0.9858 - val_loss: 0.9317 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.65902\n",
      "Epoch 17/20\n",
      " - 6s - loss: 0.0426 - acc: 0.9877 - val_loss: 0.9294 - val_acc: 0.8587\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.65902\n",
      "Epoch 18/20\n",
      " - 6s - loss: 0.0398 - acc: 0.9868 - val_loss: 0.9384 - val_acc: 0.8563\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.65902\n",
      "Epoch 19/20\n",
      " - 6s - loss: 0.0320 - acc: 0.9885 - val_loss: 0.9582 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.65902\n",
      "Epoch 20/20\n",
      " - 6s - loss: 0.0299 - acc: 0.9906 - val_loss: 0.9307 - val_acc: 0.8551\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.65902\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x28d7e6630>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/weights.best.InceptionV3.hdf5', \n",
    "                               verbose=2, save_best_only=True)\n",
    "\n",
    "DogInception_model.fit(train_inception, train_targets, \n",
    "          validation_data=(valid_inception, valid_targets),\n",
    "          epochs=20, batch_size=20, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Load the Model with the Best Validation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Load the model weights with the best validation loss.\n",
    "DogInception_model.load_weights('saved_models/weights.best.InceptionV3.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Test the Model\n",
    "\n",
    "Try out your model on the test dataset of dog images. Ensure that your test accuracy is greater than 60%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 77.5120%\n"
     ]
    }
   ],
   "source": [
    "### TODO: Calculate classification accuracy on the test dataset.\n",
    "DogInception_predictions = [np.argmax(DogInception_model.predict(np.expand_dims(feature, axis=0))) for feature in test_inception]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(DogInception_predictions)==np.argmax(test_targets, axis=1))/len(DogInception_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (IMPLEMENTATION) Predict Dog Breed with the Model\n",
    "\n",
    "Write a function that takes an image path as input and returns the dog breed (`Affenpinscher`, `Afghan_hound`, etc) that is predicted by your model.  \n",
    "\n",
    "Similar to the analogous function in Step 5, your function should have three steps:\n",
    "1. Extract the bottleneck features corresponding to the chosen CNN model.\n",
    "2. Supply the bottleneck features as input to the model to return the predicted vector.  Note that the argmax of this prediction vector gives the index of the predicted dog breed.\n",
    "3. Use the `dog_names` array defined in Step 0 of this notebook to return the corresponding breed.\n",
    "\n",
    "The functions to extract the bottleneck features can be found in `extract_bottleneck_features.py`, and they have been imported in an earlier code cell.  To obtain the bottleneck features corresponding to your chosen CNN architecture, you need to use the function\n",
    "\n",
    "    extract_{network}\n",
    "    \n",
    "where `{network}`, in the above filename, should be one of `VGG19`, `Resnet50`, `InceptionV3`, or `Xception`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write a function that takes a path to an image as input\n",
    "### and returns the dog breed that is predicted by the model.\n",
    "from extract_bottleneck_features import *\n",
    "from keras.preprocessing import image        \n",
    "\n",
    "def dog_breed_predictor(img_path):\n",
    "    # Extract the bottleneck features for Resnet CNN model\n",
    "    bottleneck_feature = extract_InceptionV3(path_to_tensor(img_path))\n",
    "   \n",
    "    #Load the best model\n",
    "    DogInception_model.load_weights('saved_models/weights.best.InceptionV3.hdf5')\n",
    "     # obtain predicted vector\n",
    "    predicted_vector = DogInception_model.predict(bottleneck_feature)\n",
    "    # return dog breed that is predicted by the model\n",
    "    return dog_names[np.argmax(predicted_vector)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step6'></a>\n",
    "## Step 6: Write your Algorithm\n",
    "\n",
    "Write an algorithm that accepts a file path to an image and first determines whether the image contains a human, dog, or neither.  Then,\n",
    "- if a __dog__ is detected in the image, return the predicted breed.\n",
    "- if a __human__ is detected in the image, return the resembling dog breed.\n",
    "- if __neither__ is detected in the image, provide output that indicates an error.\n",
    "\n",
    "You are welcome to write your own functions for detecting humans and dogs in images, but feel free to use the `face_detector` and `dog_detector` functions developed above.  You are __required__ to use your CNN from Step 5 to predict dog breed.  \n",
    "\n",
    "Some sample output for our algorithm is provided below, but feel free to design your own user experience!\n",
    "\n",
    "![Sample Human Output](images/sample_human_output.png)\n",
    "\n",
    "\n",
    "### (IMPLEMENTATION) Write your Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TODO: Write your algorithm.\n",
    "### Feel free to use as many code cells as needed.\n",
    "        \n",
    "def my_classifier(img_path):\n",
    "    if dog_detector(img_path):\n",
    "        print(\"Predicted dog breed is {dog_breed}\".format(dog_breed=dog_breed_predictor(img_path)))\n",
    "        \n",
    "    elif face_detector(img_path):\n",
    "        print(\"HUman detected, Resembling Dog breed is {dog_breed}\".format(dog_breed=dog_breed_predictor(img_path)))\n",
    "    \n",
    "    else:\n",
    "        print(\"Neither a dog nor human is detected, please pass another image as input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='step7'></a>\n",
    "## Step 7: Test Your Algorithm\n",
    "\n",
    "In this section, you will take your new algorithm for a spin!  What kind of dog does the algorithm think that __you__ look like?  If you have a dog, does it predict your dog's breed accurately?  If you have a cat, does it mistakenly think that your cat is a dog?\n",
    "\n",
    "### (IMPLEMENTATION) Test Your Algorithm on Sample Images!\n",
    "\n",
    "Test your algorithm at least six images on your computer.  Feel free to use any images you like.  Use at least two human and two dog images.  \n",
    "\n",
    "__Question 6:__ Is the output better than you expected :) ?  Or worse :( ?  Provide at least three possible points of improvement for your algorithm.\n",
    "\n",
    "__Answer:__ The output is far better than what I expected. \n",
    "1. Percentage of the first 100 images in dog_files with detected human face is 12% so may be if we can improve this percentage we can have a more accurate alogirthm.\n",
    "2. If we get any input of mixed breeds or an photo with half of one dog and other half of different breed then we may have to throw an error/exception. \n",
    "3. In the training data there are few do breed classes which has less than 10 images which can lead to lower accuracy.\n",
    "4. May be we have to explore more to reduce the time being taken by algorith to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initally passing all dog images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dog breed is Labrador_retriever\n"
     ]
    }
   ],
   "source": [
    "my_classifier('Labrador_retriever_06449.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dog breed is American_water_spaniel\n"
     ]
    }
   ],
   "source": [
    "my_classifier('American_water_spaniel_00648.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dog breed is Welsh_springer_spaniel\n"
     ]
    }
   ],
   "source": [
    "my_classifier('Welsh_springer_spaniel_08203.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dog breed is Beagle\n"
     ]
    }
   ],
   "source": [
    "my_classifier('Beagle.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# passing few human images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUman detected, Resembling Dog breed is Icelandic_sheepdog\n"
     ]
    }
   ],
   "source": [
    "my_classifier('human1.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUman detected, Resembling Dog breed is Icelandic_sheepdog\n"
     ]
    }
   ],
   "source": [
    "my_classifier('human2.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted dog breed is Labrador_retriever\n"
     ]
    }
   ],
   "source": [
    "my_classifier('haldandhalf.jpg')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
